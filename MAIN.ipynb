{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f8b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp import MLP\n",
    "from train_test import train_model, test_model\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62df5d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'torch.version' from 'C:\\\\Users\\\\Maxwell\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\\\\torch\\\\version.py'>\n",
      "2.0.0+cu118\n",
      "11.8\n",
      "['sm_37', 'sm_50', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'sm_90', 'compute_37']\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.version)\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.get_arch_list())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bad09dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/release/native/' # '../data/release/template/' for template space prediction\n",
    "task = 'sex' # 'birth_age' for birth age prediction\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 2000\n",
    "patience = 200 # for early stopping\n",
    "lr = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "665973e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File num:  1218\n"
     ]
    }
   ],
   "source": [
    "#check if datapath is right\n",
    "file_list = os.listdir(data_path)\n",
    "print('File num: ', len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45459d38",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "indices/sex_train.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_ids \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mloadtxt(\u001b[39m'\u001b[39;49m\u001b[39mindices/\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m task \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m_train.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, dtype\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mstr\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m val_ids \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mloadtxt(\u001b[39m'\u001b[39m\u001b[39mindices/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m task \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_validation.txt\u001b[39m\u001b[39m'\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstr\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m test_ids \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mloadtxt(\u001b[39m'\u001b[39m\u001b[39mindices/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m task \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_test.txt\u001b[39m\u001b[39m'\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstr\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\numpy\\lib\\npyio.py:1338\u001b[0m, in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1335\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(delimiter, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m   1336\u001b[0m     delimiter \u001b[39m=\u001b[39m delimiter\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 1338\u001b[0m arr \u001b[39m=\u001b[39m _read(fname, dtype\u001b[39m=\u001b[39;49mdtype, comment\u001b[39m=\u001b[39;49mcomment, delimiter\u001b[39m=\u001b[39;49mdelimiter,\n\u001b[0;32m   1339\u001b[0m             converters\u001b[39m=\u001b[39;49mconverters, skiplines\u001b[39m=\u001b[39;49mskiprows, usecols\u001b[39m=\u001b[39;49musecols,\n\u001b[0;32m   1340\u001b[0m             unpack\u001b[39m=\u001b[39;49munpack, ndmin\u001b[39m=\u001b[39;49mndmin, encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   1341\u001b[0m             max_rows\u001b[39m=\u001b[39;49mmax_rows, quote\u001b[39m=\u001b[39;49mquotechar)\n\u001b[0;32m   1343\u001b[0m \u001b[39mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\numpy\\lib\\npyio.py:975\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m    973\u001b[0m     fname \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mfspath(fname)\n\u001b[0;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fname, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 975\u001b[0m     fh \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlib\u001b[39m.\u001b[39;49m_datasource\u001b[39m.\u001b[39;49mopen(fname, \u001b[39m'\u001b[39;49m\u001b[39mrt\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49mencoding)\n\u001b[0;32m    976\u001b[0m     \u001b[39mif\u001b[39;00m encoding \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         encoding \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(fh, \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\numpy\\lib\\_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m ds \u001b[39m=\u001b[39m DataSource(destpath)\n\u001b[1;32m--> 193\u001b[0m \u001b[39mreturn\u001b[39;00m ds\u001b[39m.\u001b[39;49mopen(path, mode, encoding\u001b[39m=\u001b[39;49mencoding, newline\u001b[39m=\u001b[39;49mnewline)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\numpy\\lib\\_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[39m=\u001b[39mmode,\n\u001b[0;32m    531\u001b[0m                               encoding\u001b[39m=\u001b[39mencoding, newline\u001b[39m=\u001b[39mnewline)\n\u001b[0;32m    532\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m not found.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: indices/sex_train.txt not found."
     ]
    }
   ],
   "source": [
    "train_ids = np.loadtxt('indices/' + task + '_train.txt', dtype='str')\n",
    "val_ids = np.loadtxt('indices/' + task + '_validation.txt', dtype='str')\n",
    "test_ids = np.loadtxt('indices/' + task + '_test.txt', dtype='str')\n",
    "\n",
    "mirror_index = np.load('mirror_index.npy') # mirrors right hemispheres to match with left hemispheres\n",
    "\n",
    "df = pd.read_csv(\"combined.csv\")\n",
    "\n",
    "df.insert(0, \"ID\", \"sub-\" + df[\"participant_id\"] + \"_\" + \"ses-\" + df[\"session_id\"].apply(str))\n",
    "df.drop(\"participant_id\", axis=1, inplace=True)\n",
    "df.drop(\"session_id\", axis=1, inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5723ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set element sub-CC00061XX04_ses-13300 does not exist\n",
      "train set element sub-CC00084XX11_ses-31201 does not exist\n",
      "train set element sub-CC00143AN12_ses-47501 does not exist\n",
      "train set element sub-CC00170XX06_ses-56100 does not exist\n",
      "train set element sub-CC00217XX11_ses-73700 does not exist\n",
      "train set element sub-CC00221XX07_ses-75000 does not exist\n",
      "train set element sub-CC00291XX12_ses-93100 does not exist\n",
      "train set element sub-CC00307XX10_ses-98800 does not exist\n",
      "train set element sub-CC00341XX12_ses-108000 does not exist\n",
      "train set element sub-CC00319XX14_ses-117300 does not exist\n",
      "train set element sub-CC00439XX19_ses-132100 does not exist\n",
      "train set element sub-CC00442XX14_ses-133300 does not exist\n",
      "train set element sub-CC00371XX09_ses-134700 does not exist\n",
      "train set element sub-CC00468XX15_ses-139100 does not exist\n",
      "train set element sub-CC00501XX06_ses-146500 does not exist\n",
      "train set element sub-CC00227XX13_ses-92100 does not exist\n"
     ]
    }
   ],
   "source": [
    "def get_data(data_path, task, ids):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    check = 0\n",
    "    for _id in ids:\n",
    "        try:\n",
    "            img_L = nib.load(data_path + _id + '_left.shape.gii')\n",
    "            x_L = np.stack(img_L.agg_data(), axis=1)\n",
    "            for i in range(4):\n",
    "                # replaces the zeros of the medial wall cut area with mean values\n",
    "                x_L[:, i][x_L[:, i] == 0] = np.mean(x_L[:, i][x_L[:, i] != 0]) # i corresponds to what exactly?\n",
    "                #let's see; \n",
    "                #if check < 5:\n",
    "                    #print(\"Let's see what x_L is: \", x_L)\n",
    "                    #check += 1\n",
    "            xs.append(x_L.astype(np.float32))\n",
    "            y = np.array([df.loc[df['ID'] == _id, task].item()])\n",
    "            ys.append(y.astype(np.float32))\n",
    "            img_R = nib.load(data_path + _id + '_right.shape.gii')\n",
    "            x_R = np.stack(img_R.agg_data(), axis=1)[mirror_index] # mirroring\n",
    "            for i in range(4):\n",
    "                # replaces the zeros of the medial wall cut area with mean values\n",
    "                x_R[:, i][x_R[:, i] == 0] = np.mean(x_R[:, i][x_R[:, i] != 0])\n",
    "            xs.append(x_R.astype(np.float32))\n",
    "            y = np.array([df.loc[df['ID'] == _id, task].item()])\n",
    "            ys.append(y.astype(np.float32))\n",
    "        except:\n",
    "            print('train set element %s does not exist' % _id)\n",
    "    return xs, ys\n",
    "\n",
    "train_xs, train_ys = get_data(data_path, task, train_ids)\n",
    "val_xs, val_ys = get_data(data_path, task, val_ids)\n",
    "test_xs, test_ys = get_data(data_path, task, test_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np_test = np.array([np.array(xi) for xi in train_xs])\n",
    "# vstack makes it into a 2d array when I still need 3d...\n",
    "#np_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc177236",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_list = [len(inner_list) for inner_list in train_xs]\n",
    "mode = max(set(length_list), key=length_list.count)\n",
    "#offsize_list = [size for size in length_list if size != mode ]#\n",
    "#print(length_list[0:5])\n",
    "#print(max(length_list))\n",
    "#print(min(length_list))\n",
    "#print(offsize_list)\n",
    "offsize_list_ind = [i for i in range(len(length_list)) if length_list[i] != mode]\n",
    "##print(offsize_list_ind)\n",
    "only_correct_sized = [train_xs[i] for i in range(len(length_list)) if length_list[i] == mode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40962, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_xs, dtype=object)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25881ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xs = np.transpose(only_correct_sized, axes=[1, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6054185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data standardization\n",
    "\n",
    "#why doesn't this match?\n",
    "#train_xs = np.transpose(np.array(train_xs, dtype=object), axes=[1, 2, 0])\n",
    "means  = np.mean(np.mean(train_xs, axis=2), axis=0) # means of the 4 channels in the train set\n",
    "stds  = np.std(np.std(train_xs, axis=2), axis=0) # stds of the 4 channels in the train set\n",
    "train_xs = (train_xs - means.reshape(1, means.shape[0], 1)) / stds.reshape(1, means.shape[0], 1)\n",
    "train_xs = np.transpose(train_xs, axes=[2, 0, 1])\n",
    "\n",
    "val_xs = np.transpose(val_xs, axes=[1, 2, 0])\n",
    "val_xs = (val_xs - means.reshape(1, means.shape[0], 1)) / stds.reshape(1, means.shape[0], 1)\n",
    "val_xs = np.transpose(val_xs, axes=[2, 0, 1])\n",
    "\n",
    "test_xs = np.transpose(test_xs, axes=[1, 2, 0])\n",
    "test_xs = (test_xs - means.reshape(1, means.shape[0], 1)) / stds.reshape(1, means.shape[0], 1)\n",
    "test_xs = np.transpose(test_xs, axes=[2, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6197f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = [(torch.from_numpy(x), torch.from_numpy(y)) for x, y in zip(train_xs, train_ys)]\n",
    "\n",
    "val_subset = [(torch.from_numpy(x), torch.from_numpy(y)) for x, y in zip(val_xs, val_ys)]\n",
    "\n",
    "test_subset = [(torch.from_numpy(x), torch.from_numpy(y)) for x, y in zip(test_xs, test_ys)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3615f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (lin1): Linear(in_features=4, out_features=16, bias=True)\n",
      "  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lin2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lin3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (bn3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lin4): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (bn4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lin5): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (lin6): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "Number of parameters:  1313\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=len(val_subset), shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_subset, batch_size=len(test_subset), shuffle=False)\n",
    "\n",
    "model = MLP(4, [16, 16, 16, 16], 1, device=device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "print(model)\n",
    "print('Number of parameters: ', sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58185799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 0, train loss: 38.371, val loss: 38.204, test loss: 38.025\n",
      "* Epoch: 1, train loss: 38.150, val loss: 37.859, test loss: 37.725\n",
      "* Epoch: 2, train loss: 37.787, val loss: 37.413, test loss: 37.267\n",
      "* Epoch: 3, train loss: 37.202, val loss: 36.900, test loss: 36.723\n",
      "* Epoch: 4, train loss: 36.353, val loss: 36.077, test loss: 35.832\n",
      "* Epoch: 5, train loss: 35.352, val loss: 34.953, test loss: 34.723\n",
      "* Epoch: 6, train loss: 34.338, val loss: 33.926, test loss: 33.684\n",
      "* Epoch: 7, train loss: 33.411, val loss: 33.046, test loss: 32.801\n",
      "* Epoch: 8, train loss: 32.602, val loss: 32.283, test loss: 32.035\n",
      "* Epoch: 9, train loss: 31.866, val loss: 31.605, test loss: 31.353\n",
      "* Epoch: 10, train loss: 31.214, val loss: 30.980, test loss: 30.727\n",
      "* Epoch: 11, train loss: 30.601, val loss: 30.402, test loss: 30.146\n",
      "* Epoch: 12, train loss: 30.036, val loss: 29.856, test loss: 29.599\n",
      "* Epoch: 13, train loss: 29.486, val loss: 29.333, test loss: 29.076\n",
      "* Epoch: 14, train loss: 28.983, val loss: 28.830, test loss: 28.573\n",
      "* Epoch: 15, train loss: 28.474, val loss: 28.341, test loss: 28.084\n",
      "* Epoch: 16, train loss: 28.016, val loss: 27.870, test loss: 27.612\n",
      "* Epoch: 17, train loss: 27.534, val loss: 27.410, test loss: 27.151\n",
      "* Epoch: 18, train loss: 27.074, val loss: 26.959, test loss: 26.700\n",
      "* Epoch: 19, train loss: 26.643, val loss: 26.517, test loss: 26.258\n",
      "* Epoch: 20, train loss: 26.203, val loss: 26.084, test loss: 25.825\n",
      "* Epoch: 21, train loss: 25.767, val loss: 25.659, test loss: 25.399\n",
      "* Epoch: 22, train loss: 25.344, val loss: 25.240, test loss: 24.980\n",
      "* Epoch: 23, train loss: 24.925, val loss: 24.826, test loss: 24.567\n",
      "* Epoch: 24, train loss: 24.516, val loss: 24.420, test loss: 24.160\n",
      "* Epoch: 25, train loss: 24.109, val loss: 24.021, test loss: 23.761\n",
      "* Epoch: 26, train loss: 23.718, val loss: 23.625, test loss: 23.365\n",
      "* Epoch: 27, train loss: 23.311, val loss: 23.235, test loss: 22.975\n",
      "* Epoch: 28, train loss: 22.948, val loss: 22.849, test loss: 22.589\n",
      "* Epoch: 29, train loss: 22.539, val loss: 22.469, test loss: 22.209\n",
      "* Epoch: 30, train loss: 22.144, val loss: 22.092, test loss: 21.832\n",
      "* Epoch: 31, train loss: 21.802, val loss: 21.720, test loss: 21.460\n",
      "* Epoch: 32, train loss: 21.432, val loss: 21.354, test loss: 21.093\n",
      "* Epoch: 33, train loss: 21.075, val loss: 20.990, test loss: 20.729\n",
      "* Epoch: 34, train loss: 20.683, val loss: 20.630, test loss: 20.370\n",
      "* Epoch: 35, train loss: 20.362, val loss: 20.275, test loss: 20.015\n",
      "* Epoch: 36, train loss: 19.978, val loss: 19.923, test loss: 19.663\n",
      "* Epoch: 37, train loss: 19.614, val loss: 19.575, test loss: 19.314\n",
      "* Epoch: 38, train loss: 19.297, val loss: 19.232, test loss: 18.972\n",
      "* Epoch: 39, train loss: 18.955, val loss: 18.891, test loss: 18.631\n",
      "* Epoch: 40, train loss: 18.616, val loss: 18.555, test loss: 18.294\n",
      "* Epoch: 41, train loss: 18.266, val loss: 18.221, test loss: 17.961\n",
      "* Epoch: 42, train loss: 17.948, val loss: 17.891, test loss: 17.631\n",
      "* Epoch: 43, train loss: 17.607, val loss: 17.564, test loss: 17.304\n",
      "* Epoch: 44, train loss: 17.302, val loss: 17.242, test loss: 16.982\n",
      "* Epoch: 45, train loss: 17.000, val loss: 16.924, test loss: 16.663\n",
      "* Epoch: 46, train loss: 16.674, val loss: 16.607, test loss: 16.346\n",
      "* Epoch: 47, train loss: 16.315, val loss: 16.293, test loss: 16.032\n",
      "* Epoch: 48, train loss: 16.031, val loss: 15.985, test loss: 15.724\n",
      "* Epoch: 49, train loss: 15.725, val loss: 15.678, test loss: 15.418\n",
      "* Epoch: 50, train loss: 15.414, val loss: 15.376, test loss: 15.115\n",
      "* Epoch: 51, train loss: 15.118, val loss: 15.077, test loss: 14.816\n",
      "* Epoch: 52, train loss: 14.823, val loss: 14.782, test loss: 14.521\n",
      "* Epoch: 53, train loss: 14.536, val loss: 14.489, test loss: 14.228\n",
      "* Epoch: 54, train loss: 14.228, val loss: 14.197, test loss: 13.937\n",
      "* Epoch: 55, train loss: 13.959, val loss: 13.910, test loss: 13.649\n",
      "* Epoch: 56, train loss: 13.669, val loss: 13.628, test loss: 13.368\n",
      "* Epoch: 57, train loss: 13.412, val loss: 13.349, test loss: 13.088\n",
      "* Epoch: 58, train loss: 13.120, val loss: 13.071, test loss: 12.811\n",
      "* Epoch: 59, train loss: 12.840, val loss: 12.797, test loss: 12.537\n",
      "* Epoch: 60, train loss: 12.557, val loss: 12.527, test loss: 12.266\n",
      "* Epoch: 61, train loss: 12.323, val loss: 12.270, test loss: 12.001\n",
      "* Epoch: 62, train loss: 12.071, val loss: 12.016, test loss: 11.737\n",
      "* Epoch: 63, train loss: 11.820, val loss: 11.766, test loss: 11.477\n",
      "* Epoch: 64, train loss: 11.569, val loss: 11.517, test loss: 11.218\n",
      "* Epoch: 65, train loss: 11.300, val loss: 11.272, test loss: 10.963\n",
      "* Epoch: 66, train loss: 11.082, val loss: 11.033, test loss: 10.716\n",
      "* Epoch: 67, train loss: 10.849, val loss: 10.795, test loss: 10.478\n",
      "* Epoch: 68, train loss: 10.625, val loss: 10.560, test loss: 10.243\n",
      "* Epoch: 69, train loss: 10.399, val loss: 10.326, test loss: 10.017\n",
      "* Epoch: 70, train loss: 10.185, val loss: 10.100, test loss: 9.799\n",
      "* Epoch: 71, train loss: 9.965, val loss: 9.873, test loss: 9.586\n",
      "* Epoch: 72, train loss: 9.762, val loss: 9.652, test loss: 9.380\n",
      "* Epoch: 73, train loss: 9.537, val loss: 9.440, test loss: 9.177\n",
      "* Epoch: 74, train loss: 9.344, val loss: 9.233, test loss: 8.981\n",
      "* Epoch: 75, train loss: 9.147, val loss: 9.034, test loss: 8.793\n",
      "* Epoch: 76, train loss: 8.960, val loss: 8.842, test loss: 8.608\n",
      "* Epoch: 77, train loss: 8.739, val loss: 8.649, test loss: 8.424\n",
      "* Epoch: 78, train loss: 8.551, val loss: 8.462, test loss: 8.244\n",
      "* Epoch: 79, train loss: 8.358, val loss: 8.276, test loss: 8.066\n",
      "* Epoch: 80, train loss: 8.193, val loss: 8.094, test loss: 7.892\n",
      "* Epoch: 81, train loss: 8.010, val loss: 7.915, test loss: 7.720\n",
      "* Epoch: 82, train loss: 7.826, val loss: 7.738, test loss: 7.550\n",
      "* Epoch: 83, train loss: 7.649, val loss: 7.564, test loss: 7.392\n",
      "* Epoch: 84, train loss: 7.484, val loss: 7.394, test loss: 7.243\n",
      "* Epoch: 85, train loss: 7.315, val loss: 7.233, test loss: 7.096\n",
      "* Epoch: 86, train loss: 7.155, val loss: 7.075, test loss: 6.952\n",
      "* Epoch: 87, train loss: 6.997, val loss: 6.920, test loss: 6.811\n",
      "* Epoch: 88, train loss: 6.852, val loss: 6.766, test loss: 6.670\n",
      "* Epoch: 89, train loss: 6.699, val loss: 6.617, test loss: 6.534\n",
      "* Epoch: 90, train loss: 6.538, val loss: 6.473, test loss: 6.408\n",
      "* Epoch: 91, train loss: 6.401, val loss: 6.341, test loss: 6.289\n",
      "* Epoch: 92, train loss: 6.279, val loss: 6.214, test loss: 6.173\n",
      "* Epoch: 93, train loss: 6.126, val loss: 6.087, test loss: 6.058\n",
      "* Epoch: 94, train loss: 5.988, val loss: 5.960, test loss: 5.943\n",
      "* Epoch: 95, train loss: 5.868, val loss: 5.839, test loss: 5.834\n",
      "* Epoch: 96, train loss: 5.745, val loss: 5.721, test loss: 5.728\n",
      "* Epoch: 97, train loss: 5.630, val loss: 5.610, test loss: 5.628\n",
      "* Epoch: 98, train loss: 5.503, val loss: 5.490, test loss: 5.518\n",
      "* Epoch: 99, train loss: 5.389, val loss: 5.376, test loss: 5.413\n",
      "* Epoch: 100, train loss: 5.279, val loss: 5.290, test loss: 5.341\n",
      "* Epoch: 101, train loss: 5.158, val loss: 5.159, test loss: 5.217\n",
      "* Epoch: 102, train loss: 5.058, val loss: 5.055, test loss: 5.122\n",
      "* Epoch: 103, train loss: 4.962, val loss: 4.951, test loss: 5.033\n",
      "* Epoch: 104, train loss: 4.851, val loss: 4.852, test loss: 4.937\n",
      "* Epoch: 105, train loss: 4.757, val loss: 4.755, test loss: 4.834\n",
      "* Epoch: 106, train loss: 4.656, val loss: 4.668, test loss: 4.782\n",
      "* Epoch: 107, train loss: 4.571, val loss: 4.574, test loss: 4.687\n",
      "* Epoch: 108, train loss: 4.480, val loss: 4.479, test loss: 4.541\n",
      "* Epoch: 109, train loss: 4.400, val loss: 4.411, test loss: 4.554\n",
      "* Epoch: 110, train loss: 4.321, val loss: 4.323, test loss: 4.469\n",
      "* Epoch: 111, train loss: 4.232, val loss: 4.242, test loss: 4.394\n",
      "* Epoch: 112, train loss: 4.152, val loss: 4.162, test loss: 4.312\n",
      "* Epoch: 113, train loss: 4.077, val loss: 4.083, test loss: 4.242\n",
      "* Epoch: 114, train loss: 4.014, val loss: 4.009, test loss: 4.184\n",
      "* Epoch: 115, train loss: 3.937, val loss: 3.933, test loss: 4.098\n",
      "* Epoch: 116, train loss: 3.860, val loss: 3.865, test loss: 3.924\n",
      "* Epoch: 117, train loss: 3.803, val loss: 3.791, test loss: 3.963\n",
      "* Epoch: 118, train loss: 3.742, val loss: 3.731, test loss: 3.937\n",
      "* Epoch: 119, train loss: 3.669, val loss: 3.664, test loss: 3.875\n",
      "* Epoch: 120, train loss: 3.612, val loss: 3.597, test loss: 3.801\n",
      "* Epoch: 121, train loss: 3.541, val loss: 3.526, test loss: 3.572\n",
      "* Epoch: 122, train loss: 3.493, val loss: 3.486, test loss: 3.720\n",
      "* Epoch: 123, train loss: 3.454, val loss: 3.425, test loss: 3.668\n",
      "* Epoch: 124, train loss: 3.404, val loss: 3.369, test loss: 3.618\n",
      "* Epoch: 125, train loss: 3.365, val loss: 3.315, test loss: 3.559\n",
      "* Epoch: 126, train loss: 3.293, val loss: 3.259, test loss: 3.449\n",
      "* Epoch: 127, train loss: 3.229, val loss: 3.210, test loss: 3.320\n",
      "* Epoch: 128, train loss: 3.175, val loss: 3.160, test loss: 3.239\n",
      "* Epoch: 129, train loss: 3.121, val loss: 3.101, test loss: 3.243\n",
      "* Epoch: 130, train loss: 3.057, val loss: 3.040, test loss: 3.190\n",
      "  Epoch: 131, train loss: 3.044, val loss: 3.056, test loss: 3.321\n",
      "  Epoch: 132, train loss: 3.077, val loss: 3.058, test loss: 3.304\n",
      "* Epoch: 133, train loss: 3.042, val loss: 2.983, test loss: 3.257\n",
      "* Epoch: 134, train loss: 3.004, val loss: 2.944, test loss: 3.224\n",
      "* Epoch: 135, train loss: 2.970, val loss: 2.905, test loss: 3.184\n",
      "* Epoch: 136, train loss: 2.898, val loss: 2.824, test loss: 2.767\n",
      "* Epoch: 137, train loss: 2.879, val loss: 2.714, test loss: 2.856\n",
      "  Epoch: 138, train loss: 2.815, val loss: 2.740, test loss: 2.969\n",
      "  Epoch: 139, train loss: 2.800, val loss: 2.735, test loss: 3.030\n",
      "* Epoch: 140, train loss: 2.748, val loss: 2.636, test loss: 2.822\n",
      "* Epoch: 141, train loss: 2.730, val loss: 2.459, test loss: 2.680\n",
      "  Epoch: 142, train loss: 2.661, val loss: 2.565, test loss: 2.791\n",
      "  Epoch: 143, train loss: 2.623, val loss: 2.524, test loss: 2.811\n",
      "  Epoch: 144, train loss: 2.663, val loss: 2.472, test loss: 2.765\n",
      "* Epoch: 145, train loss: 2.538, val loss: 2.208, test loss: 2.702\n",
      "  Epoch: 146, train loss: 2.497, val loss: 2.246, test loss: 2.615\n",
      "  Epoch: 147, train loss: 2.454, val loss: 2.224, test loss: 2.564\n",
      "  Epoch: 148, train loss: 2.463, val loss: 2.259, test loss: 2.635\n",
      "* Epoch: 149, train loss: 2.446, val loss: 2.151, test loss: 2.626\n",
      "* Epoch: 150, train loss: 2.416, val loss: 2.082, test loss: 2.520\n",
      "* Epoch: 151, train loss: 2.360, val loss: 2.059, test loss: 2.512\n",
      "* Epoch: 152, train loss: 2.330, val loss: 2.018, test loss: 2.371\n",
      "  Epoch: 153, train loss: 2.306, val loss: 2.037, test loss: 2.302\n",
      "  Epoch: 154, train loss: 2.269, val loss: 2.036, test loss: 2.302\n",
      "* Epoch: 155, train loss: 2.250, val loss: 1.944, test loss: 2.193\n",
      "  Epoch: 156, train loss: 2.247, val loss: 2.040, test loss: 2.388\n",
      "  Epoch: 157, train loss: 2.234, val loss: 1.973, test loss: 2.268\n",
      "* Epoch: 158, train loss: 2.210, val loss: 1.916, test loss: 2.230\n",
      "* Epoch: 159, train loss: 2.159, val loss: 1.881, test loss: 2.130\n",
      "* Epoch: 160, train loss: 2.188, val loss: 1.876, test loss: 2.196\n",
      "  Epoch: 161, train loss: 2.127, val loss: 1.885, test loss: 1.980\n",
      "* Epoch: 162, train loss: 2.125, val loss: 1.842, test loss: 2.192\n",
      "  Epoch: 163, train loss: 2.087, val loss: 1.867, test loss: 2.228\n",
      "* Epoch: 164, train loss: 2.051, val loss: 1.820, test loss: 1.970\n",
      "* Epoch: 165, train loss: 2.016, val loss: 1.784, test loss: 1.984\n",
      "* Epoch: 166, train loss: 2.018, val loss: 1.771, test loss: 1.994\n",
      "* Epoch: 167, train loss: 2.021, val loss: 1.761, test loss: 2.002\n",
      "* Epoch: 168, train loss: 1.958, val loss: 1.723, test loss: 1.838\n",
      "* Epoch: 169, train loss: 1.995, val loss: 1.697, test loss: 1.836\n",
      "* Epoch: 170, train loss: 1.960, val loss: 1.680, test loss: 1.890\n",
      "* Epoch: 171, train loss: 1.950, val loss: 1.665, test loss: 1.789\n",
      "  Epoch: 172, train loss: 1.888, val loss: 1.700, test loss: 2.098\n",
      "* Epoch: 173, train loss: 1.927, val loss: 1.643, test loss: 1.656\n",
      "  Epoch: 174, train loss: 1.869, val loss: 1.660, test loss: 1.775\n",
      "* Epoch: 175, train loss: 1.910, val loss: 1.611, test loss: 1.717\n",
      "  Epoch: 176, train loss: 1.860, val loss: 1.643, test loss: 1.854\n",
      "* Epoch: 177, train loss: 1.822, val loss: 1.602, test loss: 1.666\n",
      "* Epoch: 178, train loss: 1.860, val loss: 1.584, test loss: 1.800\n",
      "  Epoch: 179, train loss: 1.821, val loss: 1.600, test loss: 1.759\n",
      "  Epoch: 180, train loss: 1.780, val loss: 1.589, test loss: 1.596\n",
      "* Epoch: 181, train loss: 1.851, val loss: 1.560, test loss: 1.713\n",
      "  Epoch: 182, train loss: 1.752, val loss: 1.586, test loss: 1.687\n",
      "* Epoch: 183, train loss: 1.809, val loss: 1.511, test loss: 1.442\n",
      "  Epoch: 184, train loss: 1.713, val loss: 1.601, test loss: 1.787\n",
      "  Epoch: 185, train loss: 1.701, val loss: 1.554, test loss: 1.780\n",
      "* Epoch: 186, train loss: 1.731, val loss: 1.497, test loss: 1.552\n",
      "  Epoch: 187, train loss: 1.718, val loss: 1.502, test loss: 1.601\n",
      "* Epoch: 188, train loss: 1.736, val loss: 1.476, test loss: 1.589\n",
      "  Epoch: 189, train loss: 1.697, val loss: 1.564, test loss: 1.734\n",
      "* Epoch: 190, train loss: 1.666, val loss: 1.472, test loss: 1.647\n",
      "  Epoch: 191, train loss: 1.692, val loss: 1.490, test loss: 1.612\n",
      "  Epoch: 192, train loss: 1.688, val loss: 1.511, test loss: 1.682\n",
      "* Epoch: 193, train loss: 1.649, val loss: 1.448, test loss: 1.557\n",
      "* Epoch: 194, train loss: 1.722, val loss: 1.408, test loss: 1.491\n",
      "  Epoch: 195, train loss: 1.732, val loss: 1.561, test loss: 1.876\n",
      "  Epoch: 196, train loss: 1.724, val loss: 1.458, test loss: 1.688\n",
      "  Epoch: 197, train loss: 1.695, val loss: 1.480, test loss: 1.693\n",
      "  Epoch: 198, train loss: 1.682, val loss: 1.544, test loss: 1.691\n",
      "  Epoch: 199, train loss: 1.625, val loss: 1.421, test loss: 1.437\n",
      "  Epoch: 200, train loss: 1.649, val loss: 1.451, test loss: 1.558\n",
      "  Epoch: 201, train loss: 1.611, val loss: 1.462, test loss: 1.712\n",
      "* Epoch: 202, train loss: 1.635, val loss: 1.401, test loss: 1.469\n",
      "  Epoch: 203, train loss: 1.603, val loss: 1.591, test loss: 1.688\n",
      "  Epoch: 204, train loss: 1.650, val loss: 1.429, test loss: 1.426\n",
      "* Epoch: 205, train loss: 1.644, val loss: 1.387, test loss: 1.629\n",
      "* Epoch: 206, train loss: 1.611, val loss: 1.375, test loss: 1.319\n",
      "  Epoch: 207, train loss: 1.622, val loss: 1.383, test loss: 1.530\n",
      "* Epoch: 208, train loss: 1.644, val loss: 1.370, test loss: 1.366\n",
      "  Epoch: 209, train loss: 1.588, val loss: 1.408, test loss: 1.472\n",
      "  Epoch: 210, train loss: 1.651, val loss: 1.405, test loss: 1.388\n",
      "  Epoch: 211, train loss: 1.599, val loss: 1.418, test loss: 1.531\n",
      "  Epoch: 212, train loss: 1.614, val loss: 1.429, test loss: 1.588\n",
      "  Epoch: 213, train loss: 1.582, val loss: 1.422, test loss: 1.404\n",
      "  Epoch: 214, train loss: 1.589, val loss: 1.439, test loss: 1.735\n",
      "  Epoch: 215, train loss: 1.606, val loss: 1.517, test loss: 1.687\n",
      "  Epoch: 216, train loss: 1.632, val loss: 1.451, test loss: 1.548\n",
      "  Epoch: 217, train loss: 1.596, val loss: 1.479, test loss: 1.839\n",
      "  Epoch: 218, train loss: 1.603, val loss: 1.431, test loss: 1.634\n",
      "  Epoch: 219, train loss: 1.554, val loss: 1.376, test loss: 1.447\n",
      "  Epoch: 220, train loss: 1.610, val loss: 1.413, test loss: 1.415\n",
      "  Epoch: 221, train loss: 1.566, val loss: 1.452, test loss: 1.574\n",
      "* Epoch: 222, train loss: 1.564, val loss: 1.341, test loss: 1.530\n",
      "  Epoch: 223, train loss: 1.567, val loss: 1.390, test loss: 1.629\n",
      "  Epoch: 224, train loss: 1.571, val loss: 1.447, test loss: 1.562\n",
      "* Epoch: 225, train loss: 1.579, val loss: 1.310, test loss: 1.350\n",
      "  Epoch: 226, train loss: 1.537, val loss: 1.311, test loss: 1.210\n",
      "  Epoch: 227, train loss: 1.550, val loss: 1.328, test loss: 1.372\n",
      "  Epoch: 228, train loss: 1.579, val loss: 1.327, test loss: 1.245\n",
      "  Epoch: 229, train loss: 1.564, val loss: 1.317, test loss: 1.357\n",
      "  Epoch: 230, train loss: 1.563, val loss: 1.385, test loss: 1.362\n",
      "  Epoch: 231, train loss: 1.522, val loss: 1.363, test loss: 1.488\n",
      "  Epoch: 232, train loss: 1.546, val loss: 1.316, test loss: 1.267\n",
      "  Epoch: 233, train loss: 1.566, val loss: 1.326, test loss: 1.333\n",
      "* Epoch: 234, train loss: 1.609, val loss: 1.307, test loss: 1.424\n",
      "  Epoch: 235, train loss: 1.556, val loss: 1.359, test loss: 1.475\n",
      "  Epoch: 236, train loss: 1.541, val loss: 1.312, test loss: 1.264\n",
      "  Epoch: 237, train loss: 1.590, val loss: 1.358, test loss: 1.422\n",
      "  Epoch: 238, train loss: 1.573, val loss: 1.372, test loss: 1.547\n",
      "  Epoch: 239, train loss: 1.571, val loss: 1.313, test loss: 1.492\n",
      "  Epoch: 240, train loss: 1.547, val loss: 1.309, test loss: 1.400\n",
      "  Epoch: 241, train loss: 1.587, val loss: 1.353, test loss: 1.276\n",
      "  Epoch: 242, train loss: 1.553, val loss: 1.342, test loss: 1.435\n",
      "  Epoch: 243, train loss: 1.567, val loss: 1.330, test loss: 1.342\n",
      "  Epoch: 244, train loss: 1.486, val loss: 1.349, test loss: 1.433\n",
      "  Epoch: 245, train loss: 1.503, val loss: 1.336, test loss: 1.405\n",
      "  Epoch: 246, train loss: 1.513, val loss: 1.371, test loss: 1.415\n",
      "  Epoch: 247, train loss: 1.517, val loss: 1.352, test loss: 1.316\n",
      "* Epoch: 248, train loss: 1.524, val loss: 1.283, test loss: 1.390\n",
      "  Epoch: 249, train loss: 1.498, val loss: 1.374, test loss: 1.384\n",
      "  Epoch: 250, train loss: 1.539, val loss: 1.385, test loss: 1.489\n",
      "  Epoch: 251, train loss: 1.518, val loss: 1.360, test loss: 1.294\n",
      "  Epoch: 252, train loss: 1.497, val loss: 1.318, test loss: 1.240\n",
      "  Epoch: 253, train loss: 1.508, val loss: 1.424, test loss: 1.587\n",
      "  Epoch: 254, train loss: 1.558, val loss: 1.343, test loss: 1.378\n",
      "  Epoch: 255, train loss: 1.528, val loss: 1.359, test loss: 1.236\n",
      "  Epoch: 256, train loss: 1.533, val loss: 1.397, test loss: 1.321\n",
      "  Epoch: 257, train loss: 1.572, val loss: 1.343, test loss: 1.352\n",
      "  Epoch: 258, train loss: 1.487, val loss: 1.399, test loss: 1.544\n",
      "  Epoch: 259, train loss: 1.551, val loss: 1.397, test loss: 1.494\n",
      "  Epoch: 260, train loss: 1.467, val loss: 1.367, test loss: 1.356\n",
      "  Epoch: 261, train loss: 1.514, val loss: 1.333, test loss: 1.323\n",
      "  Epoch: 262, train loss: 1.529, val loss: 1.349, test loss: 1.336\n",
      "  Epoch: 263, train loss: 1.522, val loss: 1.478, test loss: 1.585\n",
      "  Epoch: 264, train loss: 1.504, val loss: 1.423, test loss: 1.655\n",
      "  Epoch: 265, train loss: 1.502, val loss: 1.326, test loss: 1.402\n",
      "  Epoch: 266, train loss: 1.512, val loss: 1.339, test loss: 1.339\n",
      "  Epoch: 267, train loss: 1.546, val loss: 1.382, test loss: 1.546\n",
      "  Epoch: 268, train loss: 1.549, val loss: 1.328, test loss: 1.290\n",
      "  Epoch: 269, train loss: 1.482, val loss: 1.353, test loss: 1.371\n",
      "  Epoch: 270, train loss: 1.465, val loss: 1.335, test loss: 1.320\n",
      "  Epoch: 271, train loss: 1.514, val loss: 1.341, test loss: 1.336\n",
      "  Epoch: 272, train loss: 1.445, val loss: 1.360, test loss: 1.316\n",
      "  Epoch: 273, train loss: 1.487, val loss: 1.449, test loss: 1.537\n",
      "  Epoch: 274, train loss: 1.518, val loss: 1.351, test loss: 1.493\n",
      "  Epoch: 275, train loss: 1.500, val loss: 1.335, test loss: 1.406\n",
      "  Epoch: 276, train loss: 1.528, val loss: 1.307, test loss: 1.311\n",
      "  Epoch: 277, train loss: 1.501, val loss: 1.330, test loss: 1.326\n",
      "  Epoch: 278, train loss: 1.484, val loss: 1.322, test loss: 1.392\n",
      "  Epoch: 279, train loss: 1.471, val loss: 1.314, test loss: 1.425\n",
      "  Epoch: 280, train loss: 1.512, val loss: 1.383, test loss: 1.304\n",
      "  Epoch: 281, train loss: 1.475, val loss: 1.344, test loss: 1.349\n",
      "  Epoch: 282, train loss: 1.463, val loss: 1.327, test loss: 1.400\n",
      "* Epoch: 283, train loss: 1.516, val loss: 1.267, test loss: 1.349\n",
      "  Epoch: 284, train loss: 1.474, val loss: 1.302, test loss: 1.365\n",
      "  Epoch: 285, train loss: 1.498, val loss: 1.300, test loss: 1.363\n",
      "  Epoch: 286, train loss: 1.485, val loss: 1.353, test loss: 1.464\n",
      "  Epoch: 287, train loss: 1.464, val loss: 1.331, test loss: 1.428\n",
      "  Epoch: 288, train loss: 1.448, val loss: 1.368, test loss: 1.363\n",
      "  Epoch: 289, train loss: 1.462, val loss: 1.322, test loss: 1.329\n",
      "  Epoch: 290, train loss: 1.481, val loss: 1.309, test loss: 1.442\n",
      "  Epoch: 291, train loss: 1.476, val loss: 1.299, test loss: 1.294\n",
      "  Epoch: 292, train loss: 1.480, val loss: 1.358, test loss: 1.436\n",
      "  Epoch: 293, train loss: 1.486, val loss: 1.320, test loss: 1.333\n",
      "  Epoch: 294, train loss: 1.474, val loss: 1.355, test loss: 1.550\n",
      "  Epoch: 295, train loss: 1.462, val loss: 1.367, test loss: 1.507\n",
      "  Epoch: 296, train loss: 1.425, val loss: 1.397, test loss: 1.505\n",
      "  Epoch: 297, train loss: 1.490, val loss: 1.416, test loss: 1.762\n",
      "  Epoch: 298, train loss: 1.451, val loss: 1.375, test loss: 1.344\n",
      "  Epoch: 299, train loss: 1.432, val loss: 1.385, test loss: 1.425\n",
      "  Epoch: 300, train loss: 1.447, val loss: 1.494, test loss: 1.596\n",
      "  Epoch: 301, train loss: 1.588, val loss: 1.407, test loss: 1.731\n",
      "  Epoch: 302, train loss: 1.463, val loss: 1.331, test loss: 1.446\n",
      "  Epoch: 303, train loss: 1.477, val loss: 1.355, test loss: 1.315\n",
      "  Epoch: 304, train loss: 1.500, val loss: 1.342, test loss: 1.385\n",
      "  Epoch: 305, train loss: 1.430, val loss: 1.436, test loss: 1.514\n",
      "  Epoch: 306, train loss: 1.462, val loss: 1.325, test loss: 1.362\n",
      "  Epoch: 307, train loss: 1.474, val loss: 1.343, test loss: 1.465\n",
      "  Epoch: 308, train loss: 1.476, val loss: 1.352, test loss: 1.413\n",
      "  Epoch: 309, train loss: 1.456, val loss: 1.340, test loss: 1.404\n",
      "  Epoch: 310, train loss: 1.424, val loss: 1.376, test loss: 1.548\n",
      "  Epoch: 311, train loss: 1.446, val loss: 1.318, test loss: 1.366\n",
      "  Epoch: 312, train loss: 1.445, val loss: 1.388, test loss: 1.415\n",
      "  Epoch: 313, train loss: 1.447, val loss: 1.393, test loss: 1.513\n",
      "  Epoch: 314, train loss: 1.440, val loss: 1.375, test loss: 1.412\n",
      "  Epoch: 315, train loss: 1.413, val loss: 1.318, test loss: 1.340\n",
      "  Epoch: 316, train loss: 1.505, val loss: 1.390, test loss: 1.490\n",
      "  Epoch: 317, train loss: 1.507, val loss: 1.683, test loss: 1.841\n",
      "  Epoch: 318, train loss: 1.473, val loss: 1.359, test loss: 1.464\n",
      "  Epoch: 319, train loss: 1.415, val loss: 1.356, test loss: 1.553\n",
      "  Epoch: 320, train loss: 1.458, val loss: 1.378, test loss: 1.402\n",
      "  Epoch: 321, train loss: 1.458, val loss: 1.362, test loss: 1.448\n",
      "  Epoch: 322, train loss: 1.438, val loss: 1.359, test loss: 1.516\n",
      "  Epoch: 323, train loss: 1.435, val loss: 1.327, test loss: 1.356\n",
      "  Epoch: 324, train loss: 1.422, val loss: 1.344, test loss: 1.437\n",
      "  Epoch: 325, train loss: 1.482, val loss: 1.321, test loss: 1.306\n",
      "  Epoch: 326, train loss: 1.476, val loss: 1.289, test loss: 1.321\n",
      "  Epoch: 327, train loss: 1.447, val loss: 1.297, test loss: 1.379\n",
      "  Epoch: 328, train loss: 1.413, val loss: 1.363, test loss: 1.462\n",
      "  Epoch: 329, train loss: 1.436, val loss: 1.374, test loss: 1.560\n",
      "  Epoch: 330, train loss: 1.428, val loss: 1.365, test loss: 1.586\n",
      "  Epoch: 331, train loss: 1.439, val loss: 1.299, test loss: 1.399\n",
      "  Epoch: 332, train loss: 1.392, val loss: 1.343, test loss: 1.313\n",
      "  Epoch: 333, train loss: 1.430, val loss: 1.293, test loss: 1.418\n",
      "  Epoch: 334, train loss: 1.489, val loss: 1.316, test loss: 1.495\n",
      "  Epoch: 335, train loss: 1.400, val loss: 1.300, test loss: 1.416\n",
      "  Epoch: 336, train loss: 1.388, val loss: 1.308, test loss: 1.338\n",
      "  Epoch: 337, train loss: 1.442, val loss: 1.292, test loss: 1.337\n",
      "  Epoch: 338, train loss: 1.459, val loss: 1.359, test loss: 1.568\n",
      "  Epoch: 339, train loss: 1.449, val loss: 1.312, test loss: 1.308\n",
      "  Epoch: 340, train loss: 1.423, val loss: 1.393, test loss: 1.590\n",
      "  Epoch: 341, train loss: 1.460, val loss: 1.365, test loss: 1.506\n",
      "  Epoch: 342, train loss: 1.409, val loss: 1.305, test loss: 1.418\n",
      "  Epoch: 343, train loss: 1.416, val loss: 1.377, test loss: 1.455\n",
      "  Epoch: 344, train loss: 1.415, val loss: 1.337, test loss: 1.277\n",
      "  Epoch: 345, train loss: 1.457, val loss: 1.500, test loss: 1.772\n",
      "  Epoch: 346, train loss: 1.485, val loss: 1.394, test loss: 1.621\n",
      "  Epoch: 347, train loss: 1.444, val loss: 1.398, test loss: 1.481\n",
      "  Epoch: 348, train loss: 1.404, val loss: 1.407, test loss: 1.507\n",
      "  Epoch: 349, train loss: 1.404, val loss: 1.330, test loss: 1.399\n",
      "  Epoch: 350, train loss: 1.411, val loss: 1.299, test loss: 1.341\n",
      "  Epoch: 351, train loss: 1.423, val loss: 1.296, test loss: 1.318\n",
      "  Epoch: 352, train loss: 1.388, val loss: 1.309, test loss: 1.481\n",
      "  Epoch: 353, train loss: 1.439, val loss: 1.421, test loss: 1.374\n",
      "  Epoch: 354, train loss: 1.427, val loss: 1.305, test loss: 1.392\n",
      "  Epoch: 355, train loss: 1.441, val loss: 1.303, test loss: 1.374\n",
      "  Epoch: 356, train loss: 1.388, val loss: 1.309, test loss: 1.506\n",
      "  Epoch: 357, train loss: 1.401, val loss: 1.348, test loss: 1.402\n",
      "  Epoch: 358, train loss: 1.406, val loss: 1.343, test loss: 1.337\n",
      "  Epoch: 359, train loss: 1.411, val loss: 1.411, test loss: 1.454\n",
      "  Epoch: 360, train loss: 1.456, val loss: 1.444, test loss: 1.518\n",
      "  Epoch: 361, train loss: 1.390, val loss: 1.304, test loss: 1.365\n",
      "  Epoch: 362, train loss: 1.390, val loss: 1.318, test loss: 1.408\n",
      "  Epoch: 363, train loss: 1.398, val loss: 1.353, test loss: 1.348\n",
      "  Epoch: 364, train loss: 1.425, val loss: 1.335, test loss: 1.397\n",
      "  Epoch: 365, train loss: 1.484, val loss: 1.405, test loss: 1.352\n",
      "  Epoch: 366, train loss: 1.437, val loss: 1.337, test loss: 1.291\n",
      "  Epoch: 367, train loss: 1.391, val loss: 1.306, test loss: 1.444\n",
      "  Epoch: 368, train loss: 1.402, val loss: 1.377, test loss: 1.336\n",
      "  Epoch: 369, train loss: 1.377, val loss: 1.411, test loss: 1.402\n",
      "  Epoch: 370, train loss: 1.402, val loss: 1.320, test loss: 1.366\n",
      "  Epoch: 371, train loss: 1.379, val loss: 1.335, test loss: 1.344\n",
      "  Epoch: 372, train loss: 1.388, val loss: 1.316, test loss: 1.346\n",
      "  Epoch: 373, train loss: 1.413, val loss: 1.353, test loss: 1.482\n",
      "  Epoch: 374, train loss: 1.398, val loss: 1.320, test loss: 1.424\n",
      "  Epoch: 375, train loss: 1.383, val loss: 1.327, test loss: 1.540\n",
      "  Epoch: 376, train loss: 1.389, val loss: 1.383, test loss: 1.426\n",
      "  Epoch: 377, train loss: 1.423, val loss: 1.345, test loss: 1.343\n",
      "  Epoch: 378, train loss: 1.389, val loss: 1.319, test loss: 1.364\n",
      "  Epoch: 379, train loss: 1.395, val loss: 1.350, test loss: 1.451\n",
      "  Epoch: 380, train loss: 1.397, val loss: 1.644, test loss: 1.763\n",
      "  Epoch: 381, train loss: 1.433, val loss: 1.346, test loss: 1.402\n",
      "  Epoch: 382, train loss: 1.420, val loss: 1.330, test loss: 1.361\n",
      "  Epoch: 383, train loss: 1.430, val loss: 1.307, test loss: 1.324\n",
      "  Epoch: 384, train loss: 1.389, val loss: 1.347, test loss: 1.325\n",
      "  Epoch: 385, train loss: 1.382, val loss: 1.320, test loss: 1.458\n",
      "  Epoch: 386, train loss: 1.416, val loss: 1.339, test loss: 1.417\n",
      "  Epoch: 387, train loss: 1.373, val loss: 1.320, test loss: 1.484\n",
      "  Epoch: 388, train loss: 1.394, val loss: 1.462, test loss: 1.339\n",
      "  Epoch: 389, train loss: 1.417, val loss: 1.292, test loss: 1.352\n",
      "  Epoch: 390, train loss: 1.366, val loss: 1.349, test loss: 1.395\n",
      "  Epoch: 391, train loss: 1.335, val loss: 1.320, test loss: 1.406\n",
      "  Epoch: 392, train loss: 1.394, val loss: 1.333, test loss: 1.335\n",
      "  Epoch: 393, train loss: 1.374, val loss: 1.307, test loss: 1.448\n",
      "  Epoch: 394, train loss: 1.397, val loss: 1.359, test loss: 1.459\n",
      "  Epoch: 395, train loss: 1.383, val loss: 1.324, test loss: 1.256\n",
      "  Epoch: 396, train loss: 1.375, val loss: 1.343, test loss: 1.348\n",
      "  Epoch: 397, train loss: 1.369, val loss: 1.339, test loss: 1.258\n",
      "  Epoch: 398, train loss: 1.362, val loss: 1.305, test loss: 1.339\n",
      "  Epoch: 399, train loss: 1.400, val loss: 1.307, test loss: 1.300\n",
      "  Epoch: 400, train loss: 1.392, val loss: 1.382, test loss: 1.323\n",
      "  Epoch: 401, train loss: 1.339, val loss: 1.328, test loss: 1.342\n",
      "  Epoch: 402, train loss: 1.350, val loss: 1.339, test loss: 1.347\n",
      "  Epoch: 403, train loss: 1.380, val loss: 1.326, test loss: 1.300\n",
      "  Epoch: 404, train loss: 1.363, val loss: 1.331, test loss: 1.319\n",
      "  Epoch: 405, train loss: 1.352, val loss: 1.353, test loss: 1.307\n",
      "  Epoch: 406, train loss: 1.371, val loss: 1.350, test loss: 1.455\n",
      "  Epoch: 407, train loss: 1.349, val loss: 1.406, test loss: 1.394\n",
      "  Epoch: 408, train loss: 1.360, val loss: 1.340, test loss: 1.383\n",
      "  Epoch: 409, train loss: 1.384, val loss: 1.520, test loss: 1.501\n",
      "  Epoch: 410, train loss: 1.350, val loss: 1.405, test loss: 1.437\n",
      "  Epoch: 411, train loss: 1.366, val loss: 1.329, test loss: 1.271\n",
      "  Epoch: 412, train loss: 1.392, val loss: 1.324, test loss: 1.220\n",
      "  Epoch: 413, train loss: 1.350, val loss: 1.381, test loss: 1.330\n",
      "  Epoch: 414, train loss: 1.389, val loss: 1.362, test loss: 1.478\n",
      "  Epoch: 415, train loss: 1.386, val loss: 1.388, test loss: 1.646\n",
      "  Epoch: 416, train loss: 1.386, val loss: 1.343, test loss: 1.272\n",
      "  Epoch: 417, train loss: 1.398, val loss: 1.400, test loss: 1.313\n",
      "  Epoch: 418, train loss: 1.360, val loss: 1.299, test loss: 1.439\n",
      "  Epoch: 419, train loss: 1.340, val loss: 1.322, test loss: 1.301\n",
      "  Epoch: 420, train loss: 1.383, val loss: 1.310, test loss: 1.318\n",
      "  Epoch: 421, train loss: 1.341, val loss: 1.412, test loss: 1.393\n",
      "  Epoch: 422, train loss: 1.427, val loss: 1.399, test loss: 1.380\n",
      "  Epoch: 423, train loss: 1.354, val loss: 1.363, test loss: 1.497\n",
      "  Epoch: 424, train loss: 1.343, val loss: 1.315, test loss: 1.447\n",
      "  Epoch: 425, train loss: 1.306, val loss: 1.314, test loss: 1.307\n",
      "  Epoch: 426, train loss: 1.367, val loss: 1.339, test loss: 1.357\n",
      "  Epoch: 427, train loss: 1.378, val loss: 1.533, test loss: 1.495\n",
      "  Epoch: 428, train loss: 1.428, val loss: 1.458, test loss: 1.500\n",
      "  Epoch: 429, train loss: 1.425, val loss: 1.430, test loss: 1.395\n",
      "  Epoch: 430, train loss: 1.332, val loss: 1.310, test loss: 1.275\n",
      "  Epoch: 431, train loss: 1.374, val loss: 1.337, test loss: 1.430\n",
      "  Epoch: 432, train loss: 1.397, val loss: 1.560, test loss: 1.528\n",
      "  Epoch: 433, train loss: 1.317, val loss: 1.327, test loss: 1.348\n",
      "  Epoch: 434, train loss: 1.379, val loss: 1.410, test loss: 1.489\n",
      "  Epoch: 435, train loss: 1.317, val loss: 1.360, test loss: 1.397\n",
      "  Epoch: 436, train loss: 1.380, val loss: 1.355, test loss: 1.388\n",
      "  Epoch: 437, train loss: 1.366, val loss: 1.389, test loss: 1.423\n",
      "  Epoch: 438, train loss: 1.393, val loss: 1.366, test loss: 1.499\n",
      "  Epoch: 439, train loss: 1.373, val loss: 1.367, test loss: 1.424\n",
      "  Epoch: 440, train loss: 1.350, val loss: 1.339, test loss: 1.296\n",
      "  Epoch: 441, train loss: 1.348, val loss: 1.378, test loss: 1.346\n",
      "  Epoch: 442, train loss: 1.340, val loss: 1.298, test loss: 1.326\n",
      "  Epoch: 443, train loss: 1.378, val loss: 1.406, test loss: 1.320\n",
      "  Epoch: 444, train loss: 1.374, val loss: 1.400, test loss: 1.270\n",
      "  Epoch: 445, train loss: 1.345, val loss: 1.335, test loss: 1.438\n",
      "  Epoch: 446, train loss: 1.364, val loss: 1.347, test loss: 1.297\n",
      "  Epoch: 447, train loss: 1.363, val loss: 1.468, test loss: 1.310\n",
      "  Epoch: 448, train loss: 1.341, val loss: 1.355, test loss: 1.277\n",
      "  Epoch: 449, train loss: 1.401, val loss: 1.400, test loss: 1.290\n",
      "  Epoch: 450, train loss: 1.356, val loss: 1.330, test loss: 1.338\n",
      "  Epoch: 451, train loss: 1.329, val loss: 1.393, test loss: 1.339\n",
      "  Epoch: 452, train loss: 1.335, val loss: 1.363, test loss: 1.429\n",
      "  Epoch: 453, train loss: 1.311, val loss: 1.356, test loss: 1.271\n",
      "  Epoch: 454, train loss: 1.342, val loss: 1.339, test loss: 1.291\n",
      "  Epoch: 455, train loss: 1.405, val loss: 1.359, test loss: 1.400\n",
      "  Epoch: 456, train loss: 1.390, val loss: 1.503, test loss: 1.365\n",
      "  Epoch: 457, train loss: 1.359, val loss: 1.361, test loss: 1.484\n",
      "  Epoch: 458, train loss: 1.384, val loss: 1.368, test loss: 1.221\n",
      "  Epoch: 459, train loss: 1.390, val loss: 1.359, test loss: 1.211\n",
      "  Epoch: 460, train loss: 1.334, val loss: 1.461, test loss: 1.421\n",
      "  Epoch: 461, train loss: 1.350, val loss: 1.403, test loss: 1.398\n",
      "  Epoch: 462, train loss: 1.347, val loss: 1.327, test loss: 1.354\n",
      "  Epoch: 463, train loss: 1.359, val loss: 1.377, test loss: 1.208\n",
      "  Epoch: 464, train loss: 1.389, val loss: 1.358, test loss: 1.289\n",
      "  Epoch: 465, train loss: 1.304, val loss: 1.333, test loss: 1.357\n",
      "  Epoch: 466, train loss: 1.300, val loss: 1.413, test loss: 1.520\n",
      "  Epoch: 467, train loss: 1.384, val loss: 1.463, test loss: 1.409\n",
      "  Epoch: 468, train loss: 1.360, val loss: 1.386, test loss: 1.519\n",
      "  Epoch: 469, train loss: 1.380, val loss: 1.382, test loss: 1.320\n",
      "  Epoch: 470, train loss: 1.346, val loss: 1.463, test loss: 1.200\n",
      "  Epoch: 471, train loss: 1.322, val loss: 1.411, test loss: 1.230\n",
      "  Epoch: 472, train loss: 1.400, val loss: 1.496, test loss: 1.526\n",
      "  Epoch: 473, train loss: 1.427, val loss: 1.325, test loss: 1.300\n",
      "  Epoch: 474, train loss: 1.373, val loss: 1.395, test loss: 1.197\n",
      "  Epoch: 475, train loss: 1.324, val loss: 1.346, test loss: 1.355\n",
      "  Epoch: 476, train loss: 1.348, val loss: 1.363, test loss: 1.311\n",
      "  Epoch: 477, train loss: 1.330, val loss: 1.339, test loss: 1.256\n",
      "  Epoch: 478, train loss: 1.337, val loss: 1.397, test loss: 1.305\n",
      "  Epoch: 479, train loss: 1.340, val loss: 1.400, test loss: 1.462\n",
      "  Epoch: 480, train loss: 1.309, val loss: 1.398, test loss: 1.320\n",
      "  Epoch: 481, train loss: 1.391, val loss: 1.416, test loss: 1.447\n",
      "  Epoch: 482, train loss: 1.451, val loss: 1.569, test loss: 1.573\n",
      "  Epoch: 483, train loss: 1.348, val loss: 1.376, test loss: 1.309\n",
      "  Epoch: 484, train loss: 1.355, val loss: 1.444, test loss: 1.485\n",
      "  Epoch: 485, train loss: 1.363, val loss: 1.343, test loss: 1.266\n",
      "  Epoch: 486, train loss: 1.341, val loss: 1.311, test loss: 1.386\n",
      "  Epoch: 487, train loss: 1.358, val loss: 1.410, test loss: 1.391\n",
      "  Epoch: 488, train loss: 1.316, val loss: 1.369, test loss: 1.266\n",
      "  Epoch: 489, train loss: 1.314, val loss: 1.375, test loss: 1.229\n",
      "  Epoch: 490, train loss: 1.293, val loss: 1.322, test loss: 1.246\n",
      "  Epoch: 491, train loss: 1.325, val loss: 1.369, test loss: 1.285\n",
      "  Epoch: 492, train loss: 1.343, val loss: 1.409, test loss: 1.283\n",
      "  Epoch: 493, train loss: 1.370, val loss: 1.414, test loss: 1.362\n",
      "  Epoch: 494, train loss: 1.322, val loss: 1.342, test loss: 1.358\n",
      "  Epoch: 495, train loss: 1.326, val loss: 1.355, test loss: 1.221\n",
      "  Epoch: 496, train loss: 1.312, val loss: 1.417, test loss: 1.192\n",
      "  Epoch: 497, train loss: 1.316, val loss: 1.328, test loss: 1.312\n",
      "  Epoch: 498, train loss: 1.324, val loss: 1.397, test loss: 1.319\n",
      "  Epoch: 499, train loss: 1.366, val loss: 1.370, test loss: 1.268\n",
      "  Epoch: 500, train loss: 1.324, val loss: 1.401, test loss: 1.345\n",
      "  Epoch: 501, train loss: 1.344, val loss: 1.492, test loss: 1.574\n",
      "  Epoch: 502, train loss: 1.333, val loss: 1.380, test loss: 1.375\n",
      "  Epoch: 503, train loss: 1.348, val loss: 1.330, test loss: 1.353\n",
      "  Epoch: 504, train loss: 1.334, val loss: 1.388, test loss: 1.366\n",
      "  Epoch: 505, train loss: 1.341, val loss: 1.374, test loss: 1.486\n",
      "  Epoch: 506, train loss: 1.322, val loss: 1.411, test loss: 1.190\n",
      "  Epoch: 507, train loss: 1.347, val loss: 1.525, test loss: 1.557\n",
      "  Epoch: 508, train loss: 1.322, val loss: 1.455, test loss: 1.193\n",
      "  Epoch: 509, train loss: 1.329, val loss: 1.353, test loss: 1.238\n",
      "  Epoch: 510, train loss: 1.305, val loss: 1.355, test loss: 1.328\n",
      "  Epoch: 511, train loss: 1.284, val loss: 1.442, test loss: 1.296\n",
      "  Epoch: 512, train loss: 1.340, val loss: 1.375, test loss: 1.178\n",
      "  Epoch: 513, train loss: 1.300, val loss: 1.338, test loss: 1.309\n",
      "  Epoch: 514, train loss: 1.322, val loss: 1.357, test loss: 1.297\n",
      "  Epoch: 515, train loss: 1.312, val loss: 1.420, test loss: 1.381\n",
      "  Epoch: 516, train loss: 1.311, val loss: 1.320, test loss: 1.300\n",
      "  Epoch: 517, train loss: 1.324, val loss: 1.421, test loss: 1.224\n",
      "  Epoch: 518, train loss: 1.327, val loss: 1.375, test loss: 1.480\n",
      "  Epoch: 519, train loss: 1.332, val loss: 1.317, test loss: 1.301\n",
      "  Epoch: 520, train loss: 1.324, val loss: 1.333, test loss: 1.238\n",
      "  Epoch: 521, train loss: 1.360, val loss: 1.441, test loss: 1.307\n",
      "  Epoch: 522, train loss: 1.322, val loss: 1.363, test loss: 1.477\n",
      "  Epoch: 523, train loss: 1.315, val loss: 1.567, test loss: 1.469\n",
      "  Epoch: 524, train loss: 1.302, val loss: 1.380, test loss: 1.236\n",
      "  Epoch: 525, train loss: 1.401, val loss: 1.358, test loss: 1.203\n",
      "  Epoch: 526, train loss: 1.402, val loss: 1.402, test loss: 1.351\n",
      "  Epoch: 527, train loss: 1.311, val loss: 1.403, test loss: 1.214\n",
      "  Epoch: 528, train loss: 1.310, val loss: 1.444, test loss: 1.520\n",
      "  Epoch: 529, train loss: 1.354, val loss: 1.417, test loss: 1.242\n",
      "  Epoch: 530, train loss: 1.345, val loss: 1.427, test loss: 1.400\n",
      "  Epoch: 531, train loss: 1.308, val loss: 1.387, test loss: 1.449\n",
      "  Epoch: 532, train loss: 1.326, val loss: 1.363, test loss: 1.396\n",
      "  Epoch: 533, train loss: 1.289, val loss: 1.460, test loss: 1.367\n",
      "  Epoch: 534, train loss: 1.346, val loss: 1.408, test loss: 1.315\n",
      "  Epoch: 535, train loss: 1.385, val loss: 1.454, test loss: 1.435\n",
      "  Epoch: 536, train loss: 1.320, val loss: 1.445, test loss: 1.454\n",
      "  Epoch: 537, train loss: 1.388, val loss: 1.424, test loss: 1.501\n",
      "  Epoch: 538, train loss: 1.334, val loss: 1.444, test loss: 1.299\n",
      "  Epoch: 539, train loss: 1.292, val loss: 1.372, test loss: 1.343\n",
      "  Epoch: 540, train loss: 1.332, val loss: 1.507, test loss: 1.390\n",
      "  Epoch: 541, train loss: 1.344, val loss: 1.482, test loss: 1.444\n",
      "  Epoch: 542, train loss: 1.291, val loss: 1.601, test loss: 1.605\n",
      "  Epoch: 543, train loss: 1.333, val loss: 1.392, test loss: 1.284\n",
      "  Epoch: 544, train loss: 1.332, val loss: 1.413, test loss: 1.281\n",
      "  Epoch: 545, train loss: 1.311, val loss: 1.522, test loss: 1.303\n",
      "  Epoch: 546, train loss: 1.349, val loss: 1.527, test loss: 1.606\n",
      "  Epoch: 547, train loss: 1.355, val loss: 1.398, test loss: 1.465\n",
      "  Epoch: 548, train loss: 1.306, val loss: 1.516, test loss: 1.551\n",
      "  Epoch: 549, train loss: 1.288, val loss: 1.412, test loss: 1.235\n",
      "  Epoch: 550, train loss: 1.244, val loss: 1.381, test loss: 1.382\n",
      "  Epoch: 551, train loss: 1.293, val loss: 1.371, test loss: 1.199\n",
      "  Epoch: 552, train loss: 1.337, val loss: 1.498, test loss: 1.471\n",
      "  Epoch: 553, train loss: 1.310, val loss: 1.426, test loss: 1.275\n",
      "  Epoch: 554, train loss: 1.280, val loss: 1.428, test loss: 1.340\n",
      "  Epoch: 555, train loss: 1.344, val loss: 1.372, test loss: 1.356\n",
      "  Epoch: 556, train loss: 1.297, val loss: 1.448, test loss: 1.562\n",
      "  Epoch: 557, train loss: 1.300, val loss: 1.394, test loss: 1.219\n",
      "  Epoch: 558, train loss: 1.292, val loss: 1.392, test loss: 1.312\n",
      "  Epoch: 559, train loss: 1.285, val loss: 1.432, test loss: 1.292\n",
      "  Epoch: 560, train loss: 1.374, val loss: 1.576, test loss: 1.448\n",
      "  Epoch: 561, train loss: 1.392, val loss: 1.441, test loss: 1.320\n",
      "  Epoch: 562, train loss: 1.366, val loss: 1.353, test loss: 1.331\n",
      "  Epoch: 563, train loss: 1.284, val loss: 1.347, test loss: 1.160\n",
      "  Epoch: 564, train loss: 1.315, val loss: 1.380, test loss: 1.211\n",
      "  Epoch: 565, train loss: 1.291, val loss: 1.393, test loss: 1.408\n",
      "  Epoch: 566, train loss: 1.265, val loss: 1.353, test loss: 1.292\n",
      "  Epoch: 567, train loss: 1.270, val loss: 1.528, test loss: 1.251\n",
      "  Epoch: 568, train loss: 1.339, val loss: 1.398, test loss: 1.156\n",
      "  Epoch: 569, train loss: 1.277, val loss: 1.558, test loss: 1.207\n",
      "  Epoch: 570, train loss: 1.337, val loss: 1.329, test loss: 1.260\n",
      "  Epoch: 571, train loss: 1.243, val loss: 1.447, test loss: 1.300\n",
      "  Epoch: 572, train loss: 1.303, val loss: 1.383, test loss: 1.296\n",
      "  Epoch: 573, train loss: 1.307, val loss: 1.354, test loss: 1.247\n",
      "  Epoch: 574, train loss: 1.298, val loss: 1.440, test loss: 1.336\n",
      "  Epoch: 575, train loss: 1.289, val loss: 1.346, test loss: 1.218\n",
      "  Epoch: 576, train loss: 1.343, val loss: 1.419, test loss: 1.215\n",
      "  Epoch: 577, train loss: 1.301, val loss: 1.494, test loss: 1.537\n",
      "  Epoch: 578, train loss: 1.305, val loss: 1.472, test loss: 1.395\n",
      "  Epoch: 579, train loss: 1.315, val loss: 1.437, test loss: 1.294\n",
      "  Epoch: 580, train loss: 1.268, val loss: 1.402, test loss: 1.307\n",
      "  Epoch: 581, train loss: 1.324, val loss: 1.510, test loss: 1.329\n",
      "  Epoch: 582, train loss: 1.329, val loss: 1.470, test loss: 1.228\n",
      "  Epoch: 583, train loss: 1.321, val loss: 1.551, test loss: 1.353\n",
      "  Epoch: 584, train loss: 1.330, val loss: 1.397, test loss: 1.315\n",
      "  Epoch: 585, train loss: 1.304, val loss: 1.530, test loss: 1.438\n",
      "  Epoch: 586, train loss: 1.310, val loss: 1.507, test loss: 1.579\n",
      "  Epoch: 587, train loss: 1.367, val loss: 1.438, test loss: 1.207\n",
      "  Epoch: 588, train loss: 1.331, val loss: 1.427, test loss: 1.192\n",
      "  Epoch: 589, train loss: 1.370, val loss: 1.375, test loss: 1.382\n",
      "  Epoch: 590, train loss: 1.347, val loss: 1.507, test loss: 1.466\n",
      "  Epoch: 591, train loss: 1.300, val loss: 1.525, test loss: 1.580\n",
      "  Epoch: 592, train loss: 1.327, val loss: 1.431, test loss: 1.313\n",
      "  Epoch: 593, train loss: 1.283, val loss: 1.436, test loss: 1.282\n",
      "  Epoch: 594, train loss: 1.268, val loss: 1.397, test loss: 1.300\n",
      "  Epoch: 595, train loss: 1.275, val loss: 1.393, test loss: 1.221\n",
      "  Epoch: 596, train loss: 1.306, val loss: 1.363, test loss: 1.156\n",
      "  Epoch: 597, train loss: 1.245, val loss: 1.406, test loss: 1.209\n",
      "  Epoch: 598, train loss: 1.294, val loss: 1.424, test loss: 1.278\n",
      "  Epoch: 599, train loss: 1.343, val loss: 1.528, test loss: 1.520\n",
      "  Epoch: 600, train loss: 1.318, val loss: 1.588, test loss: 1.567\n",
      "  Epoch: 601, train loss: 1.289, val loss: 1.348, test loss: 1.173\n",
      "  Epoch: 602, train loss: 1.268, val loss: 1.382, test loss: 1.276\n",
      "  Epoch: 603, train loss: 1.242, val loss: 1.410, test loss: 1.349\n",
      "  Epoch: 604, train loss: 1.259, val loss: 1.349, test loss: 1.181\n",
      "  Epoch: 605, train loss: 1.341, val loss: 1.368, test loss: 1.208\n",
      "  Epoch: 606, train loss: 1.271, val loss: 1.427, test loss: 1.336\n",
      "  Epoch: 607, train loss: 1.271, val loss: 1.363, test loss: 1.268\n",
      "  Epoch: 608, train loss: 1.279, val loss: 1.431, test loss: 1.255\n",
      "  Epoch: 609, train loss: 1.304, val loss: 1.365, test loss: 1.239\n",
      "  Epoch: 610, train loss: 1.262, val loss: 1.343, test loss: 1.241\n",
      "  Epoch: 611, train loss: 1.312, val loss: 1.399, test loss: 1.304\n",
      "  Epoch: 612, train loss: 1.304, val loss: 1.499, test loss: 1.443\n",
      "  Epoch: 613, train loss: 1.321, val loss: 1.424, test loss: 1.223\n",
      "  Epoch: 614, train loss: 1.240, val loss: 1.360, test loss: 1.244\n",
      "  Epoch: 615, train loss: 1.309, val loss: 1.369, test loss: 1.281\n",
      "  Epoch: 616, train loss: 1.336, val loss: 1.555, test loss: 1.576\n",
      "  Epoch: 617, train loss: 1.367, val loss: 1.362, test loss: 1.261\n",
      "  Epoch: 618, train loss: 1.243, val loss: 1.451, test loss: 1.322\n",
      "  Epoch: 619, train loss: 1.235, val loss: 1.354, test loss: 1.166\n",
      "  Epoch: 620, train loss: 1.284, val loss: 1.438, test loss: 1.316\n",
      "  Epoch: 621, train loss: 1.299, val loss: 1.416, test loss: 1.338\n",
      "  Epoch: 622, train loss: 1.320, val loss: 1.462, test loss: 1.218\n",
      "  Epoch: 623, train loss: 1.275, val loss: 1.499, test loss: 1.221\n",
      "  Epoch: 624, train loss: 1.262, val loss: 1.441, test loss: 1.220\n",
      "  Epoch: 625, train loss: 1.309, val loss: 1.524, test loss: 1.454\n",
      "  Epoch: 626, train loss: 1.310, val loss: 1.463, test loss: 1.338\n",
      "  Epoch: 627, train loss: 1.343, val loss: 1.462, test loss: 1.418\n",
      "  Epoch: 628, train loss: 1.272, val loss: 1.369, test loss: 1.231\n",
      "  Epoch: 629, train loss: 1.229, val loss: 1.402, test loss: 1.326\n",
      "  Epoch: 630, train loss: 1.256, val loss: 1.549, test loss: 1.454\n",
      "  Epoch: 631, train loss: 1.247, val loss: 1.372, test loss: 1.215\n",
      "  Epoch: 632, train loss: 1.293, val loss: 1.505, test loss: 1.227\n",
      "  Epoch: 633, train loss: 1.351, val loss: 1.662, test loss: 1.306\n",
      "  Epoch: 634, train loss: 1.345, val loss: 1.395, test loss: 1.257\n",
      "  Epoch: 635, train loss: 1.280, val loss: 1.409, test loss: 1.253\n",
      "  Epoch: 636, train loss: 1.225, val loss: 1.346, test loss: 1.294\n",
      "  Epoch: 637, train loss: 1.267, val loss: 1.366, test loss: 1.162\n",
      "  Epoch: 638, train loss: 1.314, val loss: 1.407, test loss: 1.255\n",
      "  Epoch: 639, train loss: 1.277, val loss: 1.402, test loss: 1.213\n",
      "  Epoch: 640, train loss: 1.311, val loss: 1.504, test loss: 1.475\n",
      "  Epoch: 641, train loss: 1.245, val loss: 1.407, test loss: 1.295\n",
      "  Epoch: 642, train loss: 1.245, val loss: 1.480, test loss: 1.275\n",
      "  Epoch: 643, train loss: 1.252, val loss: 1.384, test loss: 1.251\n",
      "  Epoch: 644, train loss: 1.276, val loss: 1.468, test loss: 1.264\n",
      "  Epoch: 645, train loss: 1.268, val loss: 1.396, test loss: 1.310\n",
      "  Epoch: 646, train loss: 1.258, val loss: 1.457, test loss: 1.324\n",
      "  Epoch: 647, train loss: 1.272, val loss: 1.395, test loss: 1.337\n",
      "  Epoch: 648, train loss: 1.251, val loss: 1.454, test loss: 1.279\n",
      "  Epoch: 649, train loss: 1.306, val loss: 1.453, test loss: 1.428\n",
      "  Epoch: 650, train loss: 1.319, val loss: 1.364, test loss: 1.294\n",
      "  Epoch: 651, train loss: 1.329, val loss: 1.433, test loss: 1.323\n",
      "  Epoch: 652, train loss: 1.208, val loss: 1.551, test loss: 1.325\n",
      "  Epoch: 653, train loss: 1.299, val loss: 1.484, test loss: 1.479\n",
      "  Epoch: 654, train loss: 1.284, val loss: 1.449, test loss: 1.426\n",
      "  Epoch: 655, train loss: 1.254, val loss: 1.548, test loss: 1.542\n",
      "  Epoch: 656, train loss: 1.287, val loss: 1.375, test loss: 1.170\n",
      "  Epoch: 657, train loss: 1.190, val loss: 1.434, test loss: 1.316\n",
      "  Epoch: 658, train loss: 1.248, val loss: 1.670, test loss: 1.599\n",
      "  Epoch: 659, train loss: 1.311, val loss: 1.395, test loss: 1.306\n",
      "  Epoch: 660, train loss: 1.269, val loss: 1.409, test loss: 1.276\n",
      "  Epoch: 661, train loss: 1.262, val loss: 1.443, test loss: 1.268\n",
      "  Epoch: 662, train loss: 1.258, val loss: 1.469, test loss: 1.160\n",
      "  Epoch: 663, train loss: 1.217, val loss: 1.411, test loss: 1.173\n",
      "  Epoch: 664, train loss: 1.255, val loss: 1.344, test loss: 1.258\n",
      "  Epoch: 665, train loss: 1.224, val loss: 1.396, test loss: 1.258\n",
      "  Epoch: 666, train loss: 1.274, val loss: 1.401, test loss: 1.263\n",
      "  Epoch: 667, train loss: 1.326, val loss: 1.544, test loss: 1.352\n",
      "  Epoch: 668, train loss: 1.296, val loss: 1.374, test loss: 1.234\n",
      "  Epoch: 669, train loss: 1.271, val loss: 1.357, test loss: 1.254\n",
      "  Epoch: 670, train loss: 1.265, val loss: 1.361, test loss: 1.226\n",
      "  Epoch: 671, train loss: 1.307, val loss: 1.509, test loss: 1.399\n",
      "  Epoch: 672, train loss: 1.279, val loss: 1.404, test loss: 1.178\n",
      "  Epoch: 673, train loss: 1.248, val loss: 1.385, test loss: 1.249\n",
      "  Epoch: 674, train loss: 1.298, val loss: 1.417, test loss: 1.250\n",
      "  Epoch: 675, train loss: 1.278, val loss: 1.548, test loss: 1.412\n",
      "  Epoch: 676, train loss: 1.245, val loss: 1.454, test loss: 1.261\n",
      "  Epoch: 677, train loss: 1.302, val loss: 1.486, test loss: 1.365\n",
      "  Epoch: 678, train loss: 1.234, val loss: 1.409, test loss: 1.144\n",
      "  Epoch: 679, train loss: 1.275, val loss: 1.433, test loss: 1.365\n",
      "  Epoch: 680, train loss: 1.271, val loss: 1.424, test loss: 1.371\n",
      "  Epoch: 681, train loss: 1.223, val loss: 1.385, test loss: 1.144\n",
      "  Epoch: 682, train loss: 1.244, val loss: 1.461, test loss: 1.183\n",
      "  Epoch: 683, train loss: 1.229, val loss: 1.414, test loss: 1.133\n",
      "  Epoch: 684, train loss: 1.272, val loss: 1.451, test loss: 1.336\n",
      "  Epoch: 685, train loss: 1.232, val loss: 1.428, test loss: 1.152\n",
      "  Epoch: 686, train loss: 1.280, val loss: 1.436, test loss: 1.176\n",
      "  Epoch: 687, train loss: 1.207, val loss: 1.505, test loss: 1.377\n",
      "  Epoch: 688, train loss: 1.271, val loss: 1.391, test loss: 1.246\n",
      "  Epoch: 689, train loss: 1.226, val loss: 1.455, test loss: 1.237\n",
      "  Epoch: 690, train loss: 1.245, val loss: 1.507, test loss: 1.212\n",
      "  Epoch: 691, train loss: 1.260, val loss: 1.446, test loss: 1.293\n",
      "  Epoch: 692, train loss: 1.244, val loss: 1.390, test loss: 1.350\n",
      "  Epoch: 693, train loss: 1.291, val loss: 1.570, test loss: 1.482\n",
      "  Epoch: 694, train loss: 1.276, val loss: 1.461, test loss: 1.349\n",
      "  Epoch: 695, train loss: 1.200, val loss: 1.400, test loss: 1.301\n",
      "  Epoch: 696, train loss: 1.264, val loss: 1.411, test loss: 1.234\n",
      "  Epoch: 697, train loss: 1.238, val loss: 1.398, test loss: 1.203\n",
      "  Epoch: 698, train loss: 1.235, val loss: 1.586, test loss: 1.291\n",
      "  Epoch: 699, train loss: 1.270, val loss: 1.362, test loss: 1.360\n",
      "  Epoch: 700, train loss: 1.202, val loss: 1.506, test loss: 1.216\n",
      "  Epoch: 701, train loss: 1.227, val loss: 1.400, test loss: 1.151\n",
      "  Epoch: 702, train loss: 1.277, val loss: 1.398, test loss: 1.276\n",
      "  Epoch: 703, train loss: 1.225, val loss: 1.487, test loss: 1.405\n",
      "  Epoch: 704, train loss: 1.208, val loss: 1.419, test loss: 1.220\n",
      "  Epoch: 705, train loss: 1.249, val loss: 1.468, test loss: 1.337\n",
      "  Epoch: 706, train loss: 1.246, val loss: 1.472, test loss: 1.322\n",
      "  Epoch: 707, train loss: 1.239, val loss: 1.404, test loss: 1.200\n",
      "  Epoch: 708, train loss: 1.234, val loss: 1.529, test loss: 1.346\n",
      "  Epoch: 709, train loss: 1.232, val loss: 1.415, test loss: 1.281\n",
      "  Epoch: 710, train loss: 1.245, val loss: 1.905, test loss: 1.768\n",
      "  Epoch: 711, train loss: 1.263, val loss: 1.519, test loss: 1.385\n",
      "  Epoch: 712, train loss: 1.284, val loss: 1.559, test loss: 1.401\n",
      "  Epoch: 713, train loss: 1.217, val loss: 1.412, test loss: 1.238\n",
      "  Epoch: 714, train loss: 1.216, val loss: 1.629, test loss: 1.604\n",
      "  Epoch: 715, train loss: 1.274, val loss: 1.504, test loss: 1.356\n",
      "  Epoch: 716, train loss: 1.216, val loss: 1.492, test loss: 1.277\n",
      "  Epoch: 717, train loss: 1.225, val loss: 1.558, test loss: 1.399\n",
      "  Epoch: 718, train loss: 1.263, val loss: 1.451, test loss: 1.194\n",
      "  Epoch: 719, train loss: 1.201, val loss: 1.486, test loss: 1.393\n",
      "  Epoch: 720, train loss: 1.222, val loss: 1.482, test loss: 1.163\n",
      "  Epoch: 721, train loss: 1.247, val loss: 1.454, test loss: 1.311\n",
      "  Epoch: 722, train loss: 1.223, val loss: 1.610, test loss: 1.589\n",
      "  Epoch: 723, train loss: 1.296, val loss: 1.422, test loss: 1.212\n",
      "  Epoch: 724, train loss: 1.202, val loss: 1.478, test loss: 1.154\n",
      "  Epoch: 725, train loss: 1.221, val loss: 1.421, test loss: 1.298\n",
      "  Epoch: 726, train loss: 1.282, val loss: 1.598, test loss: 1.284\n",
      "  Epoch: 727, train loss: 1.262, val loss: 1.359, test loss: 1.175\n",
      "  Epoch: 728, train loss: 1.225, val loss: 1.651, test loss: 1.670\n",
      "  Epoch: 729, train loss: 1.215, val loss: 1.544, test loss: 1.485\n",
      "  Epoch: 730, train loss: 1.205, val loss: 1.650, test loss: 1.239\n",
      "  Epoch: 731, train loss: 1.237, val loss: 1.679, test loss: 1.379\n",
      "  Epoch: 732, train loss: 1.201, val loss: 1.492, test loss: 1.302\n",
      "  Epoch: 733, train loss: 1.208, val loss: 1.424, test loss: 1.142\n",
      "  Epoch: 734, train loss: 1.226, val loss: 1.402, test loss: 1.187\n",
      "  Epoch: 735, train loss: 1.204, val loss: 1.481, test loss: 1.322\n",
      "  Epoch: 736, train loss: 1.256, val loss: 1.439, test loss: 1.152\n",
      "  Epoch: 737, train loss: 1.166, val loss: 1.612, test loss: 1.237\n",
      "  Epoch: 738, train loss: 1.226, val loss: 1.550, test loss: 1.235\n",
      "  Epoch: 739, train loss: 1.187, val loss: 1.743, test loss: 1.559\n",
      "  Epoch: 740, train loss: 1.221, val loss: 1.416, test loss: 1.145\n",
      "  Epoch: 741, train loss: 1.240, val loss: 1.517, test loss: 1.415\n",
      "  Epoch: 742, train loss: 1.266, val loss: 1.508, test loss: 1.362\n",
      "  Epoch: 743, train loss: 1.225, val loss: 1.451, test loss: 1.237\n",
      "  Epoch: 744, train loss: 1.262, val loss: 1.367, test loss: 1.151\n",
      "  Epoch: 745, train loss: 1.202, val loss: 1.427, test loss: 1.154\n",
      "  Epoch: 746, train loss: 1.171, val loss: 1.416, test loss: 1.190\n",
      "  Epoch: 747, train loss: 1.278, val loss: 1.521, test loss: 1.294\n",
      "  Epoch: 748, train loss: 1.258, val loss: 1.411, test loss: 1.113\n",
      "  Epoch: 749, train loss: 1.238, val loss: 1.518, test loss: 1.347\n",
      "  Epoch: 750, train loss: 1.208, val loss: 1.440, test loss: 1.202\n",
      "  Epoch: 751, train loss: 1.189, val loss: 1.495, test loss: 1.276\n",
      "  Epoch: 752, train loss: 1.163, val loss: 1.473, test loss: 1.269\n",
      "  Epoch: 753, train loss: 1.211, val loss: 1.531, test loss: 1.294\n",
      "  Epoch: 754, train loss: 1.247, val loss: 1.554, test loss: 1.437\n",
      "  Epoch: 755, train loss: 1.194, val loss: 1.438, test loss: 1.222\n",
      "  Epoch: 756, train loss: 1.263, val loss: 1.517, test loss: 1.280\n",
      "  Epoch: 757, train loss: 1.257, val loss: 1.483, test loss: 1.299\n",
      "  Epoch: 758, train loss: 1.213, val loss: 1.709, test loss: 1.798\n",
      "  Epoch: 759, train loss: 1.230, val loss: 1.475, test loss: 1.469\n",
      "  Epoch: 760, train loss: 1.248, val loss: 1.570, test loss: 1.498\n",
      "  Epoch: 761, train loss: 1.243, val loss: 1.485, test loss: 1.230\n",
      "  Epoch: 762, train loss: 1.204, val loss: 1.783, test loss: 1.536\n",
      "  Epoch: 763, train loss: 1.244, val loss: 1.546, test loss: 1.350\n",
      "  Epoch: 764, train loss: 1.217, val loss: 1.494, test loss: 1.335\n",
      "  Epoch: 765, train loss: 1.239, val loss: 1.550, test loss: 1.265\n",
      "  Epoch: 766, train loss: 1.171, val loss: 1.633, test loss: 1.229\n",
      "  Epoch: 767, train loss: 1.180, val loss: 1.432, test loss: 1.152\n",
      "  Epoch: 768, train loss: 1.233, val loss: 1.531, test loss: 1.359\n",
      "  Epoch: 769, train loss: 1.167, val loss: 1.736, test loss: 1.259\n",
      "  Epoch: 770, train loss: 1.178, val loss: 1.519, test loss: 1.235\n",
      "  Epoch: 771, train loss: 1.173, val loss: 1.600, test loss: 1.404\n",
      "  Epoch: 772, train loss: 1.230, val loss: 1.395, test loss: 1.139\n",
      "  Epoch: 773, train loss: 1.206, val loss: 1.437, test loss: 1.181\n",
      "  Epoch: 774, train loss: 1.227, val loss: 1.514, test loss: 1.298\n",
      "  Epoch: 775, train loss: 1.227, val loss: 1.933, test loss: 1.671\n",
      "  Epoch: 776, train loss: 1.218, val loss: 1.565, test loss: 1.241\n",
      "  Epoch: 777, train loss: 1.193, val loss: 1.555, test loss: 1.411\n",
      "  Epoch: 778, train loss: 1.195, val loss: 1.767, test loss: 1.407\n",
      "  Epoch: 779, train loss: 1.225, val loss: 1.452, test loss: 1.301\n",
      "  Epoch: 780, train loss: 1.234, val loss: 1.515, test loss: 1.185\n",
      "  Epoch: 781, train loss: 1.170, val loss: 1.478, test loss: 1.374\n",
      "  Epoch: 782, train loss: 1.196, val loss: 1.720, test loss: 1.365\n",
      "  Epoch: 783, train loss: 1.192, val loss: 1.614, test loss: 1.494\n",
      "  Epoch: 784, train loss: 1.178, val loss: 1.540, test loss: 1.259\n",
      "  Epoch: 785, train loss: 1.188, val loss: 1.602, test loss: 1.320\n",
      "  Epoch: 786, train loss: 1.180, val loss: 1.504, test loss: 1.212\n",
      "  Epoch: 787, train loss: 1.175, val loss: 1.590, test loss: 1.446\n",
      "  Epoch: 788, train loss: 1.202, val loss: 1.419, test loss: 1.164\n",
      "  Epoch: 789, train loss: 1.156, val loss: 1.822, test loss: 1.482\n",
      "  Epoch: 790, train loss: 1.167, val loss: 1.473, test loss: 1.206\n",
      "  Epoch: 791, train loss: 1.245, val loss: 1.544, test loss: 1.392\n",
      "  Epoch: 792, train loss: 1.251, val loss: 1.477, test loss: 1.380\n",
      "  Epoch: 793, train loss: 1.149, val loss: 1.920, test loss: 1.683\n",
      "  Epoch: 794, train loss: 1.231, val loss: 1.643, test loss: 1.550\n",
      "  Epoch: 795, train loss: 1.204, val loss: 1.557, test loss: 1.310\n",
      "  Epoch: 796, train loss: 1.190, val loss: 1.434, test loss: 1.128\n",
      "  Epoch: 797, train loss: 1.202, val loss: 1.416, test loss: 1.150\n",
      "  Epoch: 798, train loss: 1.188, val loss: 1.554, test loss: 1.171\n",
      "  Epoch: 799, train loss: 1.192, val loss: 1.501, test loss: 1.149\n",
      "  Epoch: 800, train loss: 1.165, val loss: 1.476, test loss: 1.258\n",
      "  Epoch: 801, train loss: 1.171, val loss: 1.820, test loss: 1.519\n",
      "  Epoch: 802, train loss: 1.187, val loss: 1.455, test loss: 1.116\n",
      "  Epoch: 803, train loss: 1.194, val loss: 1.595, test loss: 1.392\n",
      "  Epoch: 804, train loss: 1.219, val loss: 1.757, test loss: 1.367\n",
      "  Epoch: 805, train loss: 1.238, val loss: 1.735, test loss: 1.391\n",
      "  Epoch: 806, train loss: 1.172, val loss: 1.442, test loss: 1.129\n",
      "  Epoch: 807, train loss: 1.180, val loss: 1.550, test loss: 1.407\n",
      "  Epoch: 808, train loss: 1.272, val loss: 1.649, test loss: 1.316\n",
      "  Epoch: 809, train loss: 1.172, val loss: 1.524, test loss: 1.408\n",
      "  Epoch: 810, train loss: 1.163, val loss: 1.535, test loss: 1.312\n",
      "  Epoch: 811, train loss: 1.182, val loss: 1.539, test loss: 1.166\n",
      "  Epoch: 812, train loss: 1.183, val loss: 1.635, test loss: 1.363\n",
      "  Epoch: 813, train loss: 1.182, val loss: 1.469, test loss: 1.233\n",
      "  Epoch: 814, train loss: 1.212, val loss: 1.595, test loss: 1.153\n",
      "  Epoch: 815, train loss: 1.172, val loss: 1.582, test loss: 1.134\n",
      "  Epoch: 816, train loss: 1.227, val loss: 1.508, test loss: 1.107\n",
      "  Epoch: 817, train loss: 1.232, val loss: 1.415, test loss: 1.165\n",
      "  Epoch: 818, train loss: 1.229, val loss: 1.536, test loss: 1.298\n",
      "  Epoch: 819, train loss: 1.195, val loss: 1.396, test loss: 1.139\n",
      "  Epoch: 820, train loss: 1.182, val loss: 1.864, test loss: 1.355\n",
      "  Epoch: 821, train loss: 1.237, val loss: 1.556, test loss: 1.289\n",
      "  Epoch: 822, train loss: 1.196, val loss: 1.449, test loss: 1.262\n",
      "  Epoch: 823, train loss: 1.205, val loss: 1.436, test loss: 1.255\n",
      "  Epoch: 824, train loss: 1.155, val loss: 1.442, test loss: 1.104\n",
      "  Epoch: 825, train loss: 1.165, val loss: 1.461, test loss: 1.158\n",
      "  Epoch: 826, train loss: 1.159, val loss: 1.511, test loss: 1.222\n",
      "  Epoch: 827, train loss: 1.162, val loss: 1.938, test loss: 1.446\n",
      "  Epoch: 828, train loss: 1.177, val loss: 1.579, test loss: 1.243\n",
      "  Epoch: 829, train loss: 1.178, val loss: 1.850, test loss: 1.346\n",
      "  Epoch: 830, train loss: 1.210, val loss: 1.543, test loss: 1.166\n",
      "  Epoch: 831, train loss: 1.209, val loss: 1.488, test loss: 1.248\n",
      "  Epoch: 832, train loss: 1.131, val loss: 1.882, test loss: 1.551\n",
      "  Epoch: 833, train loss: 1.176, val loss: 1.430, test loss: 1.267\n",
      "  Epoch: 834, train loss: 1.155, val loss: 1.548, test loss: 1.322\n",
      "  Epoch: 835, train loss: 1.221, val loss: 1.460, test loss: 1.226\n",
      "  Epoch: 836, train loss: 1.189, val loss: 1.578, test loss: 1.235\n",
      "  Epoch: 837, train loss: 1.156, val loss: 1.668, test loss: 1.571\n",
      "  Epoch: 838, train loss: 1.154, val loss: 1.600, test loss: 1.144\n",
      "  Epoch: 839, train loss: 1.173, val loss: 1.889, test loss: 1.319\n",
      "  Epoch: 840, train loss: 1.268, val loss: 1.672, test loss: 1.403\n",
      "  Epoch: 841, train loss: 1.204, val loss: 1.388, test loss: 1.154\n",
      "  Epoch: 842, train loss: 1.241, val loss: 1.647, test loss: 1.268\n",
      "  Epoch: 843, train loss: 1.200, val loss: 1.435, test loss: 1.165\n",
      "  Epoch: 844, train loss: 1.150, val loss: 1.492, test loss: 1.122\n",
      "  Epoch: 845, train loss: 1.174, val loss: 1.856, test loss: 1.507\n",
      "  Epoch: 846, train loss: 1.158, val loss: 1.585, test loss: 1.428\n",
      "  Epoch: 847, train loss: 1.187, val loss: 1.865, test loss: 1.455\n",
      "  Epoch: 848, train loss: 1.219, val loss: 1.568, test loss: 1.332\n",
      "  Epoch: 849, train loss: 1.186, val loss: 1.574, test loss: 1.253\n",
      "  Epoch: 850, train loss: 1.177, val loss: 1.536, test loss: 1.239\n",
      "  Epoch: 851, train loss: 1.144, val loss: 1.624, test loss: 1.480\n",
      "  Epoch: 852, train loss: 1.251, val loss: 2.496, test loss: 2.254\n",
      "  Epoch: 853, train loss: 1.153, val loss: 1.433, test loss: 1.221\n",
      "  Epoch: 854, train loss: 1.244, val loss: 1.608, test loss: 1.225\n",
      "  Epoch: 855, train loss: 1.163, val loss: 1.469, test loss: 1.115\n",
      "  Epoch: 856, train loss: 1.215, val loss: 1.685, test loss: 1.427\n",
      "  Epoch: 857, train loss: 1.136, val loss: 1.475, test loss: 1.155\n",
      "  Epoch: 858, train loss: 1.245, val loss: 1.486, test loss: 1.306\n",
      "  Epoch: 859, train loss: 1.162, val loss: 1.815, test loss: 1.318\n",
      "  Epoch: 860, train loss: 1.136, val loss: 1.507, test loss: 1.231\n",
      "  Epoch: 861, train loss: 1.111, val loss: 1.566, test loss: 1.244\n",
      "  Epoch: 862, train loss: 1.157, val loss: 1.710, test loss: 1.139\n",
      "  Epoch: 863, train loss: 1.145, val loss: 1.654, test loss: 1.232\n",
      "  Epoch: 864, train loss: 1.179, val loss: 1.467, test loss: 1.160\n",
      "  Epoch: 865, train loss: 1.123, val loss: 1.511, test loss: 1.243\n",
      "  Epoch: 866, train loss: 1.196, val loss: 1.535, test loss: 1.320\n",
      "  Epoch: 867, train loss: 1.181, val loss: 1.606, test loss: 1.377\n",
      "  Epoch: 868, train loss: 1.160, val loss: 1.457, test loss: 1.254\n",
      "  Epoch: 869, train loss: 1.175, val loss: 1.507, test loss: 1.320\n",
      "  Epoch: 870, train loss: 1.165, val loss: 1.604, test loss: 1.186\n",
      "  Epoch: 871, train loss: 1.172, val loss: 1.607, test loss: 1.229\n",
      "  Epoch: 872, train loss: 1.147, val loss: 1.644, test loss: 1.310\n",
      "  Epoch: 873, train loss: 1.145, val loss: 1.452, test loss: 1.149\n",
      "  Epoch: 874, train loss: 1.162, val loss: 1.460, test loss: 1.246\n",
      "  Epoch: 875, train loss: 1.195, val loss: 1.617, test loss: 1.521\n",
      "  Epoch: 876, train loss: 1.149, val loss: 2.079, test loss: 1.473\n",
      "  Epoch: 877, train loss: 1.179, val loss: 1.745, test loss: 1.730\n",
      "  Epoch: 878, train loss: 1.204, val loss: 1.741, test loss: 1.526\n",
      "  Epoch: 879, train loss: 1.182, val loss: 1.485, test loss: 1.239\n",
      "  Epoch: 880, train loss: 1.135, val loss: 1.546, test loss: 1.300\n",
      "  Epoch: 881, train loss: 1.162, val loss: 1.514, test loss: 1.180\n",
      "  Epoch: 882, train loss: 1.126, val loss: 1.506, test loss: 1.080\n",
      "  Epoch: 883, train loss: 1.128, val loss: 1.509, test loss: 1.088\n",
      "  Epoch: 884, train loss: 1.133, val loss: 1.752, test loss: 1.341\n",
      "  Epoch: 885, train loss: 1.127, val loss: 1.603, test loss: 1.404\n",
      "  Epoch: 886, train loss: 1.093, val loss: 2.413, test loss: 2.188\n",
      "  Epoch: 887, train loss: 1.145, val loss: 1.599, test loss: 1.462\n",
      "  Epoch: 888, train loss: 1.115, val loss: 2.017, test loss: 1.511\n",
      "  Epoch: 889, train loss: 1.181, val loss: 1.517, test loss: 1.195\n",
      "  Epoch: 890, train loss: 1.190, val loss: 1.885, test loss: 1.330\n",
      "  Epoch: 891, train loss: 1.126, val loss: 1.460, test loss: 1.103\n",
      "  Epoch: 892, train loss: 1.133, val loss: 1.473, test loss: 1.249\n",
      "  Epoch: 893, train loss: 1.119, val loss: 1.556, test loss: 1.280\n",
      "  Epoch: 894, train loss: 1.137, val loss: 1.574, test loss: 1.127\n",
      "  Epoch: 895, train loss: 1.146, val loss: 1.570, test loss: 1.110\n",
      "  Epoch: 896, train loss: 1.145, val loss: 2.451, test loss: 1.767\n",
      "  Epoch: 897, train loss: 1.133, val loss: 1.678, test loss: 1.223\n",
      "  Epoch: 898, train loss: 1.176, val loss: 1.485, test loss: 1.149\n",
      "  Epoch: 899, train loss: 1.108, val loss: 1.823, test loss: 1.250\n",
      "  Epoch: 900, train loss: 1.135, val loss: 1.798, test loss: 1.225\n",
      "  Epoch: 901, train loss: 1.143, val loss: 1.631, test loss: 1.303\n",
      "  Epoch: 902, train loss: 1.130, val loss: 1.586, test loss: 1.277\n",
      "  Epoch: 903, train loss: 1.132, val loss: 2.004, test loss: 1.412\n",
      "  Epoch: 904, train loss: 1.130, val loss: 1.536, test loss: 1.315\n",
      "  Epoch: 905, train loss: 1.179, val loss: 1.546, test loss: 1.093\n",
      "  Epoch: 906, train loss: 1.147, val loss: 1.595, test loss: 1.180\n",
      "  Epoch: 907, train loss: 1.176, val loss: 1.615, test loss: 1.194\n",
      "  Epoch: 908, train loss: 1.135, val loss: 1.466, test loss: 1.068\n",
      "  Epoch: 909, train loss: 1.126, val loss: 1.431, test loss: 1.235\n",
      "  Epoch: 910, train loss: 1.150, val loss: 1.709, test loss: 1.663\n",
      "  Epoch: 911, train loss: 1.153, val loss: 2.283, test loss: 1.517\n",
      "  Epoch: 912, train loss: 1.137, val loss: 1.724, test loss: 1.225\n",
      "  Epoch: 913, train loss: 1.113, val loss: 1.482, test loss: 1.195\n",
      "  Epoch: 914, train loss: 1.128, val loss: 1.528, test loss: 1.248\n",
      "  Epoch: 915, train loss: 1.116, val loss: 1.660, test loss: 1.355\n",
      "  Epoch: 916, train loss: 1.145, val loss: 2.305, test loss: 1.685\n",
      "  Epoch: 917, train loss: 1.129, val loss: 1.483, test loss: 1.124\n",
      "  Epoch: 918, train loss: 1.123, val loss: 1.648, test loss: 1.344\n",
      "  Epoch: 919, train loss: 1.238, val loss: 1.506, test loss: 1.365\n",
      "  Epoch: 920, train loss: 1.105, val loss: 1.565, test loss: 1.262\n",
      "  Epoch: 921, train loss: 1.145, val loss: 1.589, test loss: 1.343\n",
      "  Epoch: 922, train loss: 1.129, val loss: 1.574, test loss: 1.129\n",
      "  Epoch: 923, train loss: 1.199, val loss: 1.721, test loss: 1.673\n",
      "  Epoch: 924, train loss: 1.177, val loss: 3.028, test loss: 2.590\n",
      "  Epoch: 925, train loss: 1.148, val loss: 1.538, test loss: 1.318\n",
      "  Epoch: 926, train loss: 1.167, val loss: 1.588, test loss: 1.064\n",
      "  Epoch: 927, train loss: 1.132, val loss: 1.673, test loss: 1.119\n",
      "  Epoch: 928, train loss: 1.105, val loss: 1.660, test loss: 1.279\n",
      "  Epoch: 929, train loss: 1.175, val loss: 1.778, test loss: 1.317\n",
      "  Epoch: 930, train loss: 1.110, val loss: 1.457, test loss: 1.203\n",
      "  Epoch: 931, train loss: 1.113, val loss: 1.690, test loss: 1.178\n",
      "  Epoch: 932, train loss: 1.093, val loss: 1.466, test loss: 1.185\n",
      "  Epoch: 933, train loss: 1.109, val loss: 2.179, test loss: 1.601\n",
      "  Epoch: 934, train loss: 1.156, val loss: 1.417, test loss: 1.055\n",
      "  Epoch: 935, train loss: 1.163, val loss: 1.439, test loss: 1.180\n",
      "  Epoch: 936, train loss: 1.128, val loss: 1.806, test loss: 1.233\n",
      "  Epoch: 937, train loss: 1.170, val loss: 1.381, test loss: 1.137\n",
      "  Epoch: 938, train loss: 1.136, val loss: 1.580, test loss: 1.380\n",
      "  Epoch: 939, train loss: 1.120, val loss: 1.651, test loss: 1.225\n",
      "  Epoch: 940, train loss: 1.113, val loss: 1.501, test loss: 1.216\n",
      "  Epoch: 941, train loss: 1.129, val loss: 1.456, test loss: 1.131\n",
      "  Epoch: 942, train loss: 1.075, val loss: 1.524, test loss: 1.386\n",
      "  Epoch: 943, train loss: 1.137, val loss: 1.613, test loss: 1.285\n",
      "  Epoch: 944, train loss: 1.181, val loss: 1.666, test loss: 1.278\n",
      "  Epoch: 945, train loss: 1.135, val loss: 1.459, test loss: 1.263\n",
      "  Epoch: 946, train loss: 1.135, val loss: 1.653, test loss: 1.537\n",
      "  Epoch: 947, train loss: 1.125, val loss: 1.497, test loss: 1.109\n",
      "  Epoch: 948, train loss: 1.105, val loss: 1.598, test loss: 1.465\n",
      "  Epoch: 949, train loss: 1.114, val loss: 2.328, test loss: 1.682\n",
      "  Epoch: 950, train loss: 1.085, val loss: 1.514, test loss: 1.273\n",
      "  Epoch: 951, train loss: 1.100, val loss: 1.583, test loss: 1.421\n",
      "  Epoch: 952, train loss: 1.083, val loss: 1.487, test loss: 1.104\n",
      "  Epoch: 953, train loss: 1.144, val loss: 1.727, test loss: 1.235\n",
      "  Epoch: 954, train loss: 1.098, val loss: 1.621, test loss: 1.148\n",
      "  Epoch: 955, train loss: 1.194, val loss: 1.578, test loss: 1.270\n",
      "  Epoch: 956, train loss: 1.193, val loss: 1.695, test loss: 1.566\n",
      "  Epoch: 957, train loss: 1.161, val loss: 1.647, test loss: 1.324\n",
      "  Epoch: 958, train loss: 1.166, val loss: 1.501, test loss: 1.245\n",
      "  Epoch: 959, train loss: 1.167, val loss: 1.507, test loss: 1.171\n",
      "  Epoch: 960, train loss: 1.090, val loss: 1.565, test loss: 1.398\n",
      "  Epoch: 961, train loss: 1.086, val loss: 1.455, test loss: 1.235\n",
      "  Epoch: 962, train loss: 1.148, val loss: 2.387, test loss: 1.597\n",
      "  Epoch: 963, train loss: 1.112, val loss: 1.537, test loss: 1.223\n",
      "  Epoch: 964, train loss: 1.139, val loss: 1.844, test loss: 1.291\n",
      "  Epoch: 965, train loss: 1.121, val loss: 1.699, test loss: 1.670\n",
      "  Epoch: 966, train loss: 1.091, val loss: 1.570, test loss: 1.136\n",
      "  Epoch: 967, train loss: 1.134, val loss: 2.059, test loss: 1.384\n",
      "  Epoch: 968, train loss: 1.139, val loss: 1.463, test loss: 1.228\n",
      "  Epoch: 969, train loss: 1.137, val loss: 1.505, test loss: 1.261\n",
      "  Epoch: 970, train loss: 1.105, val loss: 2.257, test loss: 1.707\n",
      "  Epoch: 971, train loss: 1.168, val loss: 1.769, test loss: 1.215\n",
      "  Epoch: 972, train loss: 1.076, val loss: 1.551, test loss: 1.183\n",
      "  Epoch: 973, train loss: 1.074, val loss: 1.792, test loss: 1.214\n",
      "  Epoch: 974, train loss: 1.187, val loss: 1.543, test loss: 1.285\n",
      "  Epoch: 975, train loss: 1.135, val loss: 1.561, test loss: 1.124\n",
      "  Epoch: 976, train loss: 1.103, val loss: 1.544, test loss: 1.110\n",
      "  Epoch: 977, train loss: 1.111, val loss: 1.751, test loss: 1.236\n",
      "  Epoch: 978, train loss: 1.080, val loss: 1.583, test loss: 1.197\n",
      "  Epoch: 979, train loss: 1.112, val loss: 1.772, test loss: 1.467\n",
      "  Epoch: 980, train loss: 1.166, val loss: 1.521, test loss: 1.210\n",
      "  Epoch: 981, train loss: 1.121, val loss: 1.461, test loss: 1.184\n",
      "  Epoch: 982, train loss: 1.097, val loss: 1.574, test loss: 1.181\n",
      "  Epoch: 983, train loss: 1.109, val loss: 1.513, test loss: 1.350\n",
      "  Epoch: 984, train loss: 1.113, val loss: 1.498, test loss: 1.235\n",
      "  Epoch: 985, train loss: 1.091, val loss: 2.095, test loss: 1.485\n",
      "  Epoch: 986, train loss: 1.110, val loss: 1.602, test loss: 1.329\n",
      "  Epoch: 987, train loss: 1.115, val loss: 1.784, test loss: 1.293\n",
      "  Epoch: 988, train loss: 1.061, val loss: 1.560, test loss: 1.374\n",
      "  Epoch: 989, train loss: 1.125, val loss: 1.432, test loss: 1.222\n",
      "  Epoch: 990, train loss: 1.159, val loss: 1.408, test loss: 1.149\n",
      "  Epoch: 991, train loss: 1.075, val loss: 1.768, test loss: 1.172\n",
      "  Epoch: 992, train loss: 1.105, val loss: 1.607, test loss: 1.180\n",
      "  Epoch: 993, train loss: 1.146, val loss: 1.523, test loss: 1.315\n",
      "  Epoch: 994, train loss: 1.107, val loss: 1.549, test loss: 1.240\n",
      "  Epoch: 995, train loss: 1.087, val loss: 1.588, test loss: 1.126\n",
      "  Epoch: 996, train loss: 1.045, val loss: 1.484, test loss: 1.109\n",
      "  Epoch: 997, train loss: 1.121, val loss: 1.636, test loss: 1.538\n",
      "  Epoch: 998, train loss: 1.110, val loss: 1.659, test loss: 1.174\n",
      "  Epoch: 999, train loss: 1.060, val loss: 1.528, test loss: 1.275\n",
      "  Epoch: 1000, train loss: 1.085, val loss: 1.724, test loss: 1.611\n",
      "  Epoch: 1001, train loss: 1.112, val loss: 1.666, test loss: 1.207\n",
      "  Epoch: 1002, train loss: 1.103, val loss: 1.701, test loss: 1.299\n",
      "  Epoch: 1003, train loss: 1.217, val loss: 1.423, test loss: 1.230\n",
      "  Epoch: 1004, train loss: 1.143, val loss: 1.732, test loss: 1.625\n",
      "  Epoch: 1005, train loss: 1.065, val loss: 2.153, test loss: 1.585\n",
      "  Epoch: 1006, train loss: 1.118, val loss: 1.782, test loss: 1.320\n",
      "  Epoch: 1007, train loss: 1.096, val loss: 1.522, test loss: 1.233\n",
      "  Epoch: 1008, train loss: 1.106, val loss: 1.447, test loss: 1.161\n",
      "  Epoch: 1009, train loss: 1.072, val loss: 1.844, test loss: 1.266\n",
      "  Epoch: 1010, train loss: 1.106, val loss: 2.011, test loss: 1.302\n",
      "  Epoch: 1011, train loss: 1.123, val loss: 1.564, test loss: 1.081\n",
      "  Epoch: 1012, train loss: 1.131, val loss: 1.567, test loss: 1.176\n",
      "  Epoch: 1013, train loss: 1.100, val loss: 1.514, test loss: 1.290\n",
      "  Epoch: 1014, train loss: 1.126, val loss: 1.503, test loss: 1.303\n",
      "  Epoch: 1015, train loss: 1.130, val loss: 1.689, test loss: 1.180\n",
      "  Epoch: 1016, train loss: 1.114, val loss: 1.513, test loss: 1.209\n",
      "  Epoch: 1017, train loss: 1.097, val loss: 1.674, test loss: 1.633\n",
      "  Epoch: 1018, train loss: 1.100, val loss: 1.559, test loss: 1.319\n",
      "  Epoch: 1019, train loss: 1.119, val loss: 1.694, test loss: 1.657\n",
      "  Epoch: 1020, train loss: 1.111, val loss: 1.839, test loss: 1.293\n",
      "  Epoch: 1021, train loss: 1.090, val loss: 1.653, test loss: 1.269\n",
      "  Epoch: 1022, train loss: 1.030, val loss: 2.326, test loss: 1.726\n",
      "  Epoch: 1023, train loss: 1.072, val loss: 1.755, test loss: 1.206\n",
      "  Epoch: 1024, train loss: 1.151, val loss: 1.609, test loss: 1.185\n",
      "  Epoch: 1025, train loss: 1.117, val loss: 1.572, test loss: 1.368\n",
      "  Epoch: 1026, train loss: 1.157, val loss: 1.459, test loss: 1.286\n",
      "  Epoch: 1027, train loss: 1.092, val loss: 1.747, test loss: 1.781\n",
      "  Epoch: 1028, train loss: 1.093, val loss: 1.596, test loss: 1.470\n",
      "  Epoch: 1029, train loss: 1.070, val loss: 1.841, test loss: 1.279\n",
      "  Epoch: 1030, train loss: 1.094, val loss: 1.884, test loss: 1.337\n",
      "  Epoch: 1031, train loss: 1.129, val loss: 1.507, test loss: 1.248\n",
      "  Epoch: 1032, train loss: 1.089, val loss: 1.611, test loss: 1.292\n",
      "  Epoch: 1033, train loss: 1.065, val loss: 1.507, test loss: 1.168\n",
      "  Epoch: 1034, train loss: 1.084, val loss: 1.937, test loss: 1.288\n",
      "  Epoch: 1035, train loss: 1.068, val loss: 1.645, test loss: 1.206\n",
      "  Epoch: 1036, train loss: 1.102, val loss: 1.497, test loss: 1.107\n",
      "  Epoch: 1037, train loss: 1.083, val loss: 1.749, test loss: 1.658\n",
      "  Epoch: 1038, train loss: 1.137, val loss: 3.166, test loss: 2.473\n",
      "  Epoch: 1039, train loss: 1.083, val loss: 1.709, test loss: 1.715\n",
      "  Epoch: 1040, train loss: 1.140, val loss: 1.773, test loss: 1.456\n",
      "  Epoch: 1041, train loss: 1.129, val loss: 1.460, test loss: 1.238\n",
      "  Epoch: 1042, train loss: 1.124, val loss: 1.778, test loss: 1.792\n",
      "  Epoch: 1043, train loss: 1.101, val loss: 1.571, test loss: 1.372\n",
      "  Epoch: 1044, train loss: 1.049, val loss: 1.527, test loss: 1.164\n",
      "  Epoch: 1045, train loss: 1.032, val loss: 1.788, test loss: 1.289\n",
      "  Epoch: 1046, train loss: 1.028, val loss: 1.505, test loss: 1.252\n",
      "  Epoch: 1047, train loss: 1.116, val loss: 1.604, test loss: 1.169\n",
      "  Epoch: 1048, train loss: 1.077, val loss: 2.006, test loss: 1.459\n",
      "  Epoch: 1049, train loss: 1.093, val loss: 1.490, test loss: 1.158\n",
      "  Epoch: 1050, train loss: 1.138, val loss: 1.501, test loss: 1.050\n",
      "  Epoch: 1051, train loss: 1.088, val loss: 1.518, test loss: 1.159\n",
      "  Epoch: 1052, train loss: 1.046, val loss: 1.535, test loss: 1.213\n",
      "  Epoch: 1053, train loss: 1.083, val loss: 1.616, test loss: 1.360\n",
      "  Epoch: 1054, train loss: 1.070, val loss: 1.751, test loss: 1.154\n",
      "  Epoch: 1055, train loss: 1.111, val loss: 1.470, test loss: 1.271\n",
      "  Epoch: 1056, train loss: 1.097, val loss: 1.890, test loss: 1.388\n",
      "  Epoch: 1057, train loss: 0.997, val loss: 1.510, test loss: 1.326\n",
      "  Epoch: 1058, train loss: 1.148, val loss: 1.660, test loss: 1.196\n",
      "  Epoch: 1059, train loss: 1.106, val loss: 1.699, test loss: 1.645\n",
      "  Epoch: 1060, train loss: 1.028, val loss: 1.527, test loss: 1.187\n",
      "  Epoch: 1061, train loss: 1.091, val loss: 1.536, test loss: 1.187\n",
      "  Epoch: 1062, train loss: 1.073, val loss: 2.638, test loss: 2.020\n",
      "  Epoch: 1063, train loss: 1.073, val loss: 1.478, test loss: 1.101\n",
      "  Epoch: 1064, train loss: 1.116, val loss: 1.590, test loss: 1.205\n",
      "  Epoch: 1065, train loss: 1.075, val loss: 1.550, test loss: 1.157\n",
      "  Epoch: 1066, train loss: 1.110, val loss: 2.959, test loss: 2.402\n",
      "  Epoch: 1067, train loss: 1.091, val loss: 1.457, test loss: 1.182\n",
      "  Epoch: 1068, train loss: 1.085, val loss: 1.564, test loss: 1.324\n",
      "  Epoch: 1069, train loss: 1.060, val loss: 1.504, test loss: 1.271\n",
      "  Epoch: 1070, train loss: 1.041, val loss: 2.081, test loss: 1.495\n",
      "  Epoch: 1071, train loss: 1.053, val loss: 2.131, test loss: 1.495\n",
      "  Epoch: 1072, train loss: 1.070, val loss: 1.541, test loss: 1.321\n",
      "  Epoch: 1073, train loss: 1.069, val loss: 1.618, test loss: 1.492\n",
      "  Epoch: 1074, train loss: 1.093, val loss: 2.014, test loss: 1.676\n",
      "  Epoch: 1075, train loss: 1.097, val loss: 1.631, test loss: 1.678\n",
      "  Epoch: 1076, train loss: 1.085, val loss: 1.758, test loss: 1.249\n",
      "  Epoch: 1077, train loss: 1.051, val loss: 1.709, test loss: 1.179\n",
      "  Epoch: 1078, train loss: 1.147, val loss: 1.623, test loss: 1.376\n",
      "  Epoch: 1079, train loss: 1.022, val loss: 1.531, test loss: 1.359\n",
      "  Epoch: 1080, train loss: 1.109, val loss: 1.613, test loss: 1.209\n",
      "  Epoch: 1081, train loss: 1.109, val loss: 1.742, test loss: 1.311\n",
      "  Epoch: 1082, train loss: 1.078, val loss: 1.649, test loss: 1.153\n",
      "  Epoch: 1083, train loss: 1.052, val loss: 1.734, test loss: 1.763\n",
      "  Epoch: 1084, train loss: 1.070, val loss: 1.527, test loss: 1.311\n",
      "  Epoch: 1085, train loss: 1.122, val loss: 1.491, test loss: 1.206\n",
      "  Epoch: 1086, train loss: 1.112, val loss: 1.727, test loss: 1.574\n",
      "  Epoch: 1087, train loss: 1.129, val loss: 1.802, test loss: 1.198\n",
      "  Epoch: 1088, train loss: 1.045, val loss: 1.522, test loss: 1.269\n",
      "  Epoch: 1089, train loss: 1.023, val loss: 1.827, test loss: 1.308\n",
      "  Epoch: 1090, train loss: 1.047, val loss: 1.666, test loss: 1.535\n",
      "  Epoch: 1091, train loss: 1.051, val loss: 1.816, test loss: 1.198\n",
      "  Epoch: 1092, train loss: 1.011, val loss: 1.585, test loss: 1.438\n",
      "  Epoch: 1093, train loss: 1.040, val loss: 1.592, test loss: 1.102\n",
      "  Epoch: 1094, train loss: 0.993, val loss: 2.019, test loss: 1.427\n",
      "  Epoch: 1095, train loss: 1.048, val loss: 1.559, test loss: 1.298\n",
      "  Epoch: 1096, train loss: 1.066, val loss: 1.533, test loss: 1.335\n",
      "  Epoch: 1097, train loss: 1.017, val loss: 1.566, test loss: 1.183\n",
      "  Epoch: 1098, train loss: 1.100, val loss: 1.487, test loss: 1.292\n",
      "  Epoch: 1099, train loss: 1.100, val loss: 1.807, test loss: 1.143\n",
      "  Epoch: 1100, train loss: 1.116, val loss: 2.260, test loss: 1.512\n",
      "  Epoch: 1101, train loss: 1.064, val loss: 1.861, test loss: 1.348\n",
      "  Epoch: 1102, train loss: 1.048, val loss: 1.494, test loss: 1.337\n",
      "  Epoch: 1103, train loss: 1.046, val loss: 1.504, test loss: 1.363\n",
      "  Epoch: 1104, train loss: 1.012, val loss: 1.512, test loss: 1.353\n",
      "  Epoch: 1105, train loss: 1.014, val loss: 1.444, test loss: 1.191\n",
      "  Epoch: 1106, train loss: 1.010, val loss: 1.535, test loss: 1.245\n",
      "  Epoch: 1107, train loss: 1.008, val loss: 2.223, test loss: 1.651\n",
      "  Epoch: 1108, train loss: 1.048, val loss: 1.530, test loss: 1.193\n",
      "  Epoch: 1109, train loss: 1.060, val loss: 2.090, test loss: 1.383\n",
      "  Epoch: 1110, train loss: 1.109, val loss: 1.629, test loss: 1.558\n",
      "  Epoch: 1111, train loss: 1.043, val loss: 2.611, test loss: 2.113\n",
      "  Epoch: 1112, train loss: 1.105, val loss: 1.502, test loss: 1.329\n",
      "  Epoch: 1113, train loss: 1.040, val loss: 2.882, test loss: 2.247\n",
      "  Epoch: 1114, train loss: 1.045, val loss: 1.990, test loss: 1.312\n",
      "  Epoch: 1115, train loss: 1.078, val loss: 2.026, test loss: 1.573\n",
      "  Epoch: 1116, train loss: 1.117, val loss: 1.531, test loss: 1.341\n",
      "  Epoch: 1117, train loss: 1.033, val loss: 1.677, test loss: 1.247\n",
      "  Epoch: 1118, train loss: 1.068, val loss: 1.886, test loss: 1.336\n",
      "  Epoch: 1119, train loss: 1.034, val loss: 1.507, test loss: 1.231\n",
      "  Epoch: 1120, train loss: 1.060, val loss: 1.593, test loss: 1.575\n",
      "  Epoch: 1121, train loss: 1.027, val loss: 1.620, test loss: 1.220\n",
      "  Epoch: 1122, train loss: 1.051, val loss: 1.536, test loss: 1.093\n",
      "  Epoch: 1123, train loss: 1.043, val loss: 1.518, test loss: 1.267\n",
      "  Epoch: 1124, train loss: 1.049, val loss: 3.866, test loss: 3.573\n",
      "  Epoch: 1125, train loss: 1.104, val loss: 1.829, test loss: 1.884\n",
      "  Epoch: 1126, train loss: 1.046, val loss: 1.707, test loss: 1.145\n",
      "  Epoch: 1127, train loss: 0.968, val loss: 2.174, test loss: 1.532\n",
      "  Epoch: 1128, train loss: 1.076, val loss: 1.624, test loss: 1.172\n",
      "  Epoch: 1129, train loss: 1.075, val loss: 1.576, test loss: 1.389\n",
      "  Epoch: 1130, train loss: 1.071, val loss: 1.481, test loss: 1.257\n",
      "  Epoch: 1131, train loss: 1.042, val loss: 1.695, test loss: 1.186\n",
      "  Epoch: 1132, train loss: 1.061, val loss: 1.619, test loss: 1.171\n",
      "  Epoch: 1133, train loss: 1.091, val loss: 2.107, test loss: 1.641\n",
      "  Epoch: 1134, train loss: 1.059, val loss: 1.593, test loss: 1.614\n",
      "  Epoch: 1135, train loss: 1.000, val loss: 1.478, test loss: 1.136\n",
      "  Epoch: 1136, train loss: 1.051, val loss: 1.573, test loss: 1.073\n",
      "  Epoch: 1137, train loss: 1.093, val loss: 1.692, test loss: 1.160\n",
      "  Epoch: 1138, train loss: 1.037, val loss: 1.518, test loss: 1.099\n",
      "  Epoch: 1139, train loss: 1.099, val loss: 1.831, test loss: 1.239\n",
      "  Epoch: 1140, train loss: 1.087, val loss: 1.868, test loss: 1.304\n",
      "  Epoch: 1141, train loss: 1.070, val loss: 1.523, test loss: 1.270\n",
      "  Epoch: 1142, train loss: 1.000, val loss: 1.609, test loss: 1.236\n",
      "  Epoch: 1143, train loss: 1.054, val loss: 1.694, test loss: 1.811\n",
      "  Epoch: 1144, train loss: 1.049, val loss: 1.602, test loss: 1.173\n",
      "  Epoch: 1145, train loss: 1.058, val loss: 1.732, test loss: 1.164\n",
      "  Epoch: 1146, train loss: 1.092, val loss: 1.727, test loss: 1.171\n",
      "  Epoch: 1147, train loss: 1.099, val loss: 1.607, test loss: 1.343\n",
      "  Epoch: 1148, train loss: 1.020, val loss: 2.694, test loss: 1.875\n",
      "  Epoch: 1149, train loss: 0.995, val loss: 1.499, test loss: 1.065\n",
      "  Epoch: 1150, train loss: 1.043, val loss: 1.525, test loss: 1.187\n",
      "  Epoch: 1151, train loss: 1.150, val loss: 1.383, test loss: 1.183\n",
      "  Epoch: 1152, train loss: 0.997, val loss: 2.229, test loss: 1.520\n",
      "  Epoch: 1153, train loss: 1.032, val loss: 1.466, test loss: 1.234\n",
      "  Epoch: 1154, train loss: 1.006, val loss: 1.544, test loss: 1.435\n",
      "  Epoch: 1155, train loss: 1.010, val loss: 1.475, test loss: 1.277\n",
      "  Epoch: 1156, train loss: 1.027, val loss: 1.552, test loss: 1.513\n",
      "  Epoch: 1157, train loss: 1.007, val loss: 1.687, test loss: 1.214\n",
      "  Epoch: 1158, train loss: 0.987, val loss: 1.849, test loss: 1.237\n",
      "  Epoch: 1159, train loss: 1.052, val loss: 1.568, test loss: 1.488\n",
      "  Epoch: 1160, train loss: 0.992, val loss: 2.407, test loss: 1.781\n",
      "  Epoch: 1161, train loss: 1.071, val loss: 1.517, test loss: 1.193\n",
      "  Epoch: 1162, train loss: 1.078, val loss: 1.476, test loss: 1.208\n",
      "  Epoch: 1163, train loss: 1.021, val loss: 2.085, test loss: 1.457\n",
      "  Epoch: 1164, train loss: 1.091, val loss: 1.515, test loss: 1.310\n",
      "  Epoch: 1165, train loss: 1.036, val loss: 1.482, test loss: 1.169\n",
      "  Epoch: 1166, train loss: 1.018, val loss: 1.841, test loss: 1.238\n",
      "  Epoch: 1167, train loss: 1.015, val loss: 1.655, test loss: 1.108\n",
      "  Epoch: 1168, train loss: 1.039, val loss: 1.798, test loss: 1.277\n",
      "  Epoch: 1169, train loss: 1.006, val loss: 1.607, test loss: 1.598\n",
      "  Epoch: 1170, train loss: 0.976, val loss: 1.898, test loss: 1.339\n",
      "  Epoch: 1171, train loss: 1.007, val loss: 1.507, test loss: 1.193\n",
      "  Epoch: 1172, train loss: 1.063, val loss: 1.609, test loss: 1.560\n",
      "  Epoch: 1173, train loss: 0.990, val loss: 1.506, test loss: 1.282\n",
      "  Epoch: 1174, train loss: 1.010, val loss: 1.626, test loss: 1.157\n",
      "  Epoch: 1175, train loss: 1.022, val loss: 1.872, test loss: 1.264\n",
      "  Epoch: 1176, train loss: 1.034, val loss: 1.514, test loss: 1.299\n",
      "  Epoch: 1177, train loss: 1.001, val loss: 1.492, test loss: 1.144\n",
      "  Epoch: 1178, train loss: 1.036, val loss: 1.559, test loss: 1.357\n",
      "  Epoch: 1179, train loss: 1.054, val loss: 1.488, test loss: 1.190\n",
      "  Epoch: 1180, train loss: 1.023, val loss: 1.628, test loss: 1.527\n",
      "  Epoch: 1181, train loss: 1.034, val loss: 1.505, test loss: 1.209\n",
      "  Epoch: 1182, train loss: 1.000, val loss: 1.487, test loss: 1.308\n",
      "  Epoch: 1183, train loss: 1.053, val loss: 1.936, test loss: 1.305\n",
      "  Epoch: 1184, train loss: 1.001, val loss: 1.950, test loss: 1.339\n",
      "  Epoch: 1185, train loss: 0.977, val loss: 1.461, test loss: 1.202\n",
      "  Epoch: 1186, train loss: 0.980, val loss: 2.583, test loss: 1.810\n",
      "  Epoch: 1187, train loss: 1.017, val loss: 1.691, test loss: 1.196\n",
      "  Epoch: 1188, train loss: 1.034, val loss: 1.487, test loss: 1.279\n",
      "  Epoch: 1189, train loss: 0.995, val loss: 1.511, test loss: 1.120\n",
      "  Epoch: 1190, train loss: 1.025, val loss: 1.548, test loss: 1.432\n",
      "  Epoch: 1191, train loss: 1.032, val loss: 1.601, test loss: 1.139\n",
      "  Epoch: 1192, train loss: 1.061, val loss: 1.682, test loss: 1.247\n",
      "  Epoch: 1193, train loss: 1.084, val loss: 1.747, test loss: 1.215\n",
      "  Epoch: 1194, train loss: 1.125, val loss: 1.608, test loss: 1.477\n",
      "  Epoch: 1195, train loss: 0.975, val loss: 1.880, test loss: 1.337\n",
      "  Epoch: 1196, train loss: 0.998, val loss: 1.713, test loss: 1.175\n",
      "  Epoch: 1197, train loss: 1.027, val loss: 1.548, test loss: 1.538\n",
      "  Epoch: 1198, train loss: 1.071, val loss: 1.562, test loss: 1.455\n",
      "  Epoch: 1199, train loss: 1.038, val loss: 2.580, test loss: 1.918\n",
      "  Epoch: 1200, train loss: 1.010, val loss: 1.684, test loss: 1.126\n",
      "  Epoch: 1201, train loss: 1.007, val loss: 1.842, test loss: 1.241\n",
      "  Epoch: 1202, train loss: 1.055, val loss: 2.249, test loss: 1.733\n",
      "  Epoch: 1203, train loss: 1.119, val loss: 1.525, test loss: 1.459\n",
      "  Epoch: 1204, train loss: 1.014, val loss: 1.476, test loss: 1.048\n",
      "  Epoch: 1205, train loss: 0.987, val loss: 2.008, test loss: 1.347\n",
      "  Epoch: 1206, train loss: 1.035, val loss: 1.456, test loss: 1.202\n",
      "  Epoch: 1207, train loss: 1.047, val loss: 1.515, test loss: 1.353\n",
      "  Epoch: 1208, train loss: 1.003, val loss: 1.576, test loss: 1.122\n",
      "  Epoch: 1209, train loss: 0.984, val loss: 1.992, test loss: 1.434\n",
      "  Epoch: 1210, train loss: 1.057, val loss: 1.605, test loss: 1.587\n",
      "  Epoch: 1211, train loss: 0.998, val loss: 1.594, test loss: 1.093\n",
      "  Epoch: 1212, train loss: 0.977, val loss: 1.735, test loss: 1.176\n",
      "  Epoch: 1213, train loss: 0.979, val loss: 1.716, test loss: 1.231\n",
      "  Epoch: 1214, train loss: 1.012, val loss: 1.530, test loss: 1.474\n",
      "  Epoch: 1215, train loss: 0.978, val loss: 1.759, test loss: 1.180\n",
      "  Epoch: 1216, train loss: 1.023, val loss: 1.460, test loss: 1.144\n",
      "  Epoch: 1217, train loss: 1.006, val loss: 3.538, test loss: 2.978\n",
      "  Epoch: 1218, train loss: 1.016, val loss: 1.499, test loss: 1.252\n",
      "  Epoch: 1219, train loss: 1.007, val loss: 1.726, test loss: 1.156\n",
      "  Epoch: 1220, train loss: 1.022, val loss: 1.490, test loss: 1.407\n",
      "  Epoch: 1221, train loss: 0.956, val loss: 1.531, test loss: 1.228\n",
      "  Epoch: 1222, train loss: 0.974, val loss: 2.076, test loss: 1.376\n",
      "  Epoch: 1223, train loss: 0.959, val loss: 1.644, test loss: 1.109\n",
      "  Epoch: 1224, train loss: 1.017, val loss: 2.345, test loss: 1.747\n",
      "  Epoch: 1225, train loss: 1.003, val loss: 1.538, test loss: 1.432\n",
      "  Epoch: 1226, train loss: 0.991, val loss: 1.685, test loss: 1.714\n",
      "  Epoch: 1227, train loss: 1.023, val loss: 3.617, test loss: 2.829\n",
      "  Epoch: 1228, train loss: 1.041, val loss: 2.027, test loss: 1.405\n",
      "  Epoch: 1229, train loss: 0.965, val loss: 1.649, test loss: 1.103\n",
      "  Epoch: 1230, train loss: 1.002, val loss: 1.498, test loss: 1.183\n",
      "  Epoch: 1231, train loss: 1.006, val loss: 1.674, test loss: 1.192\n",
      "  Epoch: 1232, train loss: 0.979, val loss: 1.416, test loss: 1.150\n",
      "  Epoch: 1233, train loss: 1.006, val loss: 1.560, test loss: 1.441\n",
      "  Epoch: 1234, train loss: 1.041, val loss: 1.530, test loss: 1.467\n",
      "  Epoch: 1235, train loss: 1.011, val loss: 1.439, test loss: 1.217\n",
      "  Epoch: 1236, train loss: 1.026, val loss: 1.487, test loss: 1.117\n",
      "  Epoch: 1237, train loss: 1.048, val loss: 1.503, test loss: 1.131\n",
      "  Epoch: 1238, train loss: 1.009, val loss: 1.420, test loss: 1.202\n",
      "  Epoch: 1239, train loss: 1.027, val loss: 1.612, test loss: 1.142\n",
      "  Epoch: 1240, train loss: 0.995, val loss: 1.485, test loss: 1.367\n",
      "  Epoch: 1241, train loss: 0.953, val loss: 1.549, test loss: 1.113\n",
      "  Epoch: 1242, train loss: 0.960, val loss: 1.444, test loss: 1.174\n",
      "  Epoch: 1243, train loss: 1.001, val loss: 1.548, test loss: 1.385\n",
      "  Epoch: 1244, train loss: 1.030, val loss: 1.847, test loss: 1.260\n",
      "  Epoch: 1245, train loss: 1.002, val loss: 1.475, test loss: 1.240\n",
      "  Epoch: 1246, train loss: 0.988, val loss: 1.718, test loss: 1.195\n",
      "  Epoch: 1247, train loss: 0.998, val loss: 1.602, test loss: 1.092\n",
      "  Epoch: 1248, train loss: 0.992, val loss: 1.492, test loss: 1.264\n",
      "  Epoch: 1249, train loss: 0.940, val loss: 1.459, test loss: 1.189\n",
      "  Epoch: 1250, train loss: 1.017, val loss: 1.562, test loss: 1.087\n",
      "  Epoch: 1251, train loss: 0.986, val loss: 1.483, test loss: 1.399\n",
      "  Epoch: 1252, train loss: 0.944, val loss: 2.290, test loss: 1.599\n",
      "  Epoch: 1253, train loss: 0.948, val loss: 1.864, test loss: 1.286\n",
      "  Epoch: 1254, train loss: 0.988, val loss: 1.474, test loss: 1.289\n",
      "  Epoch: 1255, train loss: 1.017, val loss: 2.131, test loss: 1.501\n",
      "  Epoch: 1256, train loss: 0.997, val loss: 1.560, test loss: 1.320\n",
      "  Epoch: 1257, train loss: 1.005, val loss: 1.538, test loss: 1.290\n",
      "  Epoch: 1258, train loss: 1.002, val loss: 1.637, test loss: 1.628\n",
      "  Epoch: 1259, train loss: 0.982, val loss: 3.182, test loss: 2.397\n",
      "  Epoch: 1260, train loss: 0.964, val loss: 1.747, test loss: 1.188\n",
      "  Epoch: 1261, train loss: 0.978, val loss: 1.483, test loss: 1.103\n",
      "  Epoch: 1262, train loss: 1.021, val loss: 1.806, test loss: 1.205\n",
      "  Epoch: 1263, train loss: 1.006, val loss: 1.521, test loss: 1.084\n",
      "  Epoch: 1264, train loss: 0.982, val loss: 1.463, test loss: 1.192\n",
      "  Epoch: 1265, train loss: 0.947, val loss: 1.486, test loss: 1.130\n",
      "  Epoch: 1266, train loss: 1.003, val loss: 1.882, test loss: 1.347\n",
      "  Epoch: 1267, train loss: 0.960, val loss: 2.685, test loss: 2.170\n",
      "  Epoch: 1268, train loss: 0.983, val loss: 1.475, test loss: 1.119\n",
      "  Epoch: 1269, train loss: 1.016, val loss: 1.505, test loss: 1.407\n",
      "  Epoch: 1270, train loss: 1.006, val loss: 1.438, test loss: 1.129\n",
      "  Epoch: 1271, train loss: 1.017, val loss: 1.513, test loss: 1.138\n",
      "  Epoch: 1272, train loss: 0.998, val loss: 1.483, test loss: 1.233\n",
      "  Epoch: 1273, train loss: 0.948, val loss: 1.636, test loss: 1.127\n",
      "  Epoch: 1274, train loss: 0.953, val loss: 1.683, test loss: 1.175\n",
      "  Epoch: 1275, train loss: 0.924, val loss: 1.660, test loss: 1.144\n",
      "  Epoch: 1276, train loss: 1.056, val loss: 1.432, test loss: 1.213\n",
      "  Epoch: 1277, train loss: 0.947, val loss: 1.482, test loss: 1.205\n",
      "  Epoch: 1278, train loss: 0.924, val loss: 1.671, test loss: 1.132\n",
      "  Epoch: 1279, train loss: 1.035, val loss: 1.815, test loss: 1.322\n",
      "  Epoch: 1280, train loss: 0.963, val loss: 1.557, test loss: 1.490\n",
      "  Epoch: 1281, train loss: 0.981, val loss: 1.506, test loss: 1.412\n",
      "  Epoch: 1282, train loss: 1.010, val loss: 1.671, test loss: 1.608\n",
      "  Epoch: 1283, train loss: 1.027, val loss: 1.579, test loss: 1.566\n",
      "  Epoch: 1284, train loss: 0.960, val loss: 1.592, test loss: 1.105\n",
      "  Epoch: 1285, train loss: 1.052, val loss: 1.626, test loss: 1.145\n",
      "  Epoch: 1286, train loss: 1.065, val loss: 1.495, test loss: 1.413\n",
      "  Epoch: 1287, train loss: 0.935, val loss: 1.934, test loss: 1.397\n",
      "  Epoch: 1288, train loss: 0.943, val loss: 1.990, test loss: 1.373\n",
      "  Epoch: 1289, train loss: 0.991, val loss: 1.737, test loss: 1.222\n",
      "  Epoch: 1290, train loss: 0.968, val loss: 1.514, test loss: 1.149\n",
      "  Epoch: 1291, train loss: 1.019, val loss: 1.539, test loss: 1.511\n",
      "  Epoch: 1292, train loss: 0.961, val loss: 1.603, test loss: 1.085\n",
      "  Epoch: 1293, train loss: 0.941, val loss: 1.631, test loss: 1.094\n",
      "  Epoch: 1294, train loss: 1.010, val loss: 1.938, test loss: 1.417\n",
      "  Epoch: 1295, train loss: 0.999, val loss: 1.635, test loss: 1.145\n",
      "  Epoch: 1296, train loss: 0.944, val loss: 1.756, test loss: 1.196\n",
      "  Epoch: 1297, train loss: 1.044, val loss: 1.620, test loss: 1.548\n",
      "  Epoch: 1298, train loss: 0.962, val loss: 1.510, test loss: 1.206\n",
      "  Epoch: 1299, train loss: 0.934, val loss: 1.548, test loss: 1.270\n",
      "  Epoch: 1300, train loss: 0.970, val loss: 2.005, test loss: 1.324\n",
      "  Epoch: 1301, train loss: 0.984, val loss: 1.525, test loss: 1.322\n",
      "  Epoch: 1302, train loss: 0.967, val loss: 1.562, test loss: 1.372\n",
      "  Epoch: 1303, train loss: 0.990, val loss: 1.480, test loss: 1.260\n",
      "  Epoch: 1304, train loss: 0.959, val loss: 1.495, test loss: 1.130\n",
      "  Epoch: 1305, train loss: 0.997, val loss: 1.531, test loss: 1.190\n",
      "  Epoch: 1306, train loss: 0.936, val loss: 1.572, test loss: 1.382\n",
      "  Epoch: 1307, train loss: 0.914, val loss: 1.602, test loss: 1.083\n",
      "  Epoch: 1308, train loss: 0.938, val loss: 1.515, test loss: 1.356\n",
      "  Epoch: 1309, train loss: 0.979, val loss: 1.613, test loss: 1.120\n",
      "  Epoch: 1310, train loss: 0.985, val loss: 1.673, test loss: 1.148\n",
      "  Epoch: 1311, train loss: 0.939, val loss: 1.509, test loss: 1.317\n",
      "  Epoch: 1312, train loss: 0.967, val loss: 2.190, test loss: 1.501\n",
      "  Epoch: 1313, train loss: 0.935, val loss: 1.579, test loss: 1.461\n",
      "  Epoch: 1314, train loss: 0.963, val loss: 1.544, test loss: 1.136\n",
      "  Epoch: 1315, train loss: 0.942, val loss: 1.577, test loss: 1.535\n",
      "  Epoch: 1316, train loss: 0.999, val loss: 1.772, test loss: 1.177\n",
      "  Epoch: 1317, train loss: 0.986, val loss: 1.478, test loss: 1.181\n",
      "  Epoch: 1318, train loss: 0.903, val loss: 2.648, test loss: 1.943\n",
      "  Epoch: 1319, train loss: 0.968, val loss: 1.549, test loss: 1.099\n",
      "  Epoch: 1320, train loss: 0.972, val loss: 1.484, test loss: 1.253\n",
      "  Epoch: 1321, train loss: 1.019, val loss: 1.906, test loss: 1.340\n",
      "  Epoch: 1322, train loss: 0.919, val loss: 1.630, test loss: 1.174\n",
      "  Epoch: 1323, train loss: 0.924, val loss: 1.554, test loss: 1.165\n",
      "  Epoch: 1324, train loss: 1.002, val loss: 1.481, test loss: 1.278\n",
      "  Epoch: 1325, train loss: 0.931, val loss: 1.544, test loss: 1.174\n",
      "  Epoch: 1326, train loss: 0.910, val loss: 1.837, test loss: 1.222\n",
      "  Epoch: 1327, train loss: 0.947, val loss: 1.618, test loss: 1.089\n",
      "  Epoch: 1328, train loss: 0.971, val loss: 1.496, test loss: 1.100\n",
      "  Epoch: 1329, train loss: 0.934, val loss: 3.431, test loss: 2.747\n",
      "  Epoch: 1330, train loss: 1.014, val loss: 1.589, test loss: 1.185\n",
      "  Epoch: 1331, train loss: 1.000, val loss: 1.448, test loss: 1.226\n",
      "  Epoch: 1332, train loss: 0.931, val loss: 1.536, test loss: 1.157\n",
      "  Epoch: 1333, train loss: 0.891, val loss: 1.600, test loss: 1.201\n",
      "  Epoch: 1334, train loss: 0.973, val loss: 1.609, test loss: 1.124\n",
      "  Epoch: 1335, train loss: 0.955, val loss: 1.542, test loss: 1.449\n",
      "  Epoch: 1336, train loss: 0.926, val loss: 1.541, test loss: 1.399\n",
      "  Epoch: 1337, train loss: 0.898, val loss: 1.931, test loss: 1.253\n",
      "  Epoch: 1338, train loss: 0.963, val loss: 1.925, test loss: 1.296\n",
      "  Epoch: 1339, train loss: 1.034, val loss: 1.602, test loss: 1.653\n",
      "  Epoch: 1340, train loss: 0.934, val loss: 1.563, test loss: 1.134\n",
      "  Epoch: 1341, train loss: 1.005, val loss: 1.482, test loss: 1.304\n",
      "  Epoch: 1342, train loss: 1.011, val loss: 1.506, test loss: 1.317\n",
      "  Epoch: 1343, train loss: 0.960, val loss: 1.571, test loss: 1.280\n",
      "  Epoch: 1344, train loss: 0.979, val loss: 1.475, test loss: 1.171\n",
      "  Epoch: 1345, train loss: 0.965, val loss: 1.498, test loss: 1.306\n",
      "  Epoch: 1346, train loss: 0.901, val loss: 1.581, test loss: 1.109\n",
      "  Epoch: 1347, train loss: 0.917, val loss: 1.512, test loss: 1.158\n",
      "  Epoch: 1348, train loss: 0.911, val loss: 2.293, test loss: 1.586\n",
      "  Epoch: 1349, train loss: 0.923, val loss: 1.510, test loss: 1.264\n",
      "  Epoch: 1350, train loss: 0.916, val loss: 1.574, test loss: 1.362\n",
      "  Epoch: 1351, train loss: 0.935, val loss: 1.695, test loss: 1.126\n",
      "  Epoch: 1352, train loss: 1.066, val loss: 1.525, test loss: 1.149\n",
      "  Epoch: 1353, train loss: 0.948, val loss: 1.535, test loss: 1.387\n",
      "  Epoch: 1354, train loss: 0.928, val loss: 1.991, test loss: 1.373\n",
      "  Epoch: 1355, train loss: 0.930, val loss: 2.402, test loss: 1.694\n",
      "  Epoch: 1356, train loss: 0.997, val loss: 1.616, test loss: 1.477\n",
      "  Epoch: 1357, train loss: 0.929, val loss: 1.639, test loss: 1.190\n",
      "  Epoch: 1358, train loss: 0.925, val loss: 1.615, test loss: 1.233\n",
      "  Epoch: 1359, train loss: 0.956, val loss: 1.561, test loss: 1.238\n",
      "  Epoch: 1360, train loss: 0.927, val loss: 1.514, test loss: 1.244\n",
      "  Epoch: 1361, train loss: 0.969, val loss: 1.638, test loss: 1.339\n",
      "  Epoch: 1362, train loss: 1.012, val loss: 1.591, test loss: 1.521\n",
      "  Epoch: 1363, train loss: 0.977, val loss: 1.493, test loss: 1.099\n",
      "  Epoch: 1364, train loss: 0.966, val loss: 1.558, test loss: 1.526\n",
      "  Epoch: 1365, train loss: 0.925, val loss: 1.580, test loss: 1.144\n",
      "  Epoch: 1366, train loss: 0.987, val loss: 1.681, test loss: 1.118\n",
      "  Epoch: 1367, train loss: 0.961, val loss: 1.727, test loss: 1.213\n",
      "  Epoch: 1368, train loss: 0.972, val loss: 1.562, test loss: 1.304\n",
      "  Epoch: 1369, train loss: 0.975, val loss: 1.575, test loss: 1.406\n",
      "  Epoch: 1370, train loss: 0.970, val loss: 1.720, test loss: 1.233\n",
      "  Epoch: 1371, train loss: 0.982, val loss: 1.513, test loss: 1.205\n",
      "  Epoch: 1372, train loss: 0.922, val loss: 1.765, test loss: 1.195\n",
      "  Epoch: 1373, train loss: 0.887, val loss: 1.745, test loss: 1.185\n",
      "  Epoch: 1374, train loss: 0.963, val loss: 1.505, test loss: 1.255\n",
      "  Epoch: 1375, train loss: 0.954, val loss: 1.453, test loss: 1.351\n",
      "  Epoch: 1376, train loss: 1.016, val loss: 1.510, test loss: 1.293\n",
      "  Epoch: 1377, train loss: 1.004, val loss: 1.727, test loss: 1.165\n",
      "  Epoch: 1378, train loss: 0.943, val loss: 1.759, test loss: 1.229\n",
      "  Epoch: 1379, train loss: 0.931, val loss: 1.518, test loss: 1.218\n",
      "  Epoch: 1380, train loss: 0.927, val loss: 1.689, test loss: 1.203\n",
      "  Epoch: 1381, train loss: 0.902, val loss: 2.093, test loss: 1.507\n",
      "  Epoch: 1382, train loss: 1.007, val loss: 1.468, test loss: 1.331\n",
      "  Epoch: 1383, train loss: 0.962, val loss: 1.502, test loss: 1.337\n",
      "  Epoch: 1384, train loss: 0.945, val loss: 3.006, test loss: 2.476\n",
      "  Epoch: 1385, train loss: 0.958, val loss: 2.073, test loss: 1.544\n",
      "  Epoch: 1386, train loss: 0.983, val loss: 1.589, test loss: 1.407\n",
      "  Epoch: 1387, train loss: 0.934, val loss: 1.653, test loss: 1.627\n",
      "  Epoch: 1388, train loss: 0.945, val loss: 1.562, test loss: 1.464\n",
      "  Epoch: 1389, train loss: 0.913, val loss: 1.844, test loss: 1.325\n",
      "  Epoch: 1390, train loss: 0.874, val loss: 1.486, test loss: 1.196\n",
      "  Epoch: 1391, train loss: 0.895, val loss: 1.817, test loss: 1.271\n",
      "  Epoch: 1392, train loss: 0.993, val loss: 1.958, test loss: 1.353\n",
      "  Epoch: 1393, train loss: 0.959, val loss: 1.508, test loss: 1.178\n",
      "  Epoch: 1394, train loss: 1.047, val loss: 1.566, test loss: 1.244\n",
      "  Epoch: 1395, train loss: 0.922, val loss: 1.489, test loss: 1.110\n",
      "  Epoch: 1396, train loss: 0.966, val loss: 1.542, test loss: 1.321\n",
      "  Epoch: 1397, train loss: 0.915, val loss: 1.509, test loss: 1.349\n",
      "  Epoch: 1398, train loss: 0.974, val loss: 1.511, test loss: 1.275\n",
      "  Epoch: 1399, train loss: 0.923, val loss: 1.527, test loss: 1.086\n",
      "  Epoch: 1400, train loss: 0.923, val loss: 1.551, test loss: 1.331\n",
      "  Epoch: 1401, train loss: 0.944, val loss: 1.498, test loss: 1.326\n",
      "  Epoch: 1402, train loss: 0.994, val loss: 1.548, test loss: 1.377\n",
      "  Epoch: 1403, train loss: 0.941, val loss: 1.867, test loss: 1.281\n",
      "  Epoch: 1404, train loss: 0.915, val loss: 1.526, test loss: 1.456\n",
      "  Epoch: 1405, train loss: 0.975, val loss: 1.754, test loss: 1.736\n",
      "  Epoch: 1406, train loss: 1.010, val loss: 1.489, test loss: 1.342\n",
      "  Epoch: 1407, train loss: 0.905, val loss: 1.949, test loss: 1.349\n",
      "  Epoch: 1408, train loss: 0.927, val loss: 1.500, test loss: 1.157\n",
      "  Epoch: 1409, train loss: 0.952, val loss: 1.482, test loss: 1.278\n",
      "  Epoch: 1410, train loss: 0.943, val loss: 1.537, test loss: 1.159\n",
      "  Epoch: 1411, train loss: 0.942, val loss: 2.883, test loss: 2.222\n",
      "  Epoch: 1412, train loss: 0.952, val loss: 2.060, test loss: 1.444\n",
      "  Epoch: 1413, train loss: 0.925, val loss: 2.087, test loss: 1.465\n",
      "  Epoch: 1414, train loss: 1.010, val loss: 2.751, test loss: 2.235\n",
      "  Epoch: 1415, train loss: 0.954, val loss: 1.645, test loss: 1.699\n",
      "  Epoch: 1416, train loss: 0.963, val loss: 1.794, test loss: 1.268\n",
      "  Epoch: 1417, train loss: 0.952, val loss: 1.499, test loss: 1.230\n",
      "  Epoch: 1418, train loss: 0.928, val loss: 1.858, test loss: 1.271\n",
      "  Epoch: 1419, train loss: 0.896, val loss: 1.612, test loss: 1.134\n",
      "  Epoch: 1420, train loss: 0.945, val loss: 1.577, test loss: 1.509\n",
      "  Epoch: 1421, train loss: 0.983, val loss: 1.894, test loss: 1.364\n",
      "  Epoch: 1422, train loss: 0.881, val loss: 2.027, test loss: 1.447\n",
      "  Epoch: 1423, train loss: 0.891, val loss: 1.695, test loss: 1.212\n",
      "  Epoch: 1424, train loss: 0.968, val loss: 1.537, test loss: 1.113\n",
      "  Epoch: 1425, train loss: 0.949, val loss: 1.533, test loss: 1.164\n",
      "  Epoch: 1426, train loss: 0.927, val loss: 1.556, test loss: 1.143\n",
      "  Epoch: 1427, train loss: 1.027, val loss: 1.617, test loss: 1.482\n",
      "  Epoch: 1428, train loss: 0.955, val loss: 1.695, test loss: 1.245\n",
      "  Epoch: 1429, train loss: 0.941, val loss: 3.055, test loss: 2.554\n",
      "  Epoch: 1430, train loss: 0.926, val loss: 1.553, test loss: 1.098\n",
      "  Epoch: 1431, train loss: 1.007, val loss: 1.690, test loss: 1.213\n",
      "  Epoch: 1432, train loss: 0.924, val loss: 1.518, test loss: 1.164\n",
      "  Epoch: 1433, train loss: 0.927, val loss: 3.614, test loss: 2.871\n",
      "  Epoch: 1434, train loss: 0.955, val loss: 1.824, test loss: 1.364\n",
      "  Epoch: 1435, train loss: 1.009, val loss: 1.706, test loss: 1.250\n",
      "  Epoch: 1436, train loss: 0.929, val loss: 1.620, test loss: 1.242\n",
      "  Epoch: 1437, train loss: 0.890, val loss: 1.538, test loss: 1.299\n",
      "  Epoch: 1438, train loss: 0.973, val loss: 1.589, test loss: 1.550\n",
      "  Epoch: 1439, train loss: 0.877, val loss: 1.653, test loss: 1.188\n",
      "  Epoch: 1440, train loss: 0.886, val loss: 1.616, test loss: 1.175\n",
      "  Epoch: 1441, train loss: 0.914, val loss: 1.608, test loss: 1.144\n",
      "  Epoch: 1442, train loss: 0.945, val loss: 1.782, test loss: 1.295\n",
      "  Epoch: 1443, train loss: 0.961, val loss: 2.654, test loss: 2.191\n",
      "  Epoch: 1444, train loss: 0.897, val loss: 1.608, test loss: 1.319\n",
      "  Epoch: 1445, train loss: 0.947, val loss: 1.498, test loss: 1.183\n",
      "  Epoch: 1446, train loss: 0.909, val loss: 1.506, test loss: 1.123\n",
      "  Epoch: 1447, train loss: 0.859, val loss: 1.576, test loss: 1.332\n",
      "  Epoch: 1448, train loss: 0.917, val loss: 2.064, test loss: 1.521\n",
      "  Epoch: 1449, train loss: 0.904, val loss: 1.458, test loss: 1.201\n",
      "  Epoch: 1450, train loss: 0.870, val loss: 1.492, test loss: 1.178\n",
      "  Epoch: 1451, train loss: 0.925, val loss: 1.569, test loss: 1.427\n",
      "  Epoch: 1452, train loss: 0.915, val loss: 1.550, test loss: 1.375\n",
      "  Epoch: 1453, train loss: 1.000, val loss: 1.586, test loss: 1.443\n",
      "  Epoch: 1454, train loss: 0.925, val loss: 1.542, test loss: 1.146\n",
      "  Epoch: 1455, train loss: 0.917, val loss: 2.107, test loss: 1.495\n",
      "  Epoch: 1456, train loss: 0.944, val loss: 1.605, test loss: 1.335\n",
      "  Epoch: 1457, train loss: 0.942, val loss: 1.617, test loss: 1.234\n",
      "  Epoch: 1458, train loss: 0.971, val loss: 1.596, test loss: 1.462\n",
      "  Epoch: 1459, train loss: 0.875, val loss: 1.572, test loss: 1.134\n",
      "  Epoch: 1460, train loss: 0.869, val loss: 1.527, test loss: 1.206\n",
      "  Epoch: 1461, train loss: 0.890, val loss: 1.563, test loss: 1.264\n",
      "  Epoch: 1462, train loss: 0.888, val loss: 1.534, test loss: 1.261\n",
      "  Epoch: 1463, train loss: 0.919, val loss: 1.692, test loss: 1.621\n",
      "  Epoch: 1464, train loss: 0.850, val loss: 1.527, test loss: 1.340\n",
      "  Epoch: 1465, train loss: 0.897, val loss: 1.707, test loss: 1.212\n",
      "  Epoch: 1466, train loss: 0.902, val loss: 1.526, test loss: 1.218\n",
      "  Epoch: 1467, train loss: 0.853, val loss: 1.589, test loss: 1.178\n",
      "  Epoch: 1468, train loss: 0.874, val loss: 1.545, test loss: 1.363\n",
      "  Epoch: 1469, train loss: 0.958, val loss: 2.711, test loss: 2.152\n",
      "  Epoch: 1470, train loss: 0.904, val loss: 1.527, test loss: 1.304\n",
      "  Epoch: 1471, train loss: 0.894, val loss: 1.850, test loss: 1.257\n",
      "  Epoch: 1472, train loss: 0.919, val loss: 1.552, test loss: 1.264\n",
      "  Epoch: 1473, train loss: 0.955, val loss: 2.888, test loss: 2.108\n",
      "  Epoch: 1474, train loss: 0.940, val loss: 1.554, test loss: 1.342\n",
      "  Epoch: 1475, train loss: 0.947, val loss: 3.063, test loss: 2.699\n",
      "  Epoch: 1476, train loss: 0.919, val loss: 1.490, test loss: 1.243\n",
      "  Epoch: 1477, train loss: 0.884, val loss: 1.603, test loss: 1.445\n",
      "  Epoch: 1478, train loss: 0.882, val loss: 1.559, test loss: 1.269\n",
      "  Epoch: 1479, train loss: 0.896, val loss: 1.465, test loss: 1.205\n",
      "  Epoch: 1480, train loss: 0.923, val loss: 1.545, test loss: 1.273\n",
      "  Epoch: 1481, train loss: 0.908, val loss: 1.687, test loss: 1.326\n",
      "  Epoch: 1482, train loss: 0.960, val loss: 1.475, test loss: 1.227\n",
      "  Epoch: 1483, train loss: 0.870, val loss: 1.589, test loss: 1.168\n",
      "  Epoch: 1484, train loss: 0.909, val loss: 1.639, test loss: 1.277\n",
      "  Epoch: 1485, train loss: 0.867, val loss: 1.776, test loss: 1.222\n",
      "  Epoch: 1486, train loss: 0.933, val loss: 2.132, test loss: 1.571\n",
      "  Epoch: 1487, train loss: 0.922, val loss: 2.424, test loss: 1.771\n",
      "  Epoch: 1488, train loss: 0.946, val loss: 1.959, test loss: 1.344\n",
      "  Epoch: 1489, train loss: 0.845, val loss: 1.584, test loss: 1.386\n",
      "  Epoch: 1490, train loss: 0.865, val loss: 1.946, test loss: 1.441\n",
      "  Epoch: 1491, train loss: 0.869, val loss: 1.555, test loss: 1.328\n",
      "  Epoch: 1492, train loss: 0.865, val loss: 1.802, test loss: 1.268\n",
      "  Epoch: 1493, train loss: 0.870, val loss: 1.594, test loss: 1.395\n",
      "  Epoch: 1494, train loss: 0.954, val loss: 1.477, test loss: 1.176\n",
      "  Epoch: 1495, train loss: 0.897, val loss: 1.520, test loss: 1.420\n",
      "  Epoch: 1496, train loss: 0.924, val loss: 2.098, test loss: 1.671\n",
      "  Epoch: 1497, train loss: 0.941, val loss: 3.898, test loss: 3.297\n",
      "  Epoch: 1498, train loss: 0.852, val loss: 1.517, test loss: 1.304\n",
      "  Epoch: 1499, train loss: 0.877, val loss: 1.667, test loss: 1.601\n",
      "  Epoch: 1500, train loss: 0.860, val loss: 1.478, test loss: 1.255\n",
      "  Epoch: 1501, train loss: 0.892, val loss: 3.255, test loss: 2.574\n",
      "  Epoch: 1502, train loss: 0.920, val loss: 1.500, test loss: 1.119\n",
      "  Epoch: 1503, train loss: 0.867, val loss: 1.520, test loss: 1.151\n",
      "  Epoch: 1504, train loss: 0.905, val loss: 1.931, test loss: 1.382\n",
      "  Epoch: 1505, train loss: 0.904, val loss: 1.534, test loss: 1.141\n",
      "  Epoch: 1506, train loss: 0.892, val loss: 2.690, test loss: 2.168\n",
      "  Epoch: 1507, train loss: 0.890, val loss: 1.621, test loss: 1.169\n",
      "  Epoch: 1508, train loss: 0.906, val loss: 1.636, test loss: 1.232\n",
      "  Epoch: 1509, train loss: 0.901, val loss: 1.600, test loss: 1.124\n",
      "  Epoch: 1510, train loss: 0.911, val loss: 1.497, test loss: 1.321\n",
      "  Epoch: 1511, train loss: 0.955, val loss: 1.588, test loss: 1.189\n",
      "  Epoch: 1512, train loss: 0.938, val loss: 2.754, test loss: 2.024\n",
      "  Epoch: 1513, train loss: 0.884, val loss: 1.679, test loss: 1.209\n",
      "  Epoch: 1514, train loss: 0.880, val loss: 1.579, test loss: 1.403\n",
      "  Epoch: 1515, train loss: 0.950, val loss: 1.480, test loss: 1.195\n",
      "  Epoch: 1516, train loss: 0.864, val loss: 2.008, test loss: 1.487\n",
      "  Epoch: 1517, train loss: 0.882, val loss: 1.521, test loss: 1.164\n",
      "  Epoch: 1518, train loss: 0.911, val loss: 1.873, test loss: 1.347\n",
      "  Epoch: 1519, train loss: 0.872, val loss: 1.587, test loss: 1.135\n",
      "  Epoch: 1520, train loss: 0.896, val loss: 1.626, test loss: 1.354\n",
      "  Epoch: 1521, train loss: 0.842, val loss: 1.523, test loss: 1.338\n",
      "  Epoch: 1522, train loss: 0.869, val loss: 1.761, test loss: 1.275\n",
      "  Epoch: 1523, train loss: 0.874, val loss: 1.475, test loss: 1.137\n",
      "  Epoch: 1524, train loss: 0.903, val loss: 1.588, test loss: 1.487\n",
      "  Epoch: 1525, train loss: 0.867, val loss: 1.746, test loss: 1.214\n",
      "  Epoch: 1526, train loss: 0.911, val loss: 1.490, test loss: 1.218\n",
      "  Epoch: 1527, train loss: 0.822, val loss: 1.522, test loss: 1.250\n",
      "  Epoch: 1528, train loss: 0.909, val loss: 1.924, test loss: 1.647\n",
      "  Epoch: 1529, train loss: 0.905, val loss: 1.513, test loss: 1.384\n",
      "  Epoch: 1530, train loss: 0.853, val loss: 1.607, test loss: 1.430\n",
      "  Epoch: 1531, train loss: 0.922, val loss: 2.471, test loss: 1.872\n",
      "  Epoch: 1532, train loss: 0.884, val loss: 1.572, test loss: 1.410\n",
      "  Epoch: 1533, train loss: 0.849, val loss: 1.632, test loss: 1.170\n",
      "  Epoch: 1534, train loss: 0.847, val loss: 1.812, test loss: 1.366\n",
      "  Epoch: 1535, train loss: 0.916, val loss: 1.823, test loss: 1.298\n",
      "  Epoch: 1536, train loss: 0.846, val loss: 2.119, test loss: 1.608\n",
      "  Epoch: 1537, train loss: 0.874, val loss: 1.699, test loss: 1.290\n",
      "  Epoch: 1538, train loss: 0.931, val loss: 1.511, test loss: 1.138\n",
      "  Epoch: 1539, train loss: 0.960, val loss: 1.559, test loss: 1.342\n",
      "  Epoch: 1540, train loss: 0.944, val loss: 1.463, test loss: 1.253\n",
      "  Epoch: 1541, train loss: 0.861, val loss: 1.659, test loss: 1.351\n",
      "  Epoch: 1542, train loss: 0.873, val loss: 2.074, test loss: 1.618\n",
      "  Epoch: 1543, train loss: 0.857, val loss: 1.558, test loss: 1.156\n",
      "  Epoch: 1544, train loss: 0.896, val loss: 1.497, test loss: 1.265\n",
      "  Epoch: 1545, train loss: 0.851, val loss: 1.497, test loss: 1.176\n",
      "  Epoch: 1546, train loss: 0.854, val loss: 1.558, test loss: 1.150\n",
      "  Epoch: 1547, train loss: 0.882, val loss: 1.608, test loss: 1.465\n",
      "  Epoch: 1548, train loss: 0.873, val loss: 1.931, test loss: 1.423\n",
      "  Epoch: 1549, train loss: 0.880, val loss: 1.960, test loss: 1.412\n",
      "  Epoch: 1550, train loss: 0.875, val loss: 1.864, test loss: 1.414\n",
      "  Epoch: 1551, train loss: 0.835, val loss: 2.416, test loss: 1.862\n",
      "  Epoch: 1552, train loss: 0.884, val loss: 1.654, test loss: 1.197\n",
      "  Epoch: 1553, train loss: 0.886, val loss: 1.560, test loss: 1.238\n",
      "  Epoch: 1554, train loss: 0.886, val loss: 1.829, test loss: 1.234\n",
      "  Epoch: 1555, train loss: 0.888, val loss: 1.561, test loss: 1.190\n",
      "  Epoch: 1556, train loss: 0.904, val loss: 2.351, test loss: 1.711\n",
      "  Epoch: 1557, train loss: 0.951, val loss: 1.589, test loss: 1.187\n",
      "  Epoch: 1558, train loss: 0.879, val loss: 1.794, test loss: 1.306\n",
      "  Epoch: 1559, train loss: 0.920, val loss: 1.762, test loss: 1.760\n",
      "  Epoch: 1560, train loss: 0.838, val loss: 1.659, test loss: 1.633\n",
      "  Epoch: 1561, train loss: 0.891, val loss: 1.538, test loss: 1.207\n",
      "  Epoch: 1562, train loss: 0.863, val loss: 2.335, test loss: 1.794\n",
      "  Epoch: 1563, train loss: 0.867, val loss: 1.740, test loss: 1.700\n",
      "  Epoch: 1564, train loss: 0.885, val loss: 1.501, test loss: 1.216\n",
      "  Epoch: 1565, train loss: 0.922, val loss: 1.795, test loss: 1.292\n",
      "  Epoch: 1566, train loss: 0.897, val loss: 1.673, test loss: 1.216\n",
      "  Epoch: 1567, train loss: 0.815, val loss: 1.816, test loss: 1.297\n",
      "  Epoch: 1568, train loss: 0.872, val loss: 1.557, test loss: 1.328\n",
      "  Epoch: 1569, train loss: 0.880, val loss: 1.718, test loss: 1.751\n",
      "  Epoch: 1570, train loss: 0.836, val loss: 1.952, test loss: 1.497\n",
      "  Epoch: 1571, train loss: 0.839, val loss: 1.808, test loss: 1.367\n",
      "  Epoch: 1572, train loss: 0.919, val loss: 1.862, test loss: 1.431\n",
      "  Epoch: 1573, train loss: 0.869, val loss: 1.629, test loss: 1.507\n",
      "  Epoch: 1574, train loss: 0.856, val loss: 1.514, test loss: 1.164\n",
      "  Epoch: 1575, train loss: 0.845, val loss: 1.679, test loss: 1.223\n",
      "  Epoch: 1576, train loss: 0.936, val loss: 1.895, test loss: 1.485\n",
      "  Epoch: 1577, train loss: 1.028, val loss: 1.774, test loss: 1.423\n",
      "  Epoch: 1578, train loss: 0.857, val loss: 1.527, test loss: 1.376\n",
      "  Epoch: 1579, train loss: 0.854, val loss: 1.789, test loss: 1.233\n",
      "  Epoch: 1580, train loss: 0.840, val loss: 1.676, test loss: 1.175\n",
      "  Epoch: 1581, train loss: 0.873, val loss: 2.129, test loss: 1.618\n",
      "  Epoch: 1582, train loss: 0.875, val loss: 1.563, test loss: 1.350\n",
      "  Epoch: 1583, train loss: 0.823, val loss: 1.567, test loss: 1.167\n",
      "  Epoch: 1584, train loss: 0.846, val loss: 1.792, test loss: 1.290\n",
      "  Epoch: 1585, train loss: 0.834, val loss: 2.055, test loss: 1.556\n",
      "  Epoch: 1586, train loss: 0.879, val loss: 2.034, test loss: 1.510\n",
      "  Epoch: 1587, train loss: 0.842, val loss: 1.667, test loss: 1.198\n",
      "  Epoch: 1588, train loss: 0.847, val loss: 1.471, test loss: 1.245\n",
      "  Epoch: 1589, train loss: 0.852, val loss: 1.502, test loss: 1.182\n",
      "  Epoch: 1590, train loss: 0.887, val loss: 1.523, test loss: 1.330\n",
      "  Epoch: 1591, train loss: 0.866, val loss: 1.517, test loss: 1.146\n",
      "  Epoch: 1592, train loss: 0.861, val loss: 2.056, test loss: 1.569\n",
      "  Epoch: 1593, train loss: 0.867, val loss: 2.608, test loss: 2.071\n",
      "  Epoch: 1594, train loss: 0.904, val loss: 2.112, test loss: 1.648\n",
      "  Epoch: 1595, train loss: 0.908, val loss: 1.777, test loss: 1.368\n",
      "  Epoch: 1596, train loss: 0.927, val loss: 1.443, test loss: 1.198\n",
      "  Epoch: 1597, train loss: 0.881, val loss: 1.759, test loss: 1.325\n",
      "  Epoch: 1598, train loss: 0.845, val loss: 1.530, test loss: 1.405\n",
      "  Epoch: 1599, train loss: 0.919, val loss: 1.491, test loss: 1.126\n",
      "  Epoch: 1600, train loss: 0.859, val loss: 1.538, test loss: 1.237\n",
      "  Epoch: 1601, train loss: 0.867, val loss: 1.520, test loss: 1.201\n",
      "  Epoch: 1602, train loss: 0.860, val loss: 1.820, test loss: 1.331\n",
      "  Epoch: 1603, train loss: 0.854, val loss: 1.801, test loss: 1.321\n",
      "  Epoch: 1604, train loss: 0.840, val loss: 2.314, test loss: 1.765\n",
      "  Epoch: 1605, train loss: 0.811, val loss: 1.654, test loss: 1.214\n",
      "  Epoch: 1606, train loss: 0.826, val loss: 1.588, test loss: 1.205\n",
      "  Epoch: 1607, train loss: 0.862, val loss: 2.358, test loss: 1.669\n",
      "  Epoch: 1608, train loss: 0.899, val loss: 1.817, test loss: 1.418\n",
      "  Epoch: 1609, train loss: 0.937, val loss: 1.676, test loss: 1.138\n",
      "  Epoch: 1610, train loss: 0.937, val loss: 3.698, test loss: 2.636\n",
      "  Epoch: 1611, train loss: 0.919, val loss: 1.693, test loss: 1.225\n",
      "  Epoch: 1612, train loss: 0.876, val loss: 1.559, test loss: 1.232\n",
      "  Epoch: 1613, train loss: 0.931, val loss: 2.035, test loss: 1.599\n",
      "  Epoch: 1614, train loss: 0.890, val loss: 1.981, test loss: 1.453\n",
      "  Epoch: 1615, train loss: 0.838, val loss: 1.531, test loss: 1.168\n",
      "  Epoch: 1616, train loss: 0.917, val loss: 1.534, test loss: 1.147\n",
      "  Epoch: 1617, train loss: 0.868, val loss: 1.479, test loss: 1.210\n",
      "  Epoch: 1618, train loss: 0.847, val loss: 1.514, test loss: 1.202\n",
      "  Epoch: 1619, train loss: 0.933, val loss: 1.511, test loss: 1.244\n",
      "  Epoch: 1620, train loss: 0.818, val loss: 1.635, test loss: 1.333\n",
      "  Epoch: 1621, train loss: 0.852, val loss: 1.465, test loss: 1.318\n",
      "  Epoch: 1622, train loss: 0.897, val loss: 1.511, test loss: 1.143\n",
      "  Epoch: 1623, train loss: 0.869, val loss: 1.584, test loss: 1.170\n",
      "  Epoch: 1624, train loss: 0.864, val loss: 1.658, test loss: 1.191\n",
      "  Epoch: 1625, train loss: 0.837, val loss: 1.496, test loss: 1.238\n",
      "  Epoch: 1626, train loss: 0.903, val loss: 1.608, test loss: 1.308\n",
      "  Epoch: 1627, train loss: 0.833, val loss: 1.577, test loss: 1.178\n",
      "  Epoch: 1628, train loss: 0.855, val loss: 1.781, test loss: 1.246\n",
      "  Epoch: 1629, train loss: 0.816, val loss: 1.528, test loss: 1.374\n",
      "  Epoch: 1630, train loss: 0.854, val loss: 1.723, test loss: 1.286\n",
      "  Epoch: 1631, train loss: 0.869, val loss: 1.700, test loss: 1.314\n",
      "  Epoch: 1632, train loss: 0.861, val loss: 1.518, test loss: 1.376\n",
      "  Epoch: 1633, train loss: 0.889, val loss: 2.945, test loss: 2.535\n",
      "  Epoch: 1634, train loss: 0.820, val loss: 1.645, test loss: 1.245\n",
      "  Epoch: 1635, train loss: 0.853, val loss: 1.655, test loss: 1.161\n",
      "  Epoch: 1636, train loss: 0.861, val loss: 1.594, test loss: 1.483\n",
      "  Epoch: 1637, train loss: 0.860, val loss: 3.520, test loss: 2.720\n",
      "  Epoch: 1638, train loss: 0.915, val loss: 1.563, test loss: 1.457\n",
      "  Epoch: 1639, train loss: 0.929, val loss: 1.702, test loss: 1.219\n",
      "  Epoch: 1640, train loss: 0.874, val loss: 1.522, test loss: 1.349\n",
      "  Epoch: 1641, train loss: 0.874, val loss: 1.691, test loss: 1.300\n",
      "  Epoch: 1642, train loss: 0.823, val loss: 1.659, test loss: 1.235\n",
      "  Epoch: 1643, train loss: 0.882, val loss: 1.582, test loss: 1.451\n",
      "  Epoch: 1644, train loss: 0.914, val loss: 1.864, test loss: 1.480\n",
      "  Epoch: 1645, train loss: 0.855, val loss: 1.625, test loss: 1.145\n",
      "  Epoch: 1646, train loss: 0.807, val loss: 1.983, test loss: 1.409\n",
      "  Epoch: 1647, train loss: 0.819, val loss: 1.602, test loss: 1.253\n",
      "  Epoch: 1648, train loss: 0.854, val loss: 1.892, test loss: 1.396\n",
      "  Epoch: 1649, train loss: 0.844, val loss: 1.555, test loss: 1.211\n",
      "  Epoch: 1650, train loss: 0.822, val loss: 1.526, test loss: 1.175\n",
      "  Epoch: 1651, train loss: 0.833, val loss: 1.561, test loss: 1.187\n",
      "  Epoch: 1652, train loss: 0.876, val loss: 1.570, test loss: 1.207\n",
      "  Epoch: 1653, train loss: 0.870, val loss: 1.864, test loss: 1.899\n",
      "  Epoch: 1654, train loss: 0.852, val loss: 3.067, test loss: 2.459\n",
      "  Epoch: 1655, train loss: 0.833, val loss: 1.664, test loss: 1.797\n",
      "  Epoch: 1656, train loss: 0.857, val loss: 2.019, test loss: 1.516\n",
      "  Epoch: 1657, train loss: 0.787, val loss: 1.869, test loss: 1.335\n",
      "  Epoch: 1658, train loss: 0.872, val loss: 1.602, test loss: 1.187\n",
      "  Epoch: 1659, train loss: 0.859, val loss: 1.876, test loss: 1.408\n",
      "  Epoch: 1660, train loss: 0.841, val loss: 1.514, test loss: 1.287\n",
      "  Epoch: 1661, train loss: 0.907, val loss: 2.098, test loss: 1.791\n",
      "  Epoch: 1662, train loss: 0.912, val loss: 5.883, test loss: 5.398\n",
      "  Epoch: 1663, train loss: 0.867, val loss: 1.643, test loss: 1.566\n",
      "  Epoch: 1664, train loss: 0.804, val loss: 1.614, test loss: 1.426\n",
      "  Epoch: 1665, train loss: 0.884, val loss: 1.499, test loss: 1.288\n",
      "  Epoch: 1666, train loss: 0.876, val loss: 1.462, test loss: 1.210\n",
      "  Epoch: 1667, train loss: 0.869, val loss: 1.531, test loss: 1.347\n",
      "  Epoch: 1668, train loss: 0.910, val loss: 2.026, test loss: 1.556\n",
      "  Epoch: 1669, train loss: 0.901, val loss: 1.549, test loss: 1.434\n",
      "  Epoch: 1670, train loss: 0.819, val loss: 1.511, test loss: 1.139\n",
      "  Epoch: 1671, train loss: 0.835, val loss: 2.196, test loss: 1.786\n",
      "  Epoch: 1672, train loss: 0.903, val loss: 1.533, test loss: 1.144\n",
      "  Epoch: 1673, train loss: 0.815, val loss: 1.868, test loss: 1.320\n",
      "  Epoch: 1674, train loss: 0.909, val loss: 1.785, test loss: 1.363\n",
      "  Epoch: 1675, train loss: 0.872, val loss: 1.647, test loss: 1.157\n",
      "  Epoch: 1676, train loss: 0.852, val loss: 1.915, test loss: 1.335\n",
      "  Epoch: 1677, train loss: 0.851, val loss: 1.509, test loss: 1.233\n",
      "  Epoch: 1678, train loss: 0.836, val loss: 2.224, test loss: 1.736\n",
      "  Epoch: 1679, train loss: 0.860, val loss: 1.770, test loss: 1.915\n",
      "  Epoch: 1680, train loss: 0.863, val loss: 1.452, test loss: 1.221\n",
      "  Epoch: 1681, train loss: 0.792, val loss: 1.604, test loss: 1.337\n",
      "  Epoch: 1682, train loss: 0.823, val loss: 1.611, test loss: 1.439\n",
      "  Epoch: 1683, train loss: 0.876, val loss: 1.877, test loss: 1.315\n",
      "  Epoch: 1684, train loss: 0.833, val loss: 2.697, test loss: 2.077\n",
      "  Epoch: 1685, train loss: 0.858, val loss: 2.294, test loss: 1.708\n",
      "  Epoch: 1686, train loss: 0.904, val loss: 1.752, test loss: 1.267\n",
      "  Epoch: 1687, train loss: 0.804, val loss: 2.209, test loss: 1.696\n",
      "  Epoch: 1688, train loss: 0.822, val loss: 1.523, test loss: 1.200\n",
      "  Epoch: 1689, train loss: 0.839, val loss: 1.523, test loss: 1.153\n",
      "  Epoch: 1690, train loss: 0.837, val loss: 1.689, test loss: 1.235\n",
      "  Epoch: 1691, train loss: 0.838, val loss: 1.511, test loss: 1.196\n",
      "  Epoch: 1692, train loss: 0.848, val loss: 1.659, test loss: 1.293\n",
      "  Epoch: 1693, train loss: 0.823, val loss: 1.634, test loss: 1.185\n",
      "  Epoch: 1694, train loss: 0.808, val loss: 1.509, test loss: 1.309\n",
      "  Epoch: 1695, train loss: 0.871, val loss: 1.523, test loss: 1.254\n",
      "  Epoch: 1696, train loss: 0.890, val loss: 1.635, test loss: 1.335\n",
      "  Epoch: 1697, train loss: 0.838, val loss: 1.746, test loss: 1.247\n",
      "  Epoch: 1698, train loss: 0.864, val loss: 1.821, test loss: 1.292\n",
      "  Epoch: 1699, train loss: 0.803, val loss: 1.553, test loss: 1.164\n",
      "  Epoch: 1700, train loss: 0.824, val loss: 1.572, test loss: 1.415\n",
      "  Epoch: 1701, train loss: 0.871, val loss: 1.805, test loss: 1.426\n",
      "  Epoch: 1702, train loss: 0.885, val loss: 1.605, test loss: 1.349\n",
      "  Epoch: 1703, train loss: 0.876, val loss: 1.519, test loss: 1.330\n",
      "  Epoch: 1704, train loss: 0.836, val loss: 1.720, test loss: 1.294\n",
      "  Epoch: 1705, train loss: 0.815, val loss: 1.614, test loss: 1.584\n",
      "  Epoch: 1706, train loss: 0.853, val loss: 1.814, test loss: 1.384\n",
      "  Epoch: 1707, train loss: 0.902, val loss: 1.511, test loss: 1.237\n",
      "  Epoch: 1708, train loss: 0.830, val loss: 1.505, test loss: 1.245\n",
      "  Epoch: 1709, train loss: 0.818, val loss: 1.551, test loss: 1.323\n",
      "  Epoch: 1710, train loss: 0.839, val loss: 2.004, test loss: 1.446\n",
      "  Epoch: 1711, train loss: 0.828, val loss: 1.521, test loss: 1.216\n",
      "  Epoch: 1712, train loss: 0.813, val loss: 1.675, test loss: 1.362\n",
      "  Epoch: 1713, train loss: 0.832, val loss: 1.575, test loss: 1.258\n",
      "  Epoch: 1714, train loss: 0.883, val loss: 1.512, test loss: 1.202\n",
      "  Epoch: 1715, train loss: 0.859, val loss: 1.542, test loss: 1.340\n",
      "  Epoch: 1716, train loss: 0.900, val loss: 2.289, test loss: 1.730\n",
      "  Epoch: 1717, train loss: 0.833, val loss: 1.572, test loss: 1.261\n",
      "  Epoch: 1718, train loss: 0.806, val loss: 1.698, test loss: 1.223\n",
      "  Epoch: 1719, train loss: 0.857, val loss: 1.486, test loss: 1.186\n",
      "  Epoch: 1720, train loss: 0.815, val loss: 1.567, test loss: 1.281\n",
      "  Epoch: 1721, train loss: 0.813, val loss: 1.470, test loss: 1.276\n",
      "  Epoch: 1722, train loss: 0.848, val loss: 2.267, test loss: 1.854\n",
      "  Epoch: 1723, train loss: 0.846, val loss: 1.855, test loss: 1.401\n",
      "  Epoch: 1724, train loss: 0.836, val loss: 1.739, test loss: 1.225\n",
      "  Epoch: 1725, train loss: 0.863, val loss: 1.782, test loss: 1.304\n",
      "  Epoch: 1726, train loss: 0.906, val loss: 1.563, test loss: 1.243\n",
      "  Epoch: 1727, train loss: 0.879, val loss: 2.180, test loss: 1.787\n",
      "  Epoch: 1728, train loss: 0.866, val loss: 1.879, test loss: 1.543\n",
      "  Epoch: 1729, train loss: 0.833, val loss: 1.645, test loss: 1.445\n",
      "  Epoch: 1730, train loss: 0.805, val loss: 1.821, test loss: 1.354\n",
      "  Epoch: 1731, train loss: 0.802, val loss: 1.595, test loss: 1.409\n",
      "  Epoch: 1732, train loss: 0.844, val loss: 2.864, test loss: 2.356\n",
      "  Epoch: 1733, train loss: 0.864, val loss: 1.530, test loss: 1.154\n",
      "  Epoch: 1734, train loss: 0.830, val loss: 1.496, test loss: 1.253\n",
      "  Epoch: 1735, train loss: 0.791, val loss: 1.724, test loss: 1.336\n",
      "  Epoch: 1736, train loss: 0.799, val loss: 2.005, test loss: 1.497\n",
      "  Epoch: 1737, train loss: 0.871, val loss: 1.622, test loss: 1.246\n",
      "  Epoch: 1738, train loss: 0.850, val loss: 1.471, test loss: 1.295\n",
      "  Epoch: 1739, train loss: 0.821, val loss: 1.970, test loss: 1.429\n",
      "  Epoch: 1740, train loss: 0.767, val loss: 1.741, test loss: 1.302\n",
      "  Epoch: 1741, train loss: 0.820, val loss: 1.554, test loss: 1.314\n",
      "  Epoch: 1742, train loss: 0.757, val loss: 1.598, test loss: 1.171\n",
      "  Epoch: 1743, train loss: 0.886, val loss: 1.605, test loss: 1.241\n",
      "  Epoch: 1744, train loss: 0.885, val loss: 1.573, test loss: 1.355\n",
      "  Epoch: 1745, train loss: 0.844, val loss: 1.547, test loss: 1.265\n",
      "  Epoch: 1746, train loss: 0.836, val loss: 1.611, test loss: 1.177\n",
      "  Epoch: 1747, train loss: 0.827, val loss: 1.657, test loss: 1.335\n",
      "  Epoch: 1748, train loss: 0.813, val loss: 1.508, test loss: 1.254\n",
      "  Epoch: 1749, train loss: 0.858, val loss: 1.550, test loss: 1.365\n",
      "  Epoch: 1750, train loss: 0.840, val loss: 1.502, test loss: 1.210\n",
      "  Epoch: 1751, train loss: 0.908, val loss: 1.539, test loss: 1.155\n",
      "  Epoch: 1752, train loss: 0.863, val loss: 1.687, test loss: 1.254\n",
      "  Epoch: 1753, train loss: 0.833, val loss: 1.508, test loss: 1.322\n",
      "  Epoch: 1754, train loss: 0.823, val loss: 1.697, test loss: 1.243\n",
      "  Epoch: 1755, train loss: 0.853, val loss: 1.576, test loss: 1.302\n",
      "  Epoch: 1756, train loss: 0.838, val loss: 1.555, test loss: 1.412\n",
      "  Epoch: 1757, train loss: 0.854, val loss: 1.543, test loss: 1.205\n",
      "  Epoch: 1758, train loss: 0.842, val loss: 3.378, test loss: 2.775\n",
      "  Epoch: 1759, train loss: 0.822, val loss: 1.517, test loss: 1.400\n",
      "  Epoch: 1760, train loss: 0.795, val loss: 1.906, test loss: 1.397\n",
      "  Epoch: 1761, train loss: 0.781, val loss: 2.530, test loss: 2.074\n",
      "  Epoch: 1762, train loss: 0.806, val loss: 1.519, test loss: 1.328\n",
      "  Epoch: 1763, train loss: 0.799, val loss: 1.719, test loss: 1.330\n",
      "  Epoch: 1764, train loss: 0.862, val loss: 2.367, test loss: 1.865\n",
      "  Epoch: 1765, train loss: 0.817, val loss: 1.680, test loss: 1.548\n",
      "  Epoch: 1766, train loss: 0.875, val loss: 1.496, test loss: 1.206\n",
      "  Epoch: 1767, train loss: 0.814, val loss: 1.880, test loss: 1.389\n",
      "  Epoch: 1768, train loss: 0.853, val loss: 1.539, test loss: 1.317\n",
      "  Epoch: 1769, train loss: 0.824, val loss: 2.018, test loss: 1.762\n",
      "  Epoch: 1770, train loss: 0.916, val loss: 1.621, test loss: 1.646\n",
      "  Epoch: 1771, train loss: 0.868, val loss: 1.484, test loss: 1.328\n",
      "  Epoch: 1772, train loss: 0.795, val loss: 1.604, test loss: 1.418\n",
      "  Epoch: 1773, train loss: 0.801, val loss: 1.578, test loss: 1.561\n",
      "  Epoch: 1774, train loss: 0.798, val loss: 1.560, test loss: 1.270\n",
      "  Epoch: 1775, train loss: 0.819, val loss: 1.483, test loss: 1.293\n",
      "  Epoch: 1776, train loss: 0.779, val loss: 1.614, test loss: 1.535\n",
      "  Epoch: 1777, train loss: 0.798, val loss: 1.808, test loss: 1.284\n",
      "  Epoch: 1778, train loss: 0.837, val loss: 1.591, test loss: 1.350\n",
      "  Epoch: 1779, train loss: 0.858, val loss: 1.529, test loss: 1.378\n",
      "  Epoch: 1780, train loss: 0.879, val loss: 1.525, test loss: 1.351\n",
      "  Epoch: 1781, train loss: 0.855, val loss: 1.743, test loss: 1.718\n",
      "  Epoch: 1782, train loss: 0.800, val loss: 1.614, test loss: 1.196\n",
      "  Epoch: 1783, train loss: 0.833, val loss: 1.768, test loss: 1.386\n",
      "  Epoch: 1784, train loss: 0.840, val loss: 2.305, test loss: 1.673\n",
      "  Epoch: 1785, train loss: 0.879, val loss: 1.721, test loss: 1.454\n",
      "  Epoch: 1786, train loss: 0.822, val loss: 1.537, test loss: 1.283\n",
      "  Epoch: 1787, train loss: 0.844, val loss: 1.511, test loss: 1.343\n",
      "  Epoch: 1788, train loss: 0.874, val loss: 1.778, test loss: 1.890\n",
      "  Epoch: 1789, train loss: 0.838, val loss: 1.764, test loss: 1.263\n",
      "  Epoch: 1790, train loss: 0.866, val loss: 1.566, test loss: 1.345\n",
      "  Epoch: 1791, train loss: 0.812, val loss: 2.621, test loss: 2.173\n",
      "  Epoch: 1792, train loss: 0.856, val loss: 1.655, test loss: 1.755\n",
      "  Epoch: 1793, train loss: 0.832, val loss: 2.542, test loss: 2.111\n",
      "  Epoch: 1794, train loss: 0.814, val loss: 1.563, test loss: 1.406\n",
      "  Epoch: 1795, train loss: 0.843, val loss: 1.575, test loss: 1.539\n",
      "  Epoch: 1796, train loss: 0.803, val loss: 1.559, test loss: 1.227\n",
      "  Epoch: 1797, train loss: 0.785, val loss: 1.565, test loss: 1.171\n",
      "  Epoch: 1798, train loss: 0.748, val loss: 1.552, test loss: 1.319\n",
      "  Epoch: 1799, train loss: 0.820, val loss: 1.669, test loss: 1.183\n",
      "  Epoch: 1800, train loss: 0.867, val loss: 2.029, test loss: 1.463\n",
      "  Epoch: 1801, train loss: 0.830, val loss: 1.617, test loss: 1.173\n",
      "  Epoch: 1802, train loss: 0.870, val loss: 1.803, test loss: 1.284\n",
      "  Epoch: 1803, train loss: 0.831, val loss: 1.647, test loss: 1.489\n",
      "  Epoch: 1804, train loss: 0.860, val loss: 1.759, test loss: 1.274\n",
      "  Epoch: 1805, train loss: 0.827, val loss: 1.779, test loss: 1.319\n",
      "  Epoch: 1806, train loss: 0.847, val loss: 1.624, test loss: 1.599\n",
      "  Epoch: 1807, train loss: 0.867, val loss: 1.759, test loss: 1.632\n",
      "  Epoch: 1808, train loss: 0.785, val loss: 1.495, test loss: 1.296\n",
      "  Epoch: 1809, train loss: 0.746, val loss: 1.566, test loss: 1.302\n",
      "  Epoch: 1810, train loss: 0.798, val loss: 1.580, test loss: 1.374\n",
      "  Epoch: 1811, train loss: 0.813, val loss: 1.575, test loss: 1.201\n",
      "  Epoch: 1812, train loss: 0.796, val loss: 1.526, test loss: 1.422\n",
      "  Epoch: 1813, train loss: 0.774, val loss: 1.807, test loss: 1.307\n",
      "  Epoch: 1814, train loss: 0.779, val loss: 1.914, test loss: 1.438\n",
      "  Epoch: 1815, train loss: 0.881, val loss: 1.561, test loss: 1.554\n",
      "  Epoch: 1816, train loss: 0.793, val loss: 1.653, test loss: 1.167\n",
      "  Epoch: 1817, train loss: 0.862, val loss: 1.760, test loss: 1.388\n",
      "  Epoch: 1818, train loss: 0.794, val loss: 1.561, test loss: 1.222\n",
      "  Epoch: 1819, train loss: 0.790, val loss: 1.647, test loss: 1.165\n",
      "  Epoch: 1820, train loss: 0.834, val loss: 1.510, test loss: 1.254\n",
      "  Epoch: 1821, train loss: 0.843, val loss: 3.247, test loss: 2.785\n",
      "  Epoch: 1822, train loss: 0.911, val loss: 1.641, test loss: 1.251\n",
      "  Epoch: 1823, train loss: 0.808, val loss: 1.553, test loss: 1.418\n",
      "  Epoch: 1824, train loss: 0.800, val loss: 2.187, test loss: 1.629\n",
      "  Epoch: 1825, train loss: 0.909, val loss: 1.544, test loss: 1.291\n",
      "  Epoch: 1826, train loss: 0.799, val loss: 1.695, test loss: 1.254\n",
      "  Epoch: 1827, train loss: 0.786, val loss: 1.479, test loss: 1.290\n",
      "  Epoch: 1828, train loss: 0.803, val loss: 1.900, test loss: 1.383\n",
      "  Epoch: 1829, train loss: 0.803, val loss: 1.703, test loss: 1.165\n",
      "  Epoch: 1830, train loss: 0.815, val loss: 1.623, test loss: 1.577\n",
      "  Epoch: 1831, train loss: 0.846, val loss: 1.548, test loss: 1.191\n",
      "  Epoch: 1832, train loss: 0.830, val loss: 1.548, test loss: 1.330\n",
      "  Epoch: 1833, train loss: 0.776, val loss: 1.532, test loss: 1.419\n",
      "  Epoch: 1834, train loss: 0.793, val loss: 1.772, test loss: 1.228\n",
      "  Epoch: 1835, train loss: 0.787, val loss: 1.744, test loss: 1.559\n",
      "  Epoch: 1836, train loss: 0.797, val loss: 2.240, test loss: 1.742\n",
      "  Epoch: 1837, train loss: 0.837, val loss: 1.595, test loss: 1.363\n",
      "  Epoch: 1838, train loss: 0.799, val loss: 2.419, test loss: 1.816\n",
      "  Epoch: 1839, train loss: 0.848, val loss: 1.563, test loss: 1.607\n",
      "  Epoch: 1840, train loss: 0.807, val loss: 1.621, test loss: 1.303\n",
      "  Epoch: 1841, train loss: 0.780, val loss: 1.625, test loss: 1.145\n",
      "  Epoch: 1842, train loss: 0.810, val loss: 1.745, test loss: 1.274\n",
      "  Epoch: 1843, train loss: 0.812, val loss: 1.594, test loss: 1.477\n",
      "  Epoch: 1844, train loss: 0.836, val loss: 1.820, test loss: 1.430\n",
      "  Epoch: 1845, train loss: 0.845, val loss: 2.171, test loss: 1.691\n",
      "  Epoch: 1846, train loss: 0.801, val loss: 2.092, test loss: 1.579\n",
      "  Epoch: 1847, train loss: 0.798, val loss: 1.835, test loss: 1.865\n",
      "  Epoch: 1848, train loss: 0.840, val loss: 1.524, test loss: 1.402\n",
      "  Epoch: 1849, train loss: 0.800, val loss: 1.509, test loss: 1.297\n",
      "  Epoch: 1850, train loss: 0.811, val loss: 1.863, test loss: 1.465\n",
      "  Epoch: 1851, train loss: 0.816, val loss: 1.535, test loss: 1.251\n",
      "  Epoch: 1852, train loss: 0.755, val loss: 1.557, test loss: 1.253\n",
      "  Epoch: 1853, train loss: 0.825, val loss: 1.574, test loss: 1.503\n",
      "  Epoch: 1854, train loss: 0.847, val loss: 1.755, test loss: 1.298\n",
      "  Epoch: 1855, train loss: 0.854, val loss: 1.922, test loss: 1.422\n",
      "  Epoch: 1856, train loss: 0.849, val loss: 1.631, test loss: 1.250\n",
      "  Epoch: 1857, train loss: 0.896, val loss: 1.544, test loss: 1.534\n",
      "  Epoch: 1858, train loss: 0.823, val loss: 1.584, test loss: 1.266\n",
      "  Epoch: 1859, train loss: 0.790, val loss: 1.595, test loss: 1.299\n",
      "  Epoch: 1860, train loss: 0.777, val loss: 1.525, test loss: 1.212\n",
      "  Epoch: 1861, train loss: 0.841, val loss: 1.597, test loss: 1.410\n",
      "  Epoch: 1862, train loss: 0.908, val loss: 1.573, test loss: 1.212\n",
      "  Epoch: 1863, train loss: 0.824, val loss: 1.606, test loss: 1.178\n",
      "  Epoch: 1864, train loss: 0.780, val loss: 1.721, test loss: 1.212\n",
      "  Epoch: 1865, train loss: 0.782, val loss: 1.481, test loss: 1.313\n",
      "  Epoch: 1866, train loss: 0.771, val loss: 1.615, test loss: 1.485\n",
      "  Epoch: 1867, train loss: 0.839, val loss: 1.560, test loss: 1.425\n",
      "  Epoch: 1868, train loss: 0.833, val loss: 1.576, test loss: 1.267\n",
      "  Epoch: 1869, train loss: 0.843, val loss: 1.609, test loss: 1.297\n",
      "  Epoch: 1870, train loss: 0.753, val loss: 1.568, test loss: 1.227\n",
      "  Epoch: 1871, train loss: 0.785, val loss: 1.858, test loss: 1.259\n",
      "  Epoch: 1872, train loss: 0.828, val loss: 1.609, test loss: 1.339\n",
      "  Epoch: 1873, train loss: 0.826, val loss: 1.561, test loss: 1.475\n",
      "  Epoch: 1874, train loss: 0.793, val loss: 1.712, test loss: 1.295\n",
      "  Epoch: 1875, train loss: 0.806, val loss: 1.710, test loss: 1.453\n",
      "  Epoch: 1876, train loss: 0.829, val loss: 3.986, test loss: 3.325\n",
      "  Epoch: 1877, train loss: 0.813, val loss: 1.556, test loss: 1.274\n",
      "  Epoch: 1878, train loss: 0.858, val loss: 2.211, test loss: 1.743\n",
      "  Epoch: 1879, train loss: 0.794, val loss: 1.544, test loss: 1.361\n",
      "  Epoch: 1880, train loss: 0.827, val loss: 1.547, test loss: 1.377\n",
      "  Epoch: 1881, train loss: 0.809, val loss: 1.493, test loss: 1.302\n",
      "  Epoch: 1882, train loss: 0.850, val loss: 1.683, test loss: 1.263\n",
      "  Epoch: 1883, train loss: 0.822, val loss: 1.591, test loss: 1.256\n",
      "  Epoch: 1884, train loss: 0.837, val loss: 2.171, test loss: 1.628\n",
      "  Epoch: 1885, train loss: 0.783, val loss: 1.686, test loss: 1.655\n",
      "  Epoch: 1886, train loss: 0.816, val loss: 1.710, test loss: 1.316\n",
      "  Epoch: 1887, train loss: 0.781, val loss: 1.539, test loss: 1.230\n",
      "  Epoch: 1888, train loss: 0.768, val loss: 1.640, test loss: 1.187\n",
      "  Epoch: 1889, train loss: 0.715, val loss: 2.160, test loss: 1.600\n",
      "  Epoch: 1890, train loss: 0.858, val loss: 1.571, test loss: 1.237\n",
      "  Epoch: 1891, train loss: 0.807, val loss: 1.618, test loss: 1.170\n",
      "  Epoch: 1892, train loss: 0.786, val loss: 1.661, test loss: 1.344\n",
      "  Epoch: 1893, train loss: 0.880, val loss: 1.624, test loss: 1.472\n",
      "  Epoch: 1894, train loss: 0.898, val loss: 1.779, test loss: 1.861\n",
      "  Epoch: 1895, train loss: 0.824, val loss: 2.150, test loss: 1.692\n",
      "  Epoch: 1896, train loss: 0.881, val loss: 1.712, test loss: 1.493\n",
      "  Epoch: 1897, train loss: 0.854, val loss: 1.544, test loss: 1.500\n",
      "  Epoch: 1898, train loss: 0.850, val loss: 1.649, test loss: 1.185\n",
      "  Epoch: 1899, train loss: 0.810, val loss: 1.772, test loss: 1.282\n",
      "  Epoch: 1900, train loss: 0.797, val loss: 1.627, test loss: 1.248\n",
      "  Epoch: 1901, train loss: 0.774, val loss: 1.589, test loss: 1.210\n",
      "  Epoch: 1902, train loss: 0.791, val loss: 1.536, test loss: 1.475\n",
      "  Epoch: 1903, train loss: 0.826, val loss: 1.757, test loss: 1.213\n",
      "  Epoch: 1904, train loss: 0.789, val loss: 1.547, test loss: 1.349\n",
      "  Epoch: 1905, train loss: 0.807, val loss: 1.872, test loss: 1.367\n",
      "  Epoch: 1906, train loss: 0.792, val loss: 1.565, test loss: 1.238\n",
      "  Epoch: 1907, train loss: 0.811, val loss: 1.703, test loss: 1.224\n",
      "  Epoch: 1908, train loss: 0.786, val loss: 1.666, test loss: 1.223\n",
      "  Epoch: 1909, train loss: 0.816, val loss: 1.603, test loss: 1.349\n",
      "  Epoch: 1910, train loss: 0.830, val loss: 1.621, test loss: 1.476\n",
      "  Epoch: 1911, train loss: 0.802, val loss: 2.292, test loss: 1.833\n",
      "  Epoch: 1912, train loss: 0.868, val loss: 1.580, test loss: 1.195\n",
      "  Epoch: 1913, train loss: 0.841, val loss: 2.213, test loss: 1.779\n",
      "  Epoch: 1914, train loss: 0.832, val loss: 1.551, test loss: 1.313\n",
      "  Epoch: 1915, train loss: 0.909, val loss: 1.825, test loss: 1.932\n",
      "  Epoch: 1916, train loss: 0.800, val loss: 2.220, test loss: 1.700\n",
      "  Epoch: 1917, train loss: 0.892, val loss: 1.816, test loss: 1.307\n",
      "  Epoch: 1918, train loss: 0.856, val loss: 1.549, test loss: 1.207\n",
      "  Epoch: 1919, train loss: 0.852, val loss: 1.658, test loss: 1.196\n",
      "  Epoch: 1920, train loss: 0.870, val loss: 1.554, test loss: 1.246\n",
      "  Epoch: 1921, train loss: 0.804, val loss: 1.686, test loss: 1.220\n",
      "  Epoch: 1922, train loss: 0.817, val loss: 1.584, test loss: 1.400\n",
      "  Epoch: 1923, train loss: 0.789, val loss: 2.751, test loss: 2.305\n",
      "  Epoch: 1924, train loss: 0.820, val loss: 1.756, test loss: 1.653\n",
      "  Epoch: 1925, train loss: 0.802, val loss: 1.623, test loss: 1.244\n",
      "  Epoch: 1926, train loss: 0.803, val loss: 1.603, test loss: 1.193\n",
      "  Epoch: 1927, train loss: 0.767, val loss: 1.642, test loss: 1.197\n",
      "  Epoch: 1928, train loss: 0.802, val loss: 1.719, test loss: 1.209\n",
      "  Epoch: 1929, train loss: 0.839, val loss: 2.037, test loss: 1.592\n",
      "  Epoch: 1930, train loss: 0.852, val loss: 1.505, test loss: 1.336\n",
      "  Epoch: 1931, train loss: 0.838, val loss: 1.655, test loss: 1.273\n",
      "  Epoch: 1932, train loss: 0.836, val loss: 1.675, test loss: 1.234\n",
      "  Epoch: 1933, train loss: 0.809, val loss: 1.577, test loss: 1.406\n",
      "  Epoch: 1934, train loss: 0.839, val loss: 2.886, test loss: 2.422\n",
      "  Epoch: 1935, train loss: 0.811, val loss: 1.717, test loss: 1.241\n",
      "  Epoch: 1936, train loss: 0.765, val loss: 1.539, test loss: 1.248\n",
      "  Epoch: 1937, train loss: 0.804, val loss: 2.412, test loss: 1.949\n",
      "  Epoch: 1938, train loss: 0.825, val loss: 3.223, test loss: 2.482\n",
      "  Epoch: 1939, train loss: 0.773, val loss: 1.692, test loss: 1.712\n",
      "  Epoch: 1940, train loss: 0.769, val loss: 1.590, test loss: 1.320\n",
      "  Epoch: 1941, train loss: 0.764, val loss: 1.604, test loss: 1.490\n",
      "  Epoch: 1942, train loss: 0.854, val loss: 1.641, test loss: 1.632\n",
      "  Epoch: 1943, train loss: 0.875, val loss: 1.640, test loss: 1.246\n",
      "  Epoch: 1944, train loss: 0.850, val loss: 1.925, test loss: 2.015\n",
      "  Epoch: 1945, train loss: 0.817, val loss: 1.537, test loss: 1.484\n",
      "  Epoch: 1946, train loss: 0.805, val loss: 1.609, test loss: 1.269\n",
      "  Epoch: 1947, train loss: 0.768, val loss: 1.683, test loss: 1.585\n",
      "  Epoch: 1948, train loss: 0.829, val loss: 1.614, test loss: 1.314\n",
      "  Epoch: 1949, train loss: 0.767, val loss: 1.598, test loss: 1.285\n",
      "  Epoch: 1950, train loss: 0.758, val loss: 1.764, test loss: 1.315\n",
      "  Epoch: 1951, train loss: 0.776, val loss: 1.586, test loss: 1.402\n",
      "  Epoch: 1952, train loss: 0.795, val loss: 1.607, test loss: 1.411\n",
      "  Epoch: 1953, train loss: 0.832, val loss: 1.590, test loss: 1.212\n",
      "  Epoch: 1954, train loss: 0.819, val loss: 1.797, test loss: 1.409\n",
      "  Epoch: 1955, train loss: 0.780, val loss: 1.631, test loss: 1.537\n",
      "  Epoch: 1956, train loss: 0.822, val loss: 1.740, test loss: 1.248\n",
      "  Epoch: 1957, train loss: 0.754, val loss: 1.542, test loss: 1.297\n",
      "  Epoch: 1958, train loss: 0.807, val loss: 1.693, test loss: 1.730\n",
      "  Epoch: 1959, train loss: 0.771, val loss: 2.046, test loss: 1.576\n",
      "  Epoch: 1960, train loss: 0.769, val loss: 1.842, test loss: 1.334\n",
      "  Epoch: 1961, train loss: 0.858, val loss: 1.557, test loss: 1.378\n",
      "  Epoch: 1962, train loss: 0.845, val loss: 1.575, test loss: 1.240\n",
      "  Epoch: 1963, train loss: 0.777, val loss: 1.717, test loss: 1.209\n",
      "  Epoch: 1964, train loss: 0.794, val loss: 1.586, test loss: 1.195\n",
      "  Epoch: 1965, train loss: 0.760, val loss: 1.979, test loss: 1.447\n",
      "  Epoch: 1966, train loss: 0.739, val loss: 1.557, test loss: 1.370\n",
      "  Epoch: 1967, train loss: 0.791, val loss: 1.603, test loss: 1.265\n",
      "  Epoch: 1968, train loss: 0.807, val loss: 1.579, test loss: 1.341\n",
      "  Epoch: 1969, train loss: 0.770, val loss: 1.654, test loss: 1.294\n",
      "  Epoch: 1970, train loss: 0.734, val loss: 1.553, test loss: 1.412\n",
      "  Epoch: 1971, train loss: 0.757, val loss: 2.419, test loss: 1.871\n",
      "  Epoch: 1972, train loss: 0.779, val loss: 1.640, test loss: 1.530\n",
      "  Epoch: 1973, train loss: 0.802, val loss: 1.623, test loss: 1.324\n",
      "  Epoch: 1974, train loss: 0.774, val loss: 2.497, test loss: 1.897\n",
      "  Epoch: 1975, train loss: 0.784, val loss: 1.655, test loss: 1.380\n",
      "  Epoch: 1976, train loss: 0.811, val loss: 1.613, test loss: 1.438\n",
      "  Epoch: 1977, train loss: 0.836, val loss: 1.650, test loss: 1.605\n",
      "  Epoch: 1978, train loss: 0.765, val loss: 1.529, test loss: 1.498\n",
      "  Epoch: 1979, train loss: 0.804, val loss: 1.596, test loss: 1.295\n",
      "  Epoch: 1980, train loss: 0.807, val loss: 2.156, test loss: 1.595\n",
      "  Epoch: 1981, train loss: 0.798, val loss: 1.695, test loss: 1.399\n",
      "  Epoch: 1982, train loss: 0.857, val loss: 1.711, test loss: 1.732\n",
      "  Epoch: 1983, train loss: 0.759, val loss: 1.569, test loss: 1.440\n",
      "  Epoch: 1984, train loss: 0.801, val loss: 2.782, test loss: 2.295\n",
      "  Epoch: 1985, train loss: 0.847, val loss: 1.504, test loss: 1.289\n",
      "  Epoch: 1986, train loss: 0.877, val loss: 1.700, test loss: 1.559\n",
      "  Epoch: 1987, train loss: 0.820, val loss: 1.638, test loss: 1.225\n",
      "  Epoch: 1988, train loss: 0.843, val loss: 1.574, test loss: 1.565\n",
      "  Epoch: 1989, train loss: 0.823, val loss: 2.598, test loss: 2.140\n",
      "  Epoch: 1990, train loss: 0.785, val loss: 1.670, test loss: 1.298\n",
      "  Epoch: 1991, train loss: 0.751, val loss: 1.605, test loss: 1.142\n",
      "  Epoch: 1992, train loss: 0.811, val loss: 1.779, test loss: 1.398\n",
      "  Epoch: 1993, train loss: 0.857, val loss: 1.622, test loss: 1.482\n",
      "  Epoch: 1994, train loss: 0.827, val loss: 1.609, test loss: 1.163\n",
      "  Epoch: 1995, train loss: 0.732, val loss: 1.607, test loss: 1.188\n",
      "  Epoch: 1996, train loss: 0.825, val loss: 1.648, test loss: 1.267\n",
      "  Epoch: 1997, train loss: 0.768, val loss: 1.585, test loss: 1.390\n",
      "  Epoch: 1998, train loss: 0.819, val loss: 1.569, test loss: 1.501\n",
      "  Epoch: 1999, train loss: 0.801, val loss: 1.792, test loss: 1.268\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "\n",
    "best_val_index = -1\n",
    "\n",
    "training_start_time = time.time()\n",
    "time_array = []\n",
    "time_array.append(training_start_time)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_model(train_loader, model, optimizer).cpu().detach().numpy()\n",
    "    val_loss = test_model(val_loader, model).cpu().detach().numpy()\n",
    "    test_loss = test_model(test_loader, model).cpu().detach().numpy()\n",
    "    new_min = \" \"\n",
    "    if epoch > 0:\n",
    "        if val_losses[best_val_index] > val_loss:\n",
    "            new_min = \"*\"\n",
    "            best_val_index = epoch\n",
    "            torch.save(model.state_dict(), 'MLP_sex_1.pt') #this thankfully already saves the best val loss model\n",
    "        #temporarily disable early stoppage\n",
    "        # early stopping is called\n",
    "        if len(val_losses) - best_val_index > patience:\n",
    "            print (\"Early stopping, best val loss and index:\")\n",
    "            print(val_losses[best_val_index], best_val_index)\n",
    "            break\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    print(new_min, \"Epoch: %d, train loss: %1.3f, val loss: %1.3f, test loss: %1.3f\" % (epoch, train_loss, val_loss, test_loss))\n",
    "    time_array.append(time.time())\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcb1c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, color='b')\n",
    "plt.plot(val_losses, color='m')\n",
    "plt.plot(test_losses, color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve best val loss\n",
    "best_val_loss = val_losses[best_val_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa823a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time:  30.89  mins\n",
      "Average training time:  0.93  seconds\n",
      "Longest training time:  2.62 seconds\n",
      "Shortest training time:  0.91 seconds\n",
      "\n",
      "Best Val Loss:  1.2674819  at epoch num  283\n"
     ]
    }
   ],
   "source": [
    "#timing recordings\n",
    "\n",
    "end_time = time.time()\n",
    "total_train_time = end_time - training_start_time\n",
    "time_differences = []\n",
    "for i in range(len(time_array) - 1):\n",
    "    last_time = time_array[i]\n",
    "    this_time = time_array[i + 1]\n",
    "    diff = this_time - last_time\n",
    "    time_differences.append(diff)\n",
    "    \n",
    "\n",
    "average_time = total_train_time / len(time_array)\n",
    "\n",
    "print(\"Total time: \", round(total_train_time / 60, 2), \" mins\")\n",
    "print(\"Average training time: \", round(average_time, 2), \" seconds\")\n",
    "print(\"Longest training time: \", round(max(time_differences), 2), \"seconds\")\n",
    "print(\"Shortest training time: \", round(min(time_differences), 2), \"seconds\")\n",
    "print(\"\\nBest Val Loss: \", best_val_loss, \" at epoch num \", best_val_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59531369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_test import display_test_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc171f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (lin1): Linear(in_features=4, out_features=16, bias=True)\n",
       "  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lin2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lin3): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (bn3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lin4): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (bn4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lin5): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (lin6): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check against a saved version of the model\n",
    "model_choice = 'MLP_sex_1.pt'\n",
    "model = MLP(4, [16, 16, 16, 16], 1, device=device)\n",
    "model.load_state_dict(torch.load(model_choice))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This ran\n",
      "Will this loss work:  tensor(1.2675, device='cuda:0')\n",
      "test\n",
      "tensor(1.2675, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_loss, predictions, truths, mae = display_test_stats(val_loader, model)\n",
    "\n",
    "print(\"Will this loss work: \", test_loss)\n",
    "#print(\"Predictions: \", predictions, '\\n')\n",
    "print(\"test\")\n",
    "#print(\"Truths: \", truths)\n",
    "print(mae)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
