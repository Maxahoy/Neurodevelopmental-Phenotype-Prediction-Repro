{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f8b42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp import MLP\n",
    "from train_test import train_model, test_model\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df5d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'torch.version' from 'C:\\\\Users\\\\Maxwell\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\\\\torch\\\\version.py'>\n",
      "2.0.0+cu118\n",
      "11.8\n",
      "['sm_37', 'sm_50', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'sm_90', 'compute_37']\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.version)\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.get_arch_list())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad09dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/release/native/' # '../data/release/template/' for template space prediction\n",
    "task = 'scan_age' # 'birth_age' for birth age prediction\n",
    "#sex for sex prediction\n",
    "#scan_age for scan age prediction\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 2000\n",
    "patience = 200 # for early stopping\n",
    "lr = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665973e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File num:  1218\n"
     ]
    }
   ],
   "source": [
    "#check if datapath is right\n",
    "file_list = os.listdir(data_path)\n",
    "print('File num: ', len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45459d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>birth_age</th>\n",
       "      <th>scan_age</th>\n",
       "      <th>sex</th>\n",
       "      <th>birth_weight</th>\n",
       "      <th>head_circumference_scan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-CC00050XX01_ses-7201</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.29</td>\n",
       "      <td>female</td>\n",
       "      <td>3.910</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-CC00051XX02_ses-7702</td>\n",
       "      <td>39.857143</td>\n",
       "      <td>40.00</td>\n",
       "      <td>female</td>\n",
       "      <td>3.310</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-CC00052XX03_ses-8300</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>38.71</td>\n",
       "      <td>female</td>\n",
       "      <td>2.640</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-CC00053XX04_ses-8607</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.43</td>\n",
       "      <td>female</td>\n",
       "      <td>3.460</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-CC00054XX05_ses-8800</td>\n",
       "      <td>41.857143</td>\n",
       "      <td>42.14</td>\n",
       "      <td>male</td>\n",
       "      <td>3.690</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>sub-CC01232BN12_ses-152130</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.43</td>\n",
       "      <td>female</td>\n",
       "      <td>2.140</td>\n",
       "      <td>30.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>sub-CC01234AN14_ses-155030</td>\n",
       "      <td>32.857143</td>\n",
       "      <td>33.29</td>\n",
       "      <td>female</td>\n",
       "      <td>2.280</td>\n",
       "      <td>30.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>sub-CC01234BN14_ses-155230</td>\n",
       "      <td>32.857143</td>\n",
       "      <td>33.43</td>\n",
       "      <td>female</td>\n",
       "      <td>2.150</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>sub-CC01236XX16_ses-155830</td>\n",
       "      <td>40.714286</td>\n",
       "      <td>44.43</td>\n",
       "      <td>female</td>\n",
       "      <td>3.675</td>\n",
       "      <td>36.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>sub-CC00908XX17_ses-37030</td>\n",
       "      <td>40.428571</td>\n",
       "      <td>42.43</td>\n",
       "      <td>male</td>\n",
       "      <td>4.120</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>887 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             ID  birth_age  scan_age     sex  birth_weight   \n",
       "0      sub-CC00050XX01_ses-7201  43.000000     43.29  female         3.910  \\\n",
       "1      sub-CC00051XX02_ses-7702  39.857143     40.00  female         3.310   \n",
       "2      sub-CC00052XX03_ses-8300  38.000000     38.71  female         2.640   \n",
       "3      sub-CC00053XX04_ses-8607  40.000000     40.43  female         3.460   \n",
       "4      sub-CC00054XX05_ses-8800  41.857143     42.14    male         3.690   \n",
       "..                          ...        ...       ...     ...           ...   \n",
       "882  sub-CC01232BN12_ses-152130  35.000000     35.43  female         2.140   \n",
       "883  sub-CC01234AN14_ses-155030  32.857143     33.29  female         2.280   \n",
       "884  sub-CC01234BN14_ses-155230  32.857143     33.43  female         2.150   \n",
       "885  sub-CC01236XX16_ses-155830  40.714286     44.43  female         3.675   \n",
       "886   sub-CC00908XX17_ses-37030  40.428571     42.43    male         4.120   \n",
       "\n",
       "     head_circumference_scan  \n",
       "0                       37.0  \n",
       "1                       35.0  \n",
       "2                       33.0  \n",
       "3                       32.0  \n",
       "4                       35.0  \n",
       "..                       ...  \n",
       "882                     30.1  \n",
       "883                     30.5  \n",
       "884                     31.0  \n",
       "885                     36.5  \n",
       "886                     37.0  \n",
       "\n",
       "[887 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids = np.loadtxt('indices/' + task + '_train.txt', dtype='str')\n",
    "val_ids = np.loadtxt('indices/' + task + '_validation.txt', dtype='str')\n",
    "test_ids = np.loadtxt('indices/' + task + '_test.txt', dtype='str')\n",
    "\n",
    "#reshuffle versions\n",
    "train_ids = np.loadtxt('indices/' + task + '_train_reshuffle.txt', dtype='str')\n",
    "val_ids = np.loadtxt('indices/' + task + '_val_reshuffle.txt', dtype='str')\n",
    "test_ids = np.loadtxt('indices/' + task + '_test_reshuffle.txt', dtype='str')\n",
    "\n",
    "mirror_index = np.load('mirror_index.npy') # mirrors right hemispheres to match with left hemispheres\n",
    "\n",
    "df = pd.read_csv(\"combined.csv\")\n",
    "\n",
    "df.insert(0, \"ID\", \"sub-\" + df[\"participant_id\"] + \"_\" + \"ses-\" + df[\"session_id\"].apply(str))\n",
    "df.drop(\"participant_id\", axis=1, inplace=True)\n",
    "df.drop(\"session_id\", axis=1, inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5723ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set element sub-CC00307XX10_ses-98800 does not exist\n",
      "train set element sub-CC00061XX04_ses-13300 does not exist\n",
      "train set element sub-CC00501XX06_ses-146500 does not exist\n",
      "train set element sub-CC00291XX12_ses-93100 does not exist\n",
      "train set element sub-CC00439XX19_ses-132100 does not exist\n",
      "train set element sub-CC00143AN12_ses-47501 does not exist\n",
      "train set element sub-CC00319XX14_ses-117300 does not exist\n",
      "train set element sub-CC00221XX07_ses-75000 does not exist\n",
      "train set element sub-CC00170XX06_ses-56100 does not exist\n",
      "train set element sub-CC00442XX14_ses-133300 does not exist\n",
      "train set element sub-CC00217XX11_ses-73700 does not exist\n",
      "train set element sub-CC00341XX12_ses-108000 does not exist\n",
      "train set element sub-CC00468XX15_ses-139100 does not exist\n",
      "train set element sub-CC00084XX11_ses-31201 does not exist\n",
      "train set element sub-CC00371XX09_ses-134700 does not exist\n"
     ]
    }
   ],
   "source": [
    "def get_data(data_path, task, ids):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for _id in ids:\n",
    "        try:\n",
    "            img_L = nib.load(data_path + _id + '_left.shape.gii')\n",
    "            x_L = np.stack(img_L.agg_data(), axis=1)\n",
    "            for i in range(4):\n",
    "                # replaces the zeros of the medial wall cut area with mean values\n",
    "                x_L[:, i][x_L[:, i] == 0] = np.mean(x_L[:, i][x_L[:, i] != 0])\n",
    "            if task == 'birth_age' or task == 'scan_age' or task == 'sex':\n",
    "                xs.append(x_L.astype(np.float32))\n",
    "            y = np.array([df.loc[df['ID'] == _id, task].item()])\n",
    "            if task == 'birth_age' or task == 'scan_age':\n",
    "                ys.append(y.astype(np.float32))\n",
    "            elif task == 'sex':\n",
    "                #print('y[0]: ', y[0])\n",
    "                sex_indicator = np.zeros(1)\n",
    "                if y[0] == 'male':\n",
    "                    sex_indicator += 1\n",
    "                #print(\"this works\")\n",
    "                ys.append(sex_indicator.astype(np.float32))    \n",
    "                #print(\"this doesn't\")            \n",
    "            \n",
    "            img_R = nib.load(data_path + _id + '_right.shape.gii')\n",
    "            x_R = np.stack(img_R.agg_data(), axis=1)[mirror_index] # mirroring\n",
    "            for i in range(4):\n",
    "                # replaces the zeros of the medial wall cut area with mean values\n",
    "                x_R[:, i][x_R[:, i] == 0] = np.mean(x_R[:, i][x_R[:, i] != 0])\n",
    "            if task == 'birth_age' or task == 'scan_age' or task == 'sex':\n",
    "                xs.append(x_R.astype(np.float32))\n",
    "\n",
    "            y = np.array([df.loc[df['ID'] == _id, task].item()])\n",
    "            if task == 'birth_age' or task == 'scan_age':\n",
    "                ys.append(y.astype(np.float32))\n",
    "            elif task == 'sex':\n",
    "                #print('y[0]: ', y[0])\n",
    "                sex_indicator = np.zeros(1)\n",
    "                if y[0] == 'male':\n",
    "                    sex_indicator += 1\n",
    "                ys.append(sex_indicator.astype(np.float32))  \n",
    "        except:\n",
    "            print('train set element %s does not exist' % _id)\n",
    "    return xs, ys\n",
    "\n",
    "train_xs, train_ys = get_data(data_path, task, train_ids)\n",
    "val_xs, val_ys = get_data(data_path, task, val_ids)\n",
    "test_xs, test_ys = get_data(data_path, task, test_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np_test = np.array([np.array(xi) for xi in train_xs])\n",
    "# vstack makes it into a 2d array when I still need 3d...\n",
    "#np_test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3f060e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc177236",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_list_train_xs = [len(inner_list) for inner_list in train_xs]\n",
    "mode = max(set(length_list_train_xs), key=length_list_train_xs.count)\n",
    "offsize_list_ind = [i for i in range(len(length_list_train_xs)) if length_list_train_xs[i] != mode]\n",
    "only_correct_sized_train_xs = [train_xs[i] for i in range(len(length_list_train_xs)) if length_list_train_xs[i] == mode]\n",
    "train_xs = np.transpose(only_correct_sized_train_xs, axes=[1, 2, 0])\n",
    "\n",
    "length_list_val_xs = [len(inner_list) for inner_list in val_xs]\n",
    "mode = max(set(length_list_val_xs), key=length_list_val_xs.count)\n",
    "offsize_list_ind = [i for i in range(len(length_list_val_xs)) if length_list_val_xs[i] != mode]\n",
    "only_correct_sized_val_xs = [val_xs[i] for i in range(len(length_list_val_xs)) if length_list_val_xs[i] == mode]\n",
    "val_xs = np.transpose(only_correct_sized_val_xs, axes=[1, 2, 0])\n",
    "\n",
    "length_list_test_xs = [len(inner_list) for inner_list in test_xs]\n",
    "mode = max(set(length_list_test_xs), key=length_list_test_xs.count)\n",
    "offsize_list_ind = [i for i in range(len(length_list_test_xs)) if length_list_test_xs[i] != mode]\n",
    "only_correct_sized_test_xs = [test_xs[i] for i in range(len(length_list_test_xs)) if length_list_test_xs[i] == mode]\n",
    "test_xs = np.transpose(only_correct_sized_test_xs, axes=[1, 2, 0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 818)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_xs, dtype=object)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25881ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only_correct_sized_train_xs = [train_xs[i] for i in range(len(length_list)) if length_list[i] == mode]\n",
    "#train_xs = np.transpose(only_correct_sized_train_xs, axes=[1, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6054185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data standardization\n",
    "\n",
    "#why doesn't this match?\n",
    "#train_xs = np.transpose(np.array(train_xs, dtype=object), axes=[1, 2, 0])\n",
    "means  = np.mean(np.mean(train_xs, axis=2), axis=0) # means of the 4 channels in the train set\n",
    "stds  = np.std(np.std(train_xs, axis=2), axis=0) # stds of the 4 channels in the train set\n",
    "train_xs = (train_xs - means.reshape(1, means.shape[0], 1)) / stds.reshape(1, means.shape[0], 1)\n",
    "train_xs = np.transpose(train_xs, axes=[2, 0, 1])\n",
    "\n",
    "#val_xs = np.transpose(val_xs, axes=[1, 2, 0])\n",
    "val_xs = (val_xs - means.reshape(1, means.shape[0], 1)) / stds.reshape(1, means.shape[0], 1)\n",
    "val_xs = np.transpose(val_xs, axes=[2, 0, 1])\n",
    "\n",
    "#test_xs = np.transpose(test_xs, axes=[1, 2, 0])\n",
    "test_xs = (test_xs - means.reshape(1, means.shape[0], 1)) / stds.reshape(1, means.shape[0], 1)\n",
    "test_xs = np.transpose(test_xs, axes=[2, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fb018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6197f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = [(torch.from_numpy(x), torch.from_numpy(y)) for x, y in zip(train_xs, train_ys)]\n",
    "\n",
    "val_subset = [(torch.from_numpy(x), torch.from_numpy(y)) for x, y in zip(val_xs, val_ys)]\n",
    "\n",
    "test_subset = [(torch.from_numpy(x), torch.from_numpy(y)) for x, y in zip(test_xs, test_ys)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3615f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (lin1): Linear(in_features=4, out_features=16, bias=True)\n",
      "  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lin2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lin3): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (bn3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lin4): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (bn4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lin5): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (lin6): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "Number of parameters:  1313\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=len(val_subset), shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_subset, batch_size=len(test_subset), shuffle=False)\n",
    "\n",
    "model = MLP(4, [16, 16, 16, 16], 1, device=device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "print(model)\n",
    "print('Number of parameters: ', sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58185799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch: 0, train loss: 39.383, val loss: 39.728, test loss: 39.348\n",
      "* Epoch: 1, train loss: 39.168, val loss: 39.273, test loss: 38.871\n",
      "* Epoch: 2, train loss: 38.799, val loss: 38.700, test loss: 38.256\n",
      "* Epoch: 3, train loss: 38.170, val loss: 38.208, test loss: 37.740\n",
      "* Epoch: 4, train loss: 37.199, val loss: 37.257, test loss: 36.866\n",
      "* Epoch: 5, train loss: 35.986, val loss: 35.943, test loss: 35.564\n",
      "* Epoch: 6, train loss: 34.837, val loss: 34.835, test loss: 34.458\n",
      "* Epoch: 7, train loss: 33.851, val loss: 33.924, test loss: 33.547\n",
      "* Epoch: 8, train loss: 33.002, val loss: 33.148, test loss: 32.772\n",
      "* Epoch: 9, train loss: 32.287, val loss: 32.459, test loss: 32.083\n",
      "* Epoch: 10, train loss: 31.602, val loss: 31.830, test loss: 31.454\n",
      "* Epoch: 11, train loss: 31.006, val loss: 31.240, test loss: 30.864\n",
      "* Epoch: 12, train loss: 30.469, val loss: 30.685, test loss: 30.309\n",
      "* Epoch: 13, train loss: 29.890, val loss: 30.152, test loss: 29.777\n",
      "* Epoch: 14, train loss: 29.354, val loss: 29.638, test loss: 29.262\n",
      "* Epoch: 15, train loss: 28.897, val loss: 29.142, test loss: 28.766\n",
      "* Epoch: 16, train loss: 28.379, val loss: 28.657, test loss: 28.282\n",
      "* Epoch: 17, train loss: 27.927, val loss: 28.186, test loss: 27.811\n",
      "* Epoch: 18, train loss: 27.426, val loss: 27.723, test loss: 27.347\n",
      "* Epoch: 19, train loss: 26.997, val loss: 27.270, test loss: 26.894\n",
      "* Epoch: 20, train loss: 26.541, val loss: 26.825, test loss: 26.449\n",
      "* Epoch: 21, train loss: 26.094, val loss: 26.386, test loss: 26.011\n",
      "* Epoch: 22, train loss: 25.663, val loss: 25.955, test loss: 25.580\n",
      "* Epoch: 23, train loss: 25.226, val loss: 25.531, test loss: 25.155\n",
      "* Epoch: 24, train loss: 24.816, val loss: 25.112, test loss: 24.736\n",
      "* Epoch: 25, train loss: 24.391, val loss: 24.700, test loss: 24.325\n",
      "* Epoch: 26, train loss: 23.951, val loss: 24.291, test loss: 23.916\n",
      "* Epoch: 27, train loss: 23.590, val loss: 23.892, test loss: 23.517\n",
      "* Epoch: 28, train loss: 23.163, val loss: 23.494, test loss: 23.118\n",
      "* Epoch: 29, train loss: 22.775, val loss: 23.102, test loss: 22.727\n",
      "* Epoch: 30, train loss: 22.359, val loss: 22.714, test loss: 22.339\n",
      "* Epoch: 31, train loss: 21.999, val loss: 22.330, test loss: 21.955\n",
      "* Epoch: 32, train loss: 21.631, val loss: 21.953, test loss: 21.578\n",
      "* Epoch: 33, train loss: 21.234, val loss: 21.578, test loss: 21.203\n",
      "* Epoch: 34, train loss: 20.884, val loss: 21.209, test loss: 20.834\n",
      "* Epoch: 35, train loss: 20.510, val loss: 20.842, test loss: 20.467\n",
      "* Epoch: 36, train loss: 20.143, val loss: 20.480, test loss: 20.104\n",
      "* Epoch: 37, train loss: 19.790, val loss: 20.121, test loss: 19.746\n",
      "* Epoch: 38, train loss: 19.434, val loss: 19.766, test loss: 19.390\n",
      "* Epoch: 39, train loss: 19.098, val loss: 19.415, test loss: 19.040\n",
      "* Epoch: 40, train loss: 18.698, val loss: 19.067, test loss: 18.692\n",
      "* Epoch: 41, train loss: 18.386, val loss: 18.726, test loss: 18.350\n",
      "* Epoch: 42, train loss: 18.013, val loss: 18.385, test loss: 18.009\n",
      "* Epoch: 43, train loss: 17.721, val loss: 18.050, test loss: 17.675\n",
      "* Epoch: 44, train loss: 17.400, val loss: 17.717, test loss: 17.342\n",
      "* Epoch: 45, train loss: 17.040, val loss: 17.388, test loss: 17.012\n",
      "* Epoch: 46, train loss: 16.706, val loss: 17.061, test loss: 16.686\n",
      "* Epoch: 47, train loss: 16.370, val loss: 16.741, test loss: 16.365\n",
      "* Epoch: 48, train loss: 16.071, val loss: 16.421, test loss: 16.046\n",
      "* Epoch: 49, train loss: 15.737, val loss: 16.107, test loss: 15.732\n",
      "* Epoch: 50, train loss: 15.423, val loss: 15.795, test loss: 15.420\n",
      "* Epoch: 51, train loss: 15.133, val loss: 15.488, test loss: 15.112\n",
      "* Epoch: 52, train loss: 14.827, val loss: 15.184, test loss: 14.809\n",
      "* Epoch: 53, train loss: 14.513, val loss: 14.881, test loss: 14.506\n",
      "* Epoch: 54, train loss: 14.214, val loss: 14.584, test loss: 14.208\n",
      "* Epoch: 55, train loss: 13.933, val loss: 14.290, test loss: 13.914\n",
      "* Epoch: 56, train loss: 13.633, val loss: 13.998, test loss: 13.623\n",
      "* Epoch: 57, train loss: 13.312, val loss: 13.711, test loss: 13.336\n",
      "* Epoch: 58, train loss: 13.065, val loss: 13.428, test loss: 13.053\n",
      "* Epoch: 59, train loss: 12.799, val loss: 13.147, test loss: 12.772\n",
      "* Epoch: 60, train loss: 12.489, val loss: 12.869, test loss: 12.493\n",
      "* Epoch: 61, train loss: 12.202, val loss: 12.595, test loss: 12.219\n",
      "* Epoch: 62, train loss: 11.955, val loss: 12.324, test loss: 11.949\n",
      "* Epoch: 63, train loss: 11.713, val loss: 12.057, test loss: 11.682\n",
      "* Epoch: 64, train loss: 11.433, val loss: 11.793, test loss: 11.417\n",
      "* Epoch: 65, train loss: 11.165, val loss: 11.531, test loss: 11.156\n",
      "* Epoch: 66, train loss: 10.916, val loss: 11.274, test loss: 10.899\n",
      "* Epoch: 67, train loss: 10.652, val loss: 11.021, test loss: 10.645\n",
      "* Epoch: 68, train loss: 10.411, val loss: 10.771, test loss: 10.395\n",
      "* Epoch: 69, train loss: 10.164, val loss: 10.522, test loss: 10.147\n",
      "* Epoch: 70, train loss: 9.914, val loss: 10.279, test loss: 9.904\n",
      "* Epoch: 71, train loss: 9.687, val loss: 10.040, test loss: 9.664\n",
      "* Epoch: 72, train loss: 9.466, val loss: 9.813, test loss: 9.437\n",
      "* Epoch: 73, train loss: 9.210, val loss: 9.567, test loss: 9.201\n",
      "* Epoch: 74, train loss: 8.991, val loss: 9.338, test loss: 8.990\n",
      "* Epoch: 75, train loss: 8.815, val loss: 9.108, test loss: 8.768\n",
      "* Epoch: 76, train loss: 8.572, val loss: 8.884, test loss: 8.553\n",
      "* Epoch: 77, train loss: 8.385, val loss: 8.662, test loss: 8.340\n",
      "* Epoch: 78, train loss: 8.164, val loss: 8.446, test loss: 8.131\n",
      "* Epoch: 79, train loss: 7.977, val loss: 8.237, test loss: 7.927\n",
      "* Epoch: 80, train loss: 7.795, val loss: 8.034, test loss: 7.725\n",
      "* Epoch: 81, train loss: 7.604, val loss: 7.836, test loss: 7.526\n",
      "* Epoch: 82, train loss: 7.413, val loss: 7.640, test loss: 7.330\n",
      "* Epoch: 83, train loss: 7.197, val loss: 7.452, test loss: 7.137\n",
      "* Epoch: 84, train loss: 7.020, val loss: 7.265, test loss: 6.937\n",
      "* Epoch: 85, train loss: 6.852, val loss: 7.067, test loss: 6.757\n",
      "* Epoch: 86, train loss: 6.700, val loss: 6.890, test loss: 6.580\n",
      "* Epoch: 87, train loss: 6.557, val loss: 6.710, test loss: 6.399\n",
      "* Epoch: 88, train loss: 6.397, val loss: 6.544, test loss: 6.221\n",
      "* Epoch: 89, train loss: 6.253, val loss: 6.385, test loss: 6.059\n",
      "* Epoch: 90, train loss: 6.114, val loss: 6.226, test loss: 5.902\n",
      "* Epoch: 91, train loss: 5.951, val loss: 6.079, test loss: 5.745\n",
      "* Epoch: 92, train loss: 5.828, val loss: 5.954, test loss: 5.598\n",
      "* Epoch: 93, train loss: 5.696, val loss: 5.830, test loss: 5.450\n",
      "* Epoch: 94, train loss: 5.568, val loss: 5.709, test loss: 5.306\n",
      "* Epoch: 95, train loss: 5.416, val loss: 5.589, test loss: 5.166\n",
      "* Epoch: 96, train loss: 5.330, val loss: 5.472, test loss: 5.033\n",
      "* Epoch: 97, train loss: 5.200, val loss: 5.359, test loss: 4.905\n",
      "* Epoch: 98, train loss: 5.086, val loss: 5.246, test loss: 4.776\n",
      "* Epoch: 99, train loss: 4.974, val loss: 5.136, test loss: 4.652\n",
      "* Epoch: 100, train loss: 4.880, val loss: 5.025, test loss: 4.530\n",
      "* Epoch: 101, train loss: 4.713, val loss: 4.974, test loss: 4.364\n",
      "* Epoch: 102, train loss: 4.670, val loss: 4.830, test loss: 4.295\n",
      "* Epoch: 103, train loss: 4.549, val loss: 4.713, test loss: 4.169\n",
      "* Epoch: 104, train loss: 4.421, val loss: 4.627, test loss: 4.091\n",
      "* Epoch: 105, train loss: 4.424, val loss: 4.567, test loss: 4.031\n",
      "* Epoch: 106, train loss: 4.369, val loss: 4.483, test loss: 3.944\n",
      "* Epoch: 107, train loss: 4.258, val loss: 4.394, test loss: 3.854\n",
      "* Epoch: 108, train loss: 4.180, val loss: 4.295, test loss: 3.759\n",
      "* Epoch: 109, train loss: 4.015, val loss: 4.132, test loss: 3.634\n",
      "* Epoch: 110, train loss: 3.874, val loss: 4.001, test loss: 3.536\n",
      "* Epoch: 111, train loss: 3.835, val loss: 3.869, test loss: 3.426\n",
      "* Epoch: 112, train loss: 3.717, val loss: 3.762, test loss: 3.321\n",
      "* Epoch: 113, train loss: 3.609, val loss: 3.655, test loss: 3.214\n",
      "* Epoch: 114, train loss: 3.538, val loss: 3.582, test loss: 3.150\n",
      "* Epoch: 115, train loss: 3.420, val loss: 3.467, test loss: 3.043\n",
      "* Epoch: 116, train loss: 3.298, val loss: 3.327, test loss: 2.951\n",
      "* Epoch: 117, train loss: 3.226, val loss: 3.252, test loss: 2.872\n",
      "* Epoch: 118, train loss: 3.126, val loss: 3.151, test loss: 2.783\n",
      "* Epoch: 119, train loss: 3.012, val loss: 3.040, test loss: 2.709\n",
      "* Epoch: 120, train loss: 2.948, val loss: 2.938, test loss: 2.621\n",
      "* Epoch: 121, train loss: 2.816, val loss: 2.867, test loss: 2.560\n",
      "* Epoch: 122, train loss: 2.760, val loss: 2.780, test loss: 2.490\n",
      "* Epoch: 123, train loss: 2.626, val loss: 2.686, test loss: 2.419\n",
      "* Epoch: 124, train loss: 2.577, val loss: 2.620, test loss: 2.367\n",
      "* Epoch: 125, train loss: 2.487, val loss: 2.538, test loss: 2.303\n",
      "* Epoch: 126, train loss: 2.397, val loss: 2.494, test loss: 2.287\n",
      "* Epoch: 127, train loss: 2.316, val loss: 2.401, test loss: 2.200\n",
      "* Epoch: 128, train loss: 2.275, val loss: 2.359, test loss: 2.174\n",
      "* Epoch: 129, train loss: 2.207, val loss: 2.282, test loss: 2.095\n",
      "* Epoch: 130, train loss: 2.168, val loss: 2.224, test loss: 2.053\n",
      "* Epoch: 131, train loss: 2.057, val loss: 2.168, test loss: 1.998\n",
      "* Epoch: 132, train loss: 2.043, val loss: 2.117, test loss: 1.954\n",
      "* Epoch: 133, train loss: 1.965, val loss: 2.053, test loss: 1.919\n",
      "* Epoch: 134, train loss: 1.924, val loss: 2.003, test loss: 1.900\n",
      "* Epoch: 135, train loss: 1.867, val loss: 1.954, test loss: 1.845\n",
      "* Epoch: 136, train loss: 1.841, val loss: 1.942, test loss: 1.810\n",
      "* Epoch: 137, train loss: 1.774, val loss: 1.884, test loss: 1.814\n",
      "* Epoch: 138, train loss: 1.758, val loss: 1.837, test loss: 1.763\n",
      "* Epoch: 139, train loss: 1.701, val loss: 1.806, test loss: 1.710\n",
      "* Epoch: 140, train loss: 1.646, val loss: 1.754, test loss: 1.725\n",
      "* Epoch: 141, train loss: 1.637, val loss: 1.745, test loss: 1.718\n",
      "* Epoch: 142, train loss: 1.549, val loss: 1.686, test loss: 1.679\n",
      "* Epoch: 143, train loss: 1.528, val loss: 1.646, test loss: 1.645\n",
      "* Epoch: 144, train loss: 1.556, val loss: 1.646, test loss: 1.657\n",
      "* Epoch: 145, train loss: 1.520, val loss: 1.583, test loss: 1.616\n",
      "  Epoch: 146, train loss: 1.463, val loss: 1.590, test loss: 1.606\n",
      "* Epoch: 147, train loss: 1.427, val loss: 1.528, test loss: 1.595\n",
      "* Epoch: 148, train loss: 1.393, val loss: 1.492, test loss: 1.582\n",
      "* Epoch: 149, train loss: 1.383, val loss: 1.459, test loss: 1.548\n",
      "* Epoch: 150, train loss: 1.343, val loss: 1.437, test loss: 1.567\n",
      "* Epoch: 151, train loss: 1.349, val loss: 1.409, test loss: 1.500\n",
      "* Epoch: 152, train loss: 1.350, val loss: 1.386, test loss: 1.475\n",
      "* Epoch: 153, train loss: 1.293, val loss: 1.362, test loss: 1.478\n",
      "* Epoch: 154, train loss: 1.269, val loss: 1.330, test loss: 1.520\n",
      "  Epoch: 155, train loss: 1.274, val loss: 1.350, test loss: 1.450\n",
      "* Epoch: 156, train loss: 1.277, val loss: 1.319, test loss: 1.439\n",
      "  Epoch: 157, train loss: 1.232, val loss: 1.327, test loss: 1.422\n",
      "* Epoch: 158, train loss: 1.220, val loss: 1.258, test loss: 1.423\n",
      "* Epoch: 159, train loss: 1.186, val loss: 1.254, test loss: 1.440\n",
      "* Epoch: 160, train loss: 1.197, val loss: 1.224, test loss: 1.403\n",
      "  Epoch: 161, train loss: 1.166, val loss: 1.236, test loss: 1.399\n",
      "* Epoch: 162, train loss: 1.189, val loss: 1.185, test loss: 1.419\n",
      "* Epoch: 163, train loss: 1.153, val loss: 1.173, test loss: 1.390\n",
      "* Epoch: 164, train loss: 1.150, val loss: 1.168, test loss: 1.369\n",
      "* Epoch: 165, train loss: 1.098, val loss: 1.137, test loss: 1.362\n",
      "* Epoch: 166, train loss: 1.093, val loss: 1.130, test loss: 1.344\n",
      "* Epoch: 167, train loss: 1.088, val loss: 1.102, test loss: 1.357\n",
      "  Epoch: 168, train loss: 1.101, val loss: 1.161, test loss: 1.299\n",
      "  Epoch: 169, train loss: 1.085, val loss: 1.200, test loss: 1.364\n",
      "* Epoch: 170, train loss: 1.112, val loss: 1.072, test loss: 1.349\n",
      "  Epoch: 171, train loss: 1.077, val loss: 1.101, test loss: 1.360\n",
      "* Epoch: 172, train loss: 1.087, val loss: 1.044, test loss: 1.333\n",
      "  Epoch: 173, train loss: 1.009, val loss: 1.049, test loss: 1.274\n",
      "  Epoch: 174, train loss: 1.021, val loss: 1.050, test loss: 1.254\n",
      "  Epoch: 175, train loss: 1.022, val loss: 1.061, test loss: 1.243\n",
      "* Epoch: 176, train loss: 1.005, val loss: 1.038, test loss: 1.259\n",
      "* Epoch: 177, train loss: 1.000, val loss: 1.029, test loss: 1.259\n",
      "* Epoch: 178, train loss: 0.949, val loss: 1.022, test loss: 1.200\n",
      "* Epoch: 179, train loss: 0.961, val loss: 1.012, test loss: 1.198\n",
      "  Epoch: 180, train loss: 0.957, val loss: 1.030, test loss: 1.193\n",
      "* Epoch: 181, train loss: 0.929, val loss: 0.986, test loss: 1.181\n",
      "* Epoch: 182, train loss: 0.941, val loss: 0.963, test loss: 1.184\n",
      "  Epoch: 183, train loss: 0.961, val loss: 0.965, test loss: 1.188\n",
      "* Epoch: 184, train loss: 0.970, val loss: 0.927, test loss: 1.198\n",
      "  Epoch: 185, train loss: 0.987, val loss: 1.007, test loss: 1.171\n",
      "  Epoch: 186, train loss: 0.965, val loss: 1.035, test loss: 1.215\n",
      "  Epoch: 187, train loss: 0.966, val loss: 0.956, test loss: 1.167\n",
      "* Epoch: 188, train loss: 0.959, val loss: 0.926, test loss: 1.199\n",
      "  Epoch: 189, train loss: 0.919, val loss: 0.950, test loss: 1.141\n",
      "  Epoch: 190, train loss: 0.945, val loss: 1.065, test loss: 1.234\n",
      "* Epoch: 191, train loss: 0.962, val loss: 0.914, test loss: 1.159\n",
      "* Epoch: 192, train loss: 0.911, val loss: 0.903, test loss: 1.138\n",
      "* Epoch: 193, train loss: 0.910, val loss: 0.863, test loss: 1.147\n",
      "  Epoch: 194, train loss: 0.907, val loss: 0.937, test loss: 1.141\n",
      "  Epoch: 195, train loss: 0.881, val loss: 0.936, test loss: 1.151\n",
      "* Epoch: 196, train loss: 0.897, val loss: 0.841, test loss: 1.124\n",
      "  Epoch: 197, train loss: 0.877, val loss: 0.858, test loss: 1.120\n",
      "  Epoch: 198, train loss: 0.872, val loss: 0.894, test loss: 1.134\n",
      "  Epoch: 199, train loss: 0.880, val loss: 0.860, test loss: 1.113\n",
      "  Epoch: 200, train loss: 0.834, val loss: 0.862, test loss: 1.110\n",
      "  Epoch: 201, train loss: 0.863, val loss: 0.895, test loss: 1.130\n",
      "* Epoch: 202, train loss: 0.827, val loss: 0.832, test loss: 1.088\n",
      "  Epoch: 203, train loss: 0.882, val loss: 0.847, test loss: 1.094\n",
      "* Epoch: 204, train loss: 0.837, val loss: 0.801, test loss: 1.079\n",
      "  Epoch: 205, train loss: 0.826, val loss: 0.827, test loss: 1.111\n",
      "  Epoch: 206, train loss: 0.819, val loss: 0.828, test loss: 1.077\n",
      "  Epoch: 207, train loss: 0.783, val loss: 0.837, test loss: 1.094\n",
      "  Epoch: 208, train loss: 0.835, val loss: 0.806, test loss: 1.084\n",
      "* Epoch: 209, train loss: 0.815, val loss: 0.775, test loss: 1.077\n",
      "  Epoch: 210, train loss: 0.869, val loss: 0.797, test loss: 1.084\n",
      "  Epoch: 211, train loss: 0.847, val loss: 0.779, test loss: 1.093\n",
      "  Epoch: 212, train loss: 0.819, val loss: 0.835, test loss: 1.108\n",
      "  Epoch: 213, train loss: 0.823, val loss: 0.854, test loss: 1.130\n",
      "  Epoch: 214, train loss: 0.865, val loss: 0.790, test loss: 1.083\n",
      "* Epoch: 215, train loss: 0.798, val loss: 0.770, test loss: 1.059\n",
      "  Epoch: 216, train loss: 0.808, val loss: 0.787, test loss: 1.068\n",
      "* Epoch: 217, train loss: 0.771, val loss: 0.763, test loss: 1.056\n",
      "  Epoch: 218, train loss: 0.788, val loss: 0.806, test loss: 1.082\n",
      "  Epoch: 219, train loss: 0.777, val loss: 0.778, test loss: 1.061\n",
      "* Epoch: 220, train loss: 0.765, val loss: 0.761, test loss: 1.065\n",
      "* Epoch: 221, train loss: 0.760, val loss: 0.750, test loss: 1.045\n",
      "* Epoch: 222, train loss: 0.804, val loss: 0.750, test loss: 1.044\n",
      "  Epoch: 223, train loss: 0.760, val loss: 0.774, test loss: 1.104\n",
      "  Epoch: 224, train loss: 0.804, val loss: 0.758, test loss: 1.064\n",
      "  Epoch: 225, train loss: 0.764, val loss: 0.805, test loss: 1.057\n",
      "* Epoch: 226, train loss: 0.744, val loss: 0.732, test loss: 1.043\n",
      "  Epoch: 227, train loss: 0.744, val loss: 0.742, test loss: 1.038\n",
      "  Epoch: 228, train loss: 0.769, val loss: 0.813, test loss: 1.095\n",
      "* Epoch: 229, train loss: 0.741, val loss: 0.729, test loss: 1.032\n",
      "* Epoch: 230, train loss: 0.754, val loss: 0.729, test loss: 1.042\n",
      "  Epoch: 231, train loss: 0.768, val loss: 0.746, test loss: 1.064\n",
      "  Epoch: 232, train loss: 0.750, val loss: 0.766, test loss: 1.066\n",
      "  Epoch: 233, train loss: 0.817, val loss: 0.788, test loss: 1.146\n",
      "  Epoch: 234, train loss: 0.747, val loss: 0.739, test loss: 1.033\n",
      "  Epoch: 235, train loss: 0.795, val loss: 0.851, test loss: 1.175\n",
      "* Epoch: 236, train loss: 0.769, val loss: 0.712, test loss: 1.018\n",
      "  Epoch: 237, train loss: 0.767, val loss: 0.716, test loss: 1.012\n",
      "* Epoch: 238, train loss: 0.770, val loss: 0.706, test loss: 1.019\n",
      "  Epoch: 239, train loss: 0.743, val loss: 0.757, test loss: 1.025\n",
      "  Epoch: 240, train loss: 0.804, val loss: 0.708, test loss: 1.025\n",
      "* Epoch: 241, train loss: 0.740, val loss: 0.687, test loss: 1.013\n",
      "  Epoch: 242, train loss: 0.760, val loss: 0.710, test loss: 1.028\n",
      "  Epoch: 243, train loss: 0.744, val loss: 0.708, test loss: 1.025\n",
      "  Epoch: 244, train loss: 0.754, val loss: 0.765, test loss: 1.061\n",
      "  Epoch: 245, train loss: 0.729, val loss: 0.694, test loss: 1.013\n",
      "  Epoch: 246, train loss: 0.721, val loss: 0.741, test loss: 1.044\n",
      "  Epoch: 247, train loss: 0.676, val loss: 0.698, test loss: 1.022\n",
      "  Epoch: 248, train loss: 0.730, val loss: 0.698, test loss: 1.029\n",
      "* Epoch: 249, train loss: 0.785, val loss: 0.679, test loss: 1.018\n",
      "  Epoch: 250, train loss: 0.754, val loss: 0.727, test loss: 1.037\n",
      "  Epoch: 251, train loss: 0.720, val loss: 0.686, test loss: 1.013\n",
      "  Epoch: 252, train loss: 0.683, val loss: 0.732, test loss: 1.055\n",
      "  Epoch: 253, train loss: 0.758, val loss: 0.708, test loss: 1.032\n",
      "  Epoch: 254, train loss: 0.725, val loss: 0.720, test loss: 1.058\n",
      "  Epoch: 255, train loss: 0.664, val loss: 0.775, test loss: 1.082\n",
      "  Epoch: 256, train loss: 0.713, val loss: 0.696, test loss: 1.031\n",
      "  Epoch: 257, train loss: 0.676, val loss: 0.689, test loss: 0.984\n",
      "* Epoch: 258, train loss: 0.719, val loss: 0.673, test loss: 1.018\n",
      "  Epoch: 259, train loss: 0.647, val loss: 0.693, test loss: 0.991\n",
      "  Epoch: 260, train loss: 0.739, val loss: 0.691, test loss: 1.031\n",
      "* Epoch: 261, train loss: 0.672, val loss: 0.664, test loss: 1.001\n",
      "  Epoch: 262, train loss: 0.670, val loss: 0.691, test loss: 1.010\n",
      "  Epoch: 263, train loss: 0.724, val loss: 0.693, test loss: 1.046\n",
      "  Epoch: 264, train loss: 0.677, val loss: 0.694, test loss: 1.000\n",
      "  Epoch: 265, train loss: 0.711, val loss: 0.704, test loss: 1.047\n",
      "  Epoch: 266, train loss: 0.725, val loss: 0.757, test loss: 1.048\n",
      "* Epoch: 267, train loss: 0.659, val loss: 0.659, test loss: 0.992\n",
      "  Epoch: 268, train loss: 0.672, val loss: 0.688, test loss: 1.000\n",
      "  Epoch: 269, train loss: 0.709, val loss: 0.687, test loss: 1.012\n",
      "  Epoch: 270, train loss: 0.675, val loss: 0.677, test loss: 1.021\n",
      "  Epoch: 271, train loss: 0.711, val loss: 0.690, test loss: 1.018\n",
      "  Epoch: 272, train loss: 0.710, val loss: 0.698, test loss: 0.993\n",
      "  Epoch: 273, train loss: 0.658, val loss: 0.685, test loss: 1.040\n",
      "  Epoch: 274, train loss: 0.657, val loss: 0.700, test loss: 1.049\n",
      "* Epoch: 275, train loss: 0.661, val loss: 0.655, test loss: 0.979\n",
      "  Epoch: 276, train loss: 0.705, val loss: 0.739, test loss: 1.119\n",
      "  Epoch: 277, train loss: 0.708, val loss: 0.719, test loss: 1.035\n",
      "  Epoch: 278, train loss: 0.698, val loss: 0.679, test loss: 1.025\n",
      "  Epoch: 279, train loss: 0.701, val loss: 0.678, test loss: 1.020\n",
      "* Epoch: 280, train loss: 0.650, val loss: 0.647, test loss: 0.988\n",
      "  Epoch: 281, train loss: 0.665, val loss: 0.729, test loss: 1.092\n",
      "  Epoch: 282, train loss: 0.605, val loss: 0.695, test loss: 1.005\n",
      "* Epoch: 283, train loss: 0.701, val loss: 0.645, test loss: 0.993\n",
      "  Epoch: 284, train loss: 0.682, val loss: 0.700, test loss: 1.049\n",
      "  Epoch: 285, train loss: 0.667, val loss: 0.691, test loss: 0.993\n",
      "  Epoch: 286, train loss: 0.704, val loss: 0.670, test loss: 1.023\n",
      "  Epoch: 287, train loss: 0.623, val loss: 0.653, test loss: 0.977\n",
      "  Epoch: 288, train loss: 0.653, val loss: 0.754, test loss: 1.089\n",
      "  Epoch: 289, train loss: 0.669, val loss: 0.645, test loss: 0.959\n",
      "  Epoch: 290, train loss: 0.648, val loss: 0.653, test loss: 0.987\n",
      "  Epoch: 291, train loss: 0.645, val loss: 0.662, test loss: 0.974\n",
      "  Epoch: 292, train loss: 0.675, val loss: 0.668, test loss: 0.987\n",
      "  Epoch: 293, train loss: 0.629, val loss: 0.650, test loss: 0.986\n",
      "  Epoch: 294, train loss: 0.672, val loss: 0.708, test loss: 1.033\n",
      "* Epoch: 295, train loss: 0.675, val loss: 0.640, test loss: 0.973\n",
      "* Epoch: 296, train loss: 0.689, val loss: 0.635, test loss: 0.968\n",
      "  Epoch: 297, train loss: 0.683, val loss: 0.653, test loss: 0.986\n",
      "  Epoch: 298, train loss: 0.689, val loss: 0.645, test loss: 0.983\n",
      "  Epoch: 299, train loss: 0.647, val loss: 0.688, test loss: 1.015\n",
      "  Epoch: 300, train loss: 0.680, val loss: 0.666, test loss: 0.984\n",
      "* Epoch: 301, train loss: 0.643, val loss: 0.631, test loss: 0.962\n",
      "* Epoch: 302, train loss: 0.612, val loss: 0.625, test loss: 0.982\n",
      "  Epoch: 303, train loss: 0.662, val loss: 0.661, test loss: 0.979\n",
      "  Epoch: 304, train loss: 0.649, val loss: 0.633, test loss: 0.966\n",
      "  Epoch: 305, train loss: 0.631, val loss: 0.627, test loss: 0.979\n",
      "  Epoch: 306, train loss: 0.655, val loss: 0.663, test loss: 1.013\n",
      "  Epoch: 307, train loss: 0.648, val loss: 0.651, test loss: 0.963\n",
      "  Epoch: 308, train loss: 0.670, val loss: 0.750, test loss: 1.032\n",
      "  Epoch: 309, train loss: 0.641, val loss: 0.649, test loss: 0.991\n",
      "  Epoch: 310, train loss: 0.654, val loss: 0.640, test loss: 0.959\n",
      "  Epoch: 311, train loss: 0.657, val loss: 0.645, test loss: 0.972\n",
      "  Epoch: 312, train loss: 0.647, val loss: 0.644, test loss: 0.967\n",
      "  Epoch: 313, train loss: 0.713, val loss: 0.675, test loss: 1.036\n",
      "  Epoch: 314, train loss: 0.664, val loss: 0.688, test loss: 1.047\n",
      "* Epoch: 315, train loss: 0.639, val loss: 0.620, test loss: 0.957\n",
      "  Epoch: 316, train loss: 0.611, val loss: 0.671, test loss: 0.998\n",
      "  Epoch: 317, train loss: 0.700, val loss: 0.653, test loss: 1.026\n",
      "  Epoch: 318, train loss: 0.628, val loss: 0.635, test loss: 0.940\n",
      "  Epoch: 319, train loss: 0.634, val loss: 0.629, test loss: 0.973\n",
      "  Epoch: 320, train loss: 0.587, val loss: 0.660, test loss: 0.964\n",
      "  Epoch: 321, train loss: 0.649, val loss: 0.674, test loss: 0.985\n",
      "  Epoch: 322, train loss: 0.655, val loss: 0.658, test loss: 0.962\n",
      "  Epoch: 323, train loss: 0.629, val loss: 0.667, test loss: 1.016\n",
      "* Epoch: 324, train loss: 0.579, val loss: 0.610, test loss: 0.956\n",
      "  Epoch: 325, train loss: 0.612, val loss: 0.678, test loss: 1.022\n",
      "  Epoch: 326, train loss: 0.627, val loss: 0.620, test loss: 0.966\n",
      "  Epoch: 327, train loss: 0.651, val loss: 0.640, test loss: 0.976\n",
      "  Epoch: 328, train loss: 0.603, val loss: 0.626, test loss: 0.955\n",
      "  Epoch: 329, train loss: 0.650, val loss: 0.629, test loss: 0.957\n",
      "  Epoch: 330, train loss: 0.688, val loss: 0.703, test loss: 1.078\n",
      "  Epoch: 331, train loss: 0.607, val loss: 0.636, test loss: 0.965\n",
      "  Epoch: 332, train loss: 0.673, val loss: 0.631, test loss: 0.966\n",
      "  Epoch: 333, train loss: 0.603, val loss: 0.672, test loss: 1.001\n",
      "  Epoch: 334, train loss: 0.620, val loss: 0.646, test loss: 0.991\n",
      "  Epoch: 335, train loss: 0.647, val loss: 0.614, test loss: 0.968\n",
      "  Epoch: 336, train loss: 0.585, val loss: 0.616, test loss: 0.948\n",
      "  Epoch: 337, train loss: 0.623, val loss: 0.654, test loss: 0.981\n",
      "  Epoch: 338, train loss: 0.687, val loss: 0.706, test loss: 1.019\n",
      "  Epoch: 339, train loss: 0.612, val loss: 0.630, test loss: 0.963\n",
      "  Epoch: 340, train loss: 0.605, val loss: 0.613, test loss: 0.954\n",
      "  Epoch: 341, train loss: 0.649, val loss: 0.695, test loss: 1.075\n",
      "  Epoch: 342, train loss: 0.630, val loss: 0.633, test loss: 1.019\n",
      "* Epoch: 343, train loss: 0.611, val loss: 0.601, test loss: 0.969\n",
      "  Epoch: 344, train loss: 0.610, val loss: 0.638, test loss: 0.981\n",
      "  Epoch: 345, train loss: 0.627, val loss: 0.613, test loss: 0.944\n",
      "  Epoch: 346, train loss: 0.599, val loss: 0.674, test loss: 1.036\n",
      "  Epoch: 347, train loss: 0.621, val loss: 0.680, test loss: 1.005\n",
      "  Epoch: 348, train loss: 0.705, val loss: 0.602, test loss: 0.970\n",
      "  Epoch: 349, train loss: 0.593, val loss: 0.613, test loss: 0.963\n",
      "  Epoch: 350, train loss: 0.658, val loss: 0.638, test loss: 1.036\n",
      "  Epoch: 351, train loss: 0.620, val loss: 0.618, test loss: 0.996\n",
      "  Epoch: 352, train loss: 0.605, val loss: 0.628, test loss: 0.979\n",
      "  Epoch: 353, train loss: 0.610, val loss: 0.606, test loss: 0.974\n",
      "  Epoch: 354, train loss: 0.589, val loss: 0.630, test loss: 0.976\n",
      "  Epoch: 355, train loss: 0.611, val loss: 0.608, test loss: 0.970\n",
      "  Epoch: 356, train loss: 0.692, val loss: 0.627, test loss: 0.971\n",
      "  Epoch: 357, train loss: 0.635, val loss: 0.607, test loss: 0.996\n",
      "  Epoch: 358, train loss: 0.628, val loss: 0.643, test loss: 0.966\n",
      "  Epoch: 359, train loss: 0.594, val loss: 0.626, test loss: 0.975\n",
      "  Epoch: 360, train loss: 0.604, val loss: 0.608, test loss: 0.957\n",
      "  Epoch: 361, train loss: 0.639, val loss: 0.620, test loss: 0.988\n",
      "  Epoch: 362, train loss: 0.585, val loss: 0.612, test loss: 0.948\n",
      "  Epoch: 363, train loss: 0.611, val loss: 0.617, test loss: 0.966\n",
      "  Epoch: 364, train loss: 0.617, val loss: 0.617, test loss: 0.995\n",
      "* Epoch: 365, train loss: 0.604, val loss: 0.598, test loss: 0.941\n",
      "  Epoch: 366, train loss: 0.572, val loss: 0.613, test loss: 0.971\n",
      "  Epoch: 367, train loss: 0.600, val loss: 0.613, test loss: 0.984\n",
      "  Epoch: 368, train loss: 0.632, val loss: 0.677, test loss: 1.064\n",
      "  Epoch: 369, train loss: 0.614, val loss: 0.634, test loss: 0.949\n",
      "  Epoch: 370, train loss: 0.597, val loss: 0.640, test loss: 0.970\n",
      "  Epoch: 371, train loss: 0.586, val loss: 0.694, test loss: 0.992\n",
      "  Epoch: 372, train loss: 0.582, val loss: 0.628, test loss: 0.990\n",
      "  Epoch: 373, train loss: 0.628, val loss: 0.693, test loss: 1.044\n",
      "  Epoch: 374, train loss: 0.620, val loss: 0.618, test loss: 0.941\n",
      "  Epoch: 375, train loss: 0.619, val loss: 0.608, test loss: 0.968\n",
      "  Epoch: 376, train loss: 0.616, val loss: 0.715, test loss: 1.016\n",
      "  Epoch: 377, train loss: 0.606, val loss: 0.601, test loss: 0.974\n",
      "  Epoch: 378, train loss: 0.573, val loss: 0.639, test loss: 1.006\n",
      "  Epoch: 379, train loss: 0.591, val loss: 0.648, test loss: 0.955\n",
      "  Epoch: 380, train loss: 0.611, val loss: 0.602, test loss: 0.987\n",
      "* Epoch: 381, train loss: 0.626, val loss: 0.598, test loss: 0.952\n",
      "  Epoch: 382, train loss: 0.574, val loss: 0.640, test loss: 0.966\n",
      "  Epoch: 383, train loss: 0.611, val loss: 0.672, test loss: 1.032\n",
      "  Epoch: 384, train loss: 0.574, val loss: 0.616, test loss: 0.975\n",
      "  Epoch: 385, train loss: 0.634, val loss: 0.697, test loss: 1.000\n",
      "* Epoch: 386, train loss: 0.562, val loss: 0.598, test loss: 0.971\n",
      "  Epoch: 387, train loss: 0.609, val loss: 0.668, test loss: 0.966\n",
      "* Epoch: 388, train loss: 0.596, val loss: 0.596, test loss: 0.948\n",
      "  Epoch: 389, train loss: 0.579, val loss: 0.611, test loss: 1.005\n",
      "  Epoch: 390, train loss: 0.623, val loss: 0.619, test loss: 0.991\n",
      "  Epoch: 391, train loss: 0.595, val loss: 0.666, test loss: 1.022\n",
      "  Epoch: 392, train loss: 0.582, val loss: 0.604, test loss: 0.959\n",
      "* Epoch: 393, train loss: 0.560, val loss: 0.587, test loss: 0.943\n",
      "  Epoch: 394, train loss: 0.583, val loss: 0.608, test loss: 0.970\n",
      "  Epoch: 395, train loss: 0.549, val loss: 0.590, test loss: 0.942\n",
      "  Epoch: 396, train loss: 0.579, val loss: 0.590, test loss: 0.942\n",
      "  Epoch: 397, train loss: 0.643, val loss: 0.627, test loss: 1.005\n",
      "  Epoch: 398, train loss: 0.621, val loss: 0.605, test loss: 0.971\n",
      "  Epoch: 399, train loss: 0.618, val loss: 0.650, test loss: 1.019\n",
      "  Epoch: 400, train loss: 0.563, val loss: 0.616, test loss: 0.974\n",
      "  Epoch: 401, train loss: 0.567, val loss: 0.618, test loss: 0.967\n",
      "  Epoch: 402, train loss: 0.599, val loss: 0.609, test loss: 0.986\n",
      "  Epoch: 403, train loss: 0.630, val loss: 0.712, test loss: 1.104\n",
      "* Epoch: 404, train loss: 0.607, val loss: 0.578, test loss: 0.934\n",
      "  Epoch: 405, train loss: 0.591, val loss: 0.619, test loss: 0.970\n",
      "  Epoch: 406, train loss: 0.633, val loss: 0.623, test loss: 0.960\n",
      "  Epoch: 407, train loss: 0.601, val loss: 0.619, test loss: 1.003\n",
      "  Epoch: 408, train loss: 0.552, val loss: 0.597, test loss: 0.962\n",
      "  Epoch: 409, train loss: 0.611, val loss: 0.591, test loss: 0.938\n",
      "  Epoch: 410, train loss: 0.615, val loss: 0.660, test loss: 1.000\n",
      "  Epoch: 411, train loss: 0.624, val loss: 0.662, test loss: 1.007\n",
      "  Epoch: 412, train loss: 0.597, val loss: 0.610, test loss: 0.988\n",
      "  Epoch: 413, train loss: 0.596, val loss: 0.598, test loss: 0.948\n",
      "  Epoch: 414, train loss: 0.602, val loss: 0.610, test loss: 0.974\n",
      "  Epoch: 415, train loss: 0.606, val loss: 0.709, test loss: 1.010\n",
      "  Epoch: 416, train loss: 0.589, val loss: 0.635, test loss: 0.968\n",
      "  Epoch: 417, train loss: 0.590, val loss: 0.632, test loss: 0.944\n",
      "  Epoch: 418, train loss: 0.606, val loss: 0.630, test loss: 1.003\n",
      "  Epoch: 419, train loss: 0.602, val loss: 0.584, test loss: 0.956\n",
      "  Epoch: 420, train loss: 0.602, val loss: 0.653, test loss: 0.977\n",
      "  Epoch: 421, train loss: 0.628, val loss: 0.684, test loss: 0.974\n",
      "  Epoch: 422, train loss: 0.604, val loss: 0.610, test loss: 0.999\n",
      "  Epoch: 423, train loss: 0.602, val loss: 0.609, test loss: 0.951\n",
      "  Epoch: 424, train loss: 0.569, val loss: 0.589, test loss: 0.964\n",
      "  Epoch: 425, train loss: 0.563, val loss: 0.591, test loss: 0.946\n",
      "  Epoch: 426, train loss: 0.546, val loss: 0.587, test loss: 0.949\n",
      "  Epoch: 427, train loss: 0.579, val loss: 0.615, test loss: 0.997\n",
      "  Epoch: 428, train loss: 0.606, val loss: 0.696, test loss: 0.998\n",
      "  Epoch: 429, train loss: 0.572, val loss: 0.605, test loss: 0.984\n",
      "  Epoch: 430, train loss: 0.553, val loss: 0.689, test loss: 0.998\n",
      "  Epoch: 431, train loss: 0.561, val loss: 0.594, test loss: 0.965\n",
      "  Epoch: 432, train loss: 0.585, val loss: 0.634, test loss: 0.970\n",
      "  Epoch: 433, train loss: 0.563, val loss: 0.598, test loss: 0.972\n",
      "  Epoch: 434, train loss: 0.600, val loss: 0.618, test loss: 0.979\n",
      "  Epoch: 435, train loss: 0.550, val loss: 0.595, test loss: 0.946\n",
      "  Epoch: 436, train loss: 0.578, val loss: 0.613, test loss: 0.941\n",
      "  Epoch: 437, train loss: 0.569, val loss: 0.581, test loss: 0.952\n",
      "  Epoch: 438, train loss: 0.586, val loss: 0.636, test loss: 0.984\n",
      "  Epoch: 439, train loss: 0.599, val loss: 0.595, test loss: 0.939\n",
      "  Epoch: 440, train loss: 0.571, val loss: 0.599, test loss: 0.934\n",
      "  Epoch: 441, train loss: 0.576, val loss: 0.589, test loss: 0.965\n",
      "  Epoch: 442, train loss: 0.579, val loss: 0.784, test loss: 1.073\n",
      "  Epoch: 443, train loss: 0.580, val loss: 0.602, test loss: 0.964\n",
      "  Epoch: 444, train loss: 0.568, val loss: 0.595, test loss: 0.977\n",
      "  Epoch: 445, train loss: 0.550, val loss: 0.590, test loss: 0.951\n",
      "  Epoch: 446, train loss: 0.599, val loss: 0.688, test loss: 1.001\n",
      "* Epoch: 447, train loss: 0.563, val loss: 0.578, test loss: 0.949\n",
      "  Epoch: 448, train loss: 0.561, val loss: 0.628, test loss: 0.973\n",
      "  Epoch: 449, train loss: 0.528, val loss: 0.627, test loss: 0.994\n",
      "  Epoch: 450, train loss: 0.602, val loss: 0.585, test loss: 0.945\n",
      "  Epoch: 451, train loss: 0.579, val loss: 0.631, test loss: 1.003\n",
      "  Epoch: 452, train loss: 0.565, val loss: 0.592, test loss: 0.970\n",
      "  Epoch: 453, train loss: 0.598, val loss: 0.581, test loss: 0.945\n",
      "  Epoch: 454, train loss: 0.576, val loss: 0.694, test loss: 0.993\n",
      "  Epoch: 455, train loss: 0.550, val loss: 0.587, test loss: 0.965\n",
      "  Epoch: 456, train loss: 0.633, val loss: 0.627, test loss: 1.008\n",
      "  Epoch: 457, train loss: 0.572, val loss: 0.592, test loss: 0.970\n",
      "  Epoch: 458, train loss: 0.597, val loss: 0.602, test loss: 0.936\n",
      "  Epoch: 459, train loss: 0.597, val loss: 0.632, test loss: 0.983\n",
      "  Epoch: 460, train loss: 0.582, val loss: 0.637, test loss: 1.029\n",
      "* Epoch: 461, train loss: 0.535, val loss: 0.572, test loss: 0.946\n",
      "  Epoch: 462, train loss: 0.529, val loss: 0.578, test loss: 0.945\n",
      "  Epoch: 463, train loss: 0.572, val loss: 0.628, test loss: 0.971\n",
      "  Epoch: 464, train loss: 0.576, val loss: 0.812, test loss: 1.095\n",
      "  Epoch: 465, train loss: 0.577, val loss: 0.611, test loss: 0.982\n",
      "  Epoch: 466, train loss: 0.563, val loss: 0.656, test loss: 1.031\n",
      "  Epoch: 467, train loss: 0.619, val loss: 0.669, test loss: 0.989\n",
      "  Epoch: 468, train loss: 0.559, val loss: 0.674, test loss: 1.037\n",
      "  Epoch: 469, train loss: 0.609, val loss: 0.588, test loss: 0.978\n",
      "  Epoch: 470, train loss: 0.545, val loss: 0.589, test loss: 0.941\n",
      "  Epoch: 471, train loss: 0.541, val loss: 0.584, test loss: 0.963\n",
      "  Epoch: 472, train loss: 0.564, val loss: 0.629, test loss: 1.012\n",
      "  Epoch: 473, train loss: 0.595, val loss: 0.610, test loss: 0.968\n",
      "  Epoch: 474, train loss: 0.559, val loss: 0.703, test loss: 1.032\n",
      "  Epoch: 475, train loss: 0.569, val loss: 0.647, test loss: 0.971\n",
      "  Epoch: 476, train loss: 0.524, val loss: 0.585, test loss: 0.960\n",
      "  Epoch: 477, train loss: 0.598, val loss: 0.649, test loss: 0.981\n",
      "  Epoch: 478, train loss: 0.560, val loss: 0.615, test loss: 0.986\n",
      "  Epoch: 479, train loss: 0.555, val loss: 0.583, test loss: 0.947\n",
      "  Epoch: 480, train loss: 0.565, val loss: 0.628, test loss: 1.013\n",
      "  Epoch: 481, train loss: 0.573, val loss: 0.654, test loss: 0.995\n",
      "  Epoch: 482, train loss: 0.549, val loss: 0.638, test loss: 1.014\n",
      "  Epoch: 483, train loss: 0.548, val loss: 0.612, test loss: 1.002\n",
      "  Epoch: 484, train loss: 0.543, val loss: 0.598, test loss: 0.958\n",
      "  Epoch: 485, train loss: 0.560, val loss: 0.673, test loss: 1.013\n",
      "  Epoch: 486, train loss: 0.607, val loss: 0.633, test loss: 0.994\n",
      "  Epoch: 487, train loss: 0.542, val loss: 0.598, test loss: 0.971\n",
      "  Epoch: 488, train loss: 0.548, val loss: 0.600, test loss: 0.984\n",
      "  Epoch: 489, train loss: 0.590, val loss: 0.652, test loss: 1.018\n",
      "  Epoch: 490, train loss: 0.601, val loss: 0.648, test loss: 0.977\n",
      "  Epoch: 491, train loss: 0.566, val loss: 0.589, test loss: 0.985\n",
      "* Epoch: 492, train loss: 0.580, val loss: 0.570, test loss: 0.936\n",
      "  Epoch: 493, train loss: 0.557, val loss: 0.668, test loss: 0.990\n",
      "  Epoch: 494, train loss: 0.568, val loss: 0.719, test loss: 1.121\n",
      "  Epoch: 495, train loss: 0.568, val loss: 0.710, test loss: 1.030\n",
      "  Epoch: 496, train loss: 0.556, val loss: 0.604, test loss: 0.951\n",
      "  Epoch: 497, train loss: 0.571, val loss: 0.578, test loss: 0.960\n",
      "  Epoch: 498, train loss: 0.561, val loss: 0.730, test loss: 1.043\n",
      "  Epoch: 499, train loss: 0.580, val loss: 0.595, test loss: 0.964\n",
      "  Epoch: 500, train loss: 0.540, val loss: 0.582, test loss: 0.957\n",
      "  Epoch: 501, train loss: 0.534, val loss: 0.595, test loss: 0.958\n",
      "* Epoch: 502, train loss: 0.558, val loss: 0.569, test loss: 0.955\n",
      "  Epoch: 503, train loss: 0.556, val loss: 0.648, test loss: 0.997\n",
      "  Epoch: 504, train loss: 0.583, val loss: 0.580, test loss: 0.960\n",
      "  Epoch: 505, train loss: 0.536, val loss: 0.615, test loss: 0.994\n",
      "  Epoch: 506, train loss: 0.601, val loss: 0.674, test loss: 1.063\n",
      "  Epoch: 507, train loss: 0.570, val loss: 0.583, test loss: 0.963\n",
      "  Epoch: 508, train loss: 0.537, val loss: 0.708, test loss: 1.044\n",
      "  Epoch: 509, train loss: 0.558, val loss: 0.574, test loss: 0.946\n",
      "  Epoch: 510, train loss: 0.532, val loss: 0.612, test loss: 1.003\n",
      "  Epoch: 511, train loss: 0.551, val loss: 0.608, test loss: 0.994\n",
      "  Epoch: 512, train loss: 0.556, val loss: 0.591, test loss: 0.986\n",
      "  Epoch: 513, train loss: 0.549, val loss: 0.576, test loss: 0.987\n",
      "  Epoch: 514, train loss: 0.610, val loss: 0.646, test loss: 1.010\n",
      "  Epoch: 515, train loss: 0.547, val loss: 0.650, test loss: 1.001\n",
      "  Epoch: 516, train loss: 0.540, val loss: 0.661, test loss: 1.033\n",
      "  Epoch: 517, train loss: 0.580, val loss: 0.662, test loss: 1.052\n",
      "  Epoch: 518, train loss: 0.532, val loss: 0.589, test loss: 0.978\n",
      "  Epoch: 519, train loss: 0.561, val loss: 0.623, test loss: 0.965\n",
      "  Epoch: 520, train loss: 0.582, val loss: 0.701, test loss: 1.040\n",
      "  Epoch: 521, train loss: 0.560, val loss: 0.673, test loss: 1.023\n",
      "  Epoch: 522, train loss: 0.564, val loss: 0.632, test loss: 0.991\n",
      "  Epoch: 523, train loss: 0.548, val loss: 0.574, test loss: 0.948\n",
      "  Epoch: 524, train loss: 0.532, val loss: 0.588, test loss: 0.981\n",
      "  Epoch: 525, train loss: 0.536, val loss: 0.692, test loss: 1.011\n",
      "  Epoch: 526, train loss: 0.561, val loss: 0.648, test loss: 1.048\n",
      "  Epoch: 527, train loss: 0.595, val loss: 0.598, test loss: 0.980\n",
      "  Epoch: 528, train loss: 0.521, val loss: 0.612, test loss: 0.960\n",
      "  Epoch: 529, train loss: 0.547, val loss: 0.580, test loss: 0.963\n",
      "  Epoch: 530, train loss: 0.564, val loss: 0.684, test loss: 1.003\n",
      "  Epoch: 531, train loss: 0.572, val loss: 0.784, test loss: 1.175\n",
      "  Epoch: 532, train loss: 0.615, val loss: 0.603, test loss: 0.962\n",
      "  Epoch: 533, train loss: 0.559, val loss: 0.647, test loss: 1.028\n",
      "  Epoch: 534, train loss: 0.537, val loss: 0.603, test loss: 0.991\n",
      "  Epoch: 535, train loss: 0.566, val loss: 0.652, test loss: 0.963\n",
      "  Epoch: 536, train loss: 0.570, val loss: 0.673, test loss: 1.031\n",
      "  Epoch: 537, train loss: 0.537, val loss: 0.602, test loss: 0.949\n",
      "  Epoch: 538, train loss: 0.536, val loss: 0.574, test loss: 0.959\n",
      "  Epoch: 539, train loss: 0.531, val loss: 0.610, test loss: 0.960\n",
      "  Epoch: 540, train loss: 0.551, val loss: 0.667, test loss: 1.000\n",
      "  Epoch: 541, train loss: 0.504, val loss: 0.581, test loss: 0.941\n",
      "  Epoch: 542, train loss: 0.534, val loss: 0.588, test loss: 0.950\n",
      "  Epoch: 543, train loss: 0.566, val loss: 0.951, test loss: 1.206\n",
      "  Epoch: 544, train loss: 0.573, val loss: 0.583, test loss: 0.987\n",
      "  Epoch: 545, train loss: 0.591, val loss: 0.580, test loss: 0.963\n",
      "  Epoch: 546, train loss: 0.564, val loss: 0.633, test loss: 0.978\n",
      "* Epoch: 547, train loss: 0.538, val loss: 0.569, test loss: 0.953\n",
      "  Epoch: 548, train loss: 0.565, val loss: 0.700, test loss: 1.039\n",
      "  Epoch: 549, train loss: 0.577, val loss: 0.583, test loss: 0.964\n",
      "  Epoch: 550, train loss: 0.540, val loss: 0.623, test loss: 0.999\n",
      "  Epoch: 551, train loss: 0.541, val loss: 0.608, test loss: 0.953\n",
      "  Epoch: 552, train loss: 0.572, val loss: 0.603, test loss: 0.984\n",
      "  Epoch: 553, train loss: 0.545, val loss: 0.575, test loss: 0.946\n",
      "  Epoch: 554, train loss: 0.550, val loss: 0.769, test loss: 1.090\n",
      "  Epoch: 555, train loss: 0.561, val loss: 0.666, test loss: 1.065\n",
      "  Epoch: 556, train loss: 0.530, val loss: 0.582, test loss: 0.950\n",
      "  Epoch: 557, train loss: 0.580, val loss: 0.697, test loss: 1.071\n",
      "  Epoch: 558, train loss: 0.624, val loss: 0.656, test loss: 1.016\n",
      "  Epoch: 559, train loss: 0.523, val loss: 0.591, test loss: 0.956\n",
      "  Epoch: 560, train loss: 0.554, val loss: 0.682, test loss: 1.039\n",
      "  Epoch: 561, train loss: 0.555, val loss: 0.584, test loss: 0.953\n",
      "  Epoch: 562, train loss: 0.538, val loss: 0.576, test loss: 0.936\n",
      "  Epoch: 563, train loss: 0.589, val loss: 0.675, test loss: 1.008\n",
      "  Epoch: 564, train loss: 0.561, val loss: 0.590, test loss: 0.959\n",
      "  Epoch: 565, train loss: 0.553, val loss: 0.629, test loss: 0.963\n",
      "  Epoch: 566, train loss: 0.527, val loss: 0.717, test loss: 1.124\n",
      "  Epoch: 567, train loss: 0.530, val loss: 0.739, test loss: 1.109\n",
      "  Epoch: 568, train loss: 0.553, val loss: 0.570, test loss: 0.956\n",
      "  Epoch: 569, train loss: 0.540, val loss: 0.811, test loss: 1.170\n",
      "  Epoch: 570, train loss: 0.575, val loss: 0.627, test loss: 1.027\n",
      "  Epoch: 571, train loss: 0.576, val loss: 0.644, test loss: 0.979\n",
      "  Epoch: 572, train loss: 0.520, val loss: 0.599, test loss: 0.961\n",
      "  Epoch: 573, train loss: 0.564, val loss: 0.658, test loss: 1.026\n",
      "  Epoch: 574, train loss: 0.592, val loss: 0.618, test loss: 0.951\n",
      "  Epoch: 575, train loss: 0.574, val loss: 0.683, test loss: 1.018\n",
      "  Epoch: 576, train loss: 0.510, val loss: 0.573, test loss: 0.947\n",
      "  Epoch: 577, train loss: 0.537, val loss: 0.592, test loss: 0.955\n",
      "  Epoch: 578, train loss: 0.561, val loss: 0.597, test loss: 0.970\n",
      "  Epoch: 579, train loss: 0.565, val loss: 0.607, test loss: 0.972\n",
      "  Epoch: 580, train loss: 0.557, val loss: 0.670, test loss: 0.940\n",
      "  Epoch: 581, train loss: 0.564, val loss: 0.640, test loss: 0.994\n",
      "  Epoch: 582, train loss: 0.526, val loss: 0.597, test loss: 0.956\n",
      "  Epoch: 583, train loss: 0.523, val loss: 0.593, test loss: 0.977\n",
      "  Epoch: 584, train loss: 0.525, val loss: 0.587, test loss: 0.972\n",
      "  Epoch: 585, train loss: 0.530, val loss: 0.635, test loss: 1.012\n",
      "  Epoch: 586, train loss: 0.547, val loss: 0.668, test loss: 1.040\n",
      "  Epoch: 587, train loss: 0.542, val loss: 0.751, test loss: 1.072\n",
      "  Epoch: 588, train loss: 0.573, val loss: 0.590, test loss: 0.966\n",
      "  Epoch: 589, train loss: 0.523, val loss: 0.581, test loss: 0.960\n",
      "* Epoch: 590, train loss: 0.571, val loss: 0.563, test loss: 0.951\n",
      "  Epoch: 591, train loss: 0.541, val loss: 0.879, test loss: 1.162\n",
      "  Epoch: 592, train loss: 0.533, val loss: 0.600, test loss: 0.992\n",
      "  Epoch: 593, train loss: 0.583, val loss: 0.604, test loss: 0.978\n",
      "  Epoch: 594, train loss: 0.533, val loss: 0.598, test loss: 0.970\n",
      "  Epoch: 595, train loss: 0.531, val loss: 0.575, test loss: 0.950\n",
      "  Epoch: 596, train loss: 0.542, val loss: 0.799, test loss: 1.166\n",
      "  Epoch: 597, train loss: 0.598, val loss: 0.650, test loss: 0.993\n",
      "  Epoch: 598, train loss: 0.634, val loss: 0.611, test loss: 1.013\n",
      "  Epoch: 599, train loss: 0.556, val loss: 0.699, test loss: 1.081\n",
      "  Epoch: 600, train loss: 0.528, val loss: 0.644, test loss: 1.046\n",
      "  Epoch: 601, train loss: 0.560, val loss: 0.803, test loss: 1.102\n",
      "  Epoch: 602, train loss: 0.553, val loss: 0.614, test loss: 0.994\n",
      "  Epoch: 603, train loss: 0.539, val loss: 0.568, test loss: 0.951\n",
      "  Epoch: 604, train loss: 0.557, val loss: 0.577, test loss: 0.956\n",
      "  Epoch: 605, train loss: 0.530, val loss: 0.608, test loss: 0.950\n",
      "  Epoch: 606, train loss: 0.549, val loss: 0.667, test loss: 1.052\n",
      "  Epoch: 607, train loss: 0.549, val loss: 0.727, test loss: 1.084\n",
      "  Epoch: 608, train loss: 0.563, val loss: 0.597, test loss: 0.969\n",
      "  Epoch: 609, train loss: 0.548, val loss: 0.586, test loss: 0.975\n",
      "  Epoch: 610, train loss: 0.520, val loss: 0.598, test loss: 0.949\n",
      "  Epoch: 611, train loss: 0.512, val loss: 0.682, test loss: 1.077\n",
      "  Epoch: 612, train loss: 0.542, val loss: 0.609, test loss: 0.971\n",
      "  Epoch: 613, train loss: 0.491, val loss: 0.614, test loss: 1.009\n",
      "  Epoch: 614, train loss: 0.582, val loss: 0.577, test loss: 0.947\n",
      "  Epoch: 615, train loss: 0.567, val loss: 0.653, test loss: 1.043\n",
      "  Epoch: 616, train loss: 0.572, val loss: 0.575, test loss: 0.961\n",
      "  Epoch: 617, train loss: 0.547, val loss: 0.634, test loss: 1.012\n",
      "  Epoch: 618, train loss: 0.558, val loss: 0.615, test loss: 0.999\n",
      "  Epoch: 619, train loss: 0.555, val loss: 0.672, test loss: 1.037\n",
      "  Epoch: 620, train loss: 0.569, val loss: 0.677, test loss: 1.023\n",
      "  Epoch: 621, train loss: 0.545, val loss: 0.621, test loss: 1.010\n",
      "  Epoch: 622, train loss: 0.509, val loss: 0.578, test loss: 0.940\n",
      "  Epoch: 623, train loss: 0.537, val loss: 0.632, test loss: 0.956\n",
      "  Epoch: 624, train loss: 0.524, val loss: 0.632, test loss: 1.017\n",
      "  Epoch: 625, train loss: 0.534, val loss: 0.729, test loss: 1.028\n",
      "  Epoch: 626, train loss: 0.553, val loss: 0.612, test loss: 0.984\n",
      "  Epoch: 627, train loss: 0.600, val loss: 0.616, test loss: 0.977\n",
      "  Epoch: 628, train loss: 0.528, val loss: 0.579, test loss: 0.966\n",
      "  Epoch: 629, train loss: 0.560, val loss: 0.742, test loss: 1.084\n",
      "  Epoch: 630, train loss: 0.506, val loss: 0.604, test loss: 0.987\n",
      "  Epoch: 631, train loss: 0.538, val loss: 0.570, test loss: 0.951\n",
      "  Epoch: 632, train loss: 0.503, val loss: 0.567, test loss: 0.949\n",
      "  Epoch: 633, train loss: 0.535, val loss: 0.627, test loss: 1.025\n",
      "  Epoch: 634, train loss: 0.568, val loss: 0.631, test loss: 0.999\n",
      "* Epoch: 635, train loss: 0.547, val loss: 0.552, test loss: 0.948\n",
      "  Epoch: 636, train loss: 0.510, val loss: 0.571, test loss: 0.949\n",
      "  Epoch: 637, train loss: 0.564, val loss: 0.692, test loss: 1.034\n",
      "  Epoch: 638, train loss: 0.517, val loss: 0.596, test loss: 0.969\n",
      "  Epoch: 639, train loss: 0.543, val loss: 0.630, test loss: 0.975\n",
      "  Epoch: 640, train loss: 0.557, val loss: 0.594, test loss: 0.970\n",
      "  Epoch: 641, train loss: 0.554, val loss: 0.725, test loss: 1.060\n",
      "  Epoch: 642, train loss: 0.531, val loss: 0.636, test loss: 1.018\n",
      "  Epoch: 643, train loss: 0.545, val loss: 0.748, test loss: 1.071\n",
      "  Epoch: 644, train loss: 0.557, val loss: 0.616, test loss: 0.981\n",
      "  Epoch: 645, train loss: 0.512, val loss: 0.593, test loss: 0.970\n",
      "  Epoch: 646, train loss: 0.543, val loss: 0.586, test loss: 0.948\n",
      "  Epoch: 647, train loss: 0.516, val loss: 0.687, test loss: 1.078\n",
      "  Epoch: 648, train loss: 0.533, val loss: 0.602, test loss: 0.972\n",
      "  Epoch: 649, train loss: 0.534, val loss: 0.631, test loss: 0.977\n",
      "  Epoch: 650, train loss: 0.560, val loss: 0.632, test loss: 0.998\n",
      "  Epoch: 651, train loss: 0.553, val loss: 0.652, test loss: 1.027\n",
      "  Epoch: 652, train loss: 0.517, val loss: 0.578, test loss: 0.952\n",
      "  Epoch: 653, train loss: 0.569, val loss: 0.594, test loss: 0.960\n",
      "  Epoch: 654, train loss: 0.520, val loss: 0.569, test loss: 0.939\n",
      "  Epoch: 655, train loss: 0.508, val loss: 0.616, test loss: 0.981\n",
      "  Epoch: 656, train loss: 0.490, val loss: 0.565, test loss: 0.970\n",
      "  Epoch: 657, train loss: 0.494, val loss: 0.732, test loss: 1.100\n",
      "  Epoch: 658, train loss: 0.549, val loss: 0.594, test loss: 0.953\n",
      "  Epoch: 659, train loss: 0.532, val loss: 0.637, test loss: 1.016\n",
      "  Epoch: 660, train loss: 0.528, val loss: 0.662, test loss: 1.053\n",
      "  Epoch: 661, train loss: 0.523, val loss: 0.593, test loss: 0.990\n",
      "  Epoch: 662, train loss: 0.553, val loss: 0.580, test loss: 0.976\n",
      "  Epoch: 663, train loss: 0.540, val loss: 0.662, test loss: 1.012\n",
      "  Epoch: 664, train loss: 0.556, val loss: 0.672, test loss: 1.034\n",
      "  Epoch: 665, train loss: 0.547, val loss: 0.587, test loss: 0.980\n",
      "  Epoch: 666, train loss: 0.548, val loss: 0.647, test loss: 0.982\n",
      "  Epoch: 667, train loss: 0.502, val loss: 0.597, test loss: 0.960\n",
      "  Epoch: 668, train loss: 0.551, val loss: 0.629, test loss: 1.009\n",
      "  Epoch: 669, train loss: 0.506, val loss: 0.568, test loss: 0.967\n",
      "  Epoch: 670, train loss: 0.535, val loss: 0.565, test loss: 0.986\n",
      "  Epoch: 671, train loss: 0.552, val loss: 0.703, test loss: 1.101\n",
      "  Epoch: 672, train loss: 0.549, val loss: 0.627, test loss: 1.006\n",
      "  Epoch: 673, train loss: 0.542, val loss: 0.651, test loss: 1.034\n",
      "  Epoch: 674, train loss: 0.508, val loss: 0.639, test loss: 1.012\n",
      "  Epoch: 675, train loss: 0.563, val loss: 0.635, test loss: 0.961\n",
      "  Epoch: 676, train loss: 0.524, val loss: 0.579, test loss: 0.988\n",
      "  Epoch: 677, train loss: 0.522, val loss: 0.630, test loss: 0.980\n",
      "  Epoch: 678, train loss: 0.573, val loss: 0.628, test loss: 0.962\n",
      "  Epoch: 679, train loss: 0.570, val loss: 0.588, test loss: 0.940\n",
      "  Epoch: 680, train loss: 0.539, val loss: 0.588, test loss: 0.966\n",
      "  Epoch: 681, train loss: 0.539, val loss: 0.588, test loss: 0.946\n",
      "  Epoch: 682, train loss: 0.517, val loss: 0.597, test loss: 0.977\n",
      "  Epoch: 683, train loss: 0.498, val loss: 0.608, test loss: 0.991\n",
      "  Epoch: 684, train loss: 0.526, val loss: 0.650, test loss: 1.051\n",
      "  Epoch: 685, train loss: 0.567, val loss: 0.655, test loss: 1.018\n",
      "  Epoch: 686, train loss: 0.540, val loss: 0.619, test loss: 0.998\n",
      "  Epoch: 687, train loss: 0.544, val loss: 0.598, test loss: 0.952\n",
      "  Epoch: 688, train loss: 0.529, val loss: 0.573, test loss: 0.939\n",
      "  Epoch: 689, train loss: 0.552, val loss: 0.625, test loss: 1.015\n",
      "  Epoch: 690, train loss: 0.568, val loss: 0.673, test loss: 1.052\n",
      "  Epoch: 691, train loss: 0.519, val loss: 0.601, test loss: 0.994\n",
      "  Epoch: 692, train loss: 0.514, val loss: 0.687, test loss: 1.044\n",
      "  Epoch: 693, train loss: 0.540, val loss: 0.587, test loss: 0.989\n",
      "  Epoch: 694, train loss: 0.545, val loss: 0.582, test loss: 0.948\n",
      "  Epoch: 695, train loss: 0.533, val loss: 0.799, test loss: 1.125\n",
      "  Epoch: 696, train loss: 0.550, val loss: 0.606, test loss: 0.987\n",
      "  Epoch: 697, train loss: 0.527, val loss: 0.655, test loss: 1.041\n",
      "  Epoch: 698, train loss: 0.569, val loss: 0.630, test loss: 1.002\n",
      "  Epoch: 699, train loss: 0.532, val loss: 0.627, test loss: 0.954\n",
      "  Epoch: 700, train loss: 0.560, val loss: 0.620, test loss: 1.019\n",
      "  Epoch: 701, train loss: 0.498, val loss: 0.659, test loss: 1.002\n",
      "  Epoch: 702, train loss: 0.557, val loss: 0.632, test loss: 1.002\n",
      "  Epoch: 703, train loss: 0.517, val loss: 0.719, test loss: 1.097\n",
      "  Epoch: 704, train loss: 0.522, val loss: 0.656, test loss: 1.006\n",
      "  Epoch: 705, train loss: 0.510, val loss: 0.577, test loss: 0.966\n",
      "  Epoch: 706, train loss: 0.531, val loss: 0.676, test loss: 1.035\n",
      "  Epoch: 707, train loss: 0.532, val loss: 0.718, test loss: 1.058\n",
      "  Epoch: 708, train loss: 0.522, val loss: 0.567, test loss: 0.955\n",
      "  Epoch: 709, train loss: 0.529, val loss: 0.654, test loss: 1.028\n",
      "  Epoch: 710, train loss: 0.564, val loss: 0.669, test loss: 1.041\n",
      "  Epoch: 711, train loss: 0.540, val loss: 0.615, test loss: 1.006\n",
      "  Epoch: 712, train loss: 0.527, val loss: 0.639, test loss: 1.031\n",
      "  Epoch: 713, train loss: 0.494, val loss: 0.614, test loss: 0.979\n",
      "  Epoch: 714, train loss: 0.519, val loss: 0.592, test loss: 0.993\n",
      "  Epoch: 715, train loss: 0.548, val loss: 0.587, test loss: 0.957\n",
      "  Epoch: 716, train loss: 0.531, val loss: 0.608, test loss: 0.983\n",
      "  Epoch: 717, train loss: 0.551, val loss: 0.600, test loss: 0.960\n",
      "  Epoch: 718, train loss: 0.563, val loss: 0.810, test loss: 1.145\n",
      "  Epoch: 719, train loss: 0.561, val loss: 0.613, test loss: 1.021\n",
      "  Epoch: 720, train loss: 0.558, val loss: 0.670, test loss: 1.004\n",
      "  Epoch: 721, train loss: 0.598, val loss: 0.612, test loss: 0.994\n",
      "  Epoch: 722, train loss: 0.534, val loss: 0.580, test loss: 0.982\n",
      "  Epoch: 723, train loss: 0.509, val loss: 0.813, test loss: 1.104\n",
      "  Epoch: 724, train loss: 0.558, val loss: 0.700, test loss: 1.106\n",
      "  Epoch: 725, train loss: 0.525, val loss: 0.568, test loss: 0.944\n",
      "  Epoch: 726, train loss: 0.493, val loss: 0.593, test loss: 0.998\n",
      "  Epoch: 727, train loss: 0.522, val loss: 0.565, test loss: 0.941\n",
      "  Epoch: 728, train loss: 0.494, val loss: 0.606, test loss: 0.983\n",
      "  Epoch: 729, train loss: 0.510, val loss: 0.618, test loss: 1.000\n",
      "  Epoch: 730, train loss: 0.532, val loss: 0.637, test loss: 1.027\n",
      "  Epoch: 731, train loss: 0.560, val loss: 0.588, test loss: 0.985\n",
      "  Epoch: 732, train loss: 0.515, val loss: 0.586, test loss: 0.965\n",
      "  Epoch: 733, train loss: 0.512, val loss: 0.633, test loss: 1.013\n",
      "  Epoch: 734, train loss: 0.513, val loss: 0.607, test loss: 0.998\n",
      "  Epoch: 735, train loss: 0.515, val loss: 0.756, test loss: 1.134\n",
      "  Epoch: 736, train loss: 0.539, val loss: 0.584, test loss: 0.956\n",
      "  Epoch: 737, train loss: 0.501, val loss: 0.687, test loss: 1.046\n",
      "  Epoch: 738, train loss: 0.532, val loss: 0.670, test loss: 1.026\n",
      "  Epoch: 739, train loss: 0.525, val loss: 0.597, test loss: 0.984\n",
      "  Epoch: 740, train loss: 0.536, val loss: 0.585, test loss: 0.974\n",
      "  Epoch: 741, train loss: 0.547, val loss: 0.586, test loss: 0.988\n",
      "  Epoch: 742, train loss: 0.516, val loss: 0.588, test loss: 0.960\n",
      "  Epoch: 743, train loss: 0.527, val loss: 0.626, test loss: 1.003\n",
      "  Epoch: 744, train loss: 0.526, val loss: 0.577, test loss: 0.987\n",
      "  Epoch: 745, train loss: 0.533, val loss: 0.575, test loss: 0.986\n",
      "  Epoch: 746, train loss: 0.543, val loss: 0.583, test loss: 0.961\n",
      "  Epoch: 747, train loss: 0.500, val loss: 0.688, test loss: 1.026\n",
      "  Epoch: 748, train loss: 0.494, val loss: 0.654, test loss: 1.021\n",
      "  Epoch: 749, train loss: 0.515, val loss: 0.573, test loss: 0.962\n",
      "  Epoch: 750, train loss: 0.491, val loss: 0.632, test loss: 0.993\n",
      "  Epoch: 751, train loss: 0.514, val loss: 0.791, test loss: 1.202\n",
      "  Epoch: 752, train loss: 0.524, val loss: 0.583, test loss: 0.943\n",
      "  Epoch: 753, train loss: 0.544, val loss: 0.575, test loss: 0.951\n",
      "  Epoch: 754, train loss: 0.517, val loss: 0.729, test loss: 1.109\n",
      "  Epoch: 755, train loss: 0.543, val loss: 0.634, test loss: 0.987\n",
      "  Epoch: 756, train loss: 0.558, val loss: 0.578, test loss: 0.936\n",
      "  Epoch: 757, train loss: 0.590, val loss: 0.783, test loss: 1.124\n",
      "  Epoch: 758, train loss: 0.526, val loss: 0.612, test loss: 0.964\n",
      "  Epoch: 759, train loss: 0.539, val loss: 0.596, test loss: 0.948\n",
      "  Epoch: 760, train loss: 0.512, val loss: 0.595, test loss: 0.968\n",
      "  Epoch: 761, train loss: 0.523, val loss: 0.600, test loss: 0.999\n",
      "  Epoch: 762, train loss: 0.501, val loss: 0.717, test loss: 1.031\n",
      "  Epoch: 763, train loss: 0.522, val loss: 0.623, test loss: 1.018\n",
      "  Epoch: 764, train loss: 0.496, val loss: 0.577, test loss: 0.957\n",
      "  Epoch: 765, train loss: 0.525, val loss: 0.645, test loss: 1.004\n",
      "  Epoch: 766, train loss: 0.553, val loss: 0.578, test loss: 0.969\n",
      "  Epoch: 767, train loss: 0.514, val loss: 0.612, test loss: 0.972\n",
      "  Epoch: 768, train loss: 0.541, val loss: 0.575, test loss: 0.974\n",
      "  Epoch: 769, train loss: 0.507, val loss: 0.595, test loss: 0.965\n",
      "  Epoch: 770, train loss: 0.525, val loss: 0.619, test loss: 0.992\n",
      "  Epoch: 771, train loss: 0.521, val loss: 0.601, test loss: 0.991\n",
      "  Epoch: 772, train loss: 0.514, val loss: 0.662, test loss: 0.992\n",
      "  Epoch: 773, train loss: 0.542, val loss: 0.615, test loss: 0.971\n",
      "  Epoch: 774, train loss: 0.568, val loss: 0.583, test loss: 0.973\n",
      "  Epoch: 775, train loss: 0.516, val loss: 0.612, test loss: 0.990\n",
      "  Epoch: 776, train loss: 0.515, val loss: 0.589, test loss: 0.973\n",
      "  Epoch: 777, train loss: 0.550, val loss: 0.622, test loss: 0.988\n",
      "  Epoch: 778, train loss: 0.492, val loss: 0.612, test loss: 0.973\n",
      "  Epoch: 779, train loss: 0.520, val loss: 0.627, test loss: 0.996\n",
      "  Epoch: 780, train loss: 0.507, val loss: 0.627, test loss: 0.971\n",
      "  Epoch: 781, train loss: 0.539, val loss: 0.567, test loss: 0.964\n",
      "  Epoch: 782, train loss: 0.523, val loss: 0.661, test loss: 1.063\n",
      "  Epoch: 783, train loss: 0.526, val loss: 0.613, test loss: 0.999\n",
      "  Epoch: 784, train loss: 0.548, val loss: 0.602, test loss: 0.987\n",
      "  Epoch: 785, train loss: 0.527, val loss: 0.773, test loss: 1.084\n",
      "  Epoch: 786, train loss: 0.533, val loss: 0.707, test loss: 1.106\n",
      "  Epoch: 787, train loss: 0.532, val loss: 0.638, test loss: 0.997\n",
      "  Epoch: 788, train loss: 0.561, val loss: 0.720, test loss: 1.047\n",
      "  Epoch: 789, train loss: 0.517, val loss: 0.691, test loss: 1.045\n",
      "  Epoch: 790, train loss: 0.522, val loss: 0.701, test loss: 1.032\n",
      "  Epoch: 791, train loss: 0.499, val loss: 0.576, test loss: 0.958\n",
      "  Epoch: 792, train loss: 0.509, val loss: 0.599, test loss: 0.947\n",
      "  Epoch: 793, train loss: 0.526, val loss: 0.585, test loss: 0.952\n",
      "  Epoch: 794, train loss: 0.536, val loss: 0.660, test loss: 1.047\n",
      "  Epoch: 795, train loss: 0.519, val loss: 0.585, test loss: 0.970\n",
      "  Epoch: 796, train loss: 0.524, val loss: 0.599, test loss: 0.967\n",
      "  Epoch: 797, train loss: 0.537, val loss: 0.611, test loss: 0.971\n",
      "  Epoch: 798, train loss: 0.524, val loss: 0.613, test loss: 1.000\n",
      "  Epoch: 799, train loss: 0.497, val loss: 0.583, test loss: 0.971\n",
      "  Epoch: 800, train loss: 0.503, val loss: 0.596, test loss: 0.966\n",
      "  Epoch: 801, train loss: 0.511, val loss: 0.582, test loss: 0.996\n",
      "  Epoch: 802, train loss: 0.508, val loss: 0.683, test loss: 1.037\n",
      "  Epoch: 803, train loss: 0.542, val loss: 0.582, test loss: 0.965\n",
      "  Epoch: 804, train loss: 0.518, val loss: 0.593, test loss: 0.989\n",
      "  Epoch: 805, train loss: 0.516, val loss: 0.777, test loss: 1.163\n",
      "  Epoch: 806, train loss: 0.562, val loss: 0.583, test loss: 0.966\n",
      "  Epoch: 807, train loss: 0.542, val loss: 0.670, test loss: 1.041\n",
      "  Epoch: 808, train loss: 0.521, val loss: 0.643, test loss: 0.996\n",
      "  Epoch: 809, train loss: 0.507, val loss: 0.665, test loss: 1.048\n",
      "  Epoch: 810, train loss: 0.530, val loss: 0.626, test loss: 0.989\n",
      "  Epoch: 811, train loss: 0.527, val loss: 0.663, test loss: 0.997\n",
      "  Epoch: 812, train loss: 0.507, val loss: 0.740, test loss: 1.080\n",
      "  Epoch: 813, train loss: 0.527, val loss: 0.668, test loss: 1.047\n",
      "  Epoch: 814, train loss: 0.518, val loss: 0.588, test loss: 0.956\n",
      "  Epoch: 815, train loss: 0.505, val loss: 0.707, test loss: 1.052\n",
      "  Epoch: 816, train loss: 0.519, val loss: 0.587, test loss: 0.990\n",
      "  Epoch: 817, train loss: 0.562, val loss: 0.616, test loss: 0.945\n",
      "  Epoch: 818, train loss: 0.536, val loss: 0.603, test loss: 0.988\n",
      "  Epoch: 819, train loss: 0.503, val loss: 0.642, test loss: 1.000\n",
      "  Epoch: 820, train loss: 0.533, val loss: 0.617, test loss: 0.995\n",
      "  Epoch: 821, train loss: 0.509, val loss: 0.594, test loss: 0.988\n",
      "  Epoch: 822, train loss: 0.516, val loss: 0.585, test loss: 0.957\n",
      "  Epoch: 823, train loss: 0.503, val loss: 0.651, test loss: 1.002\n",
      "  Epoch: 824, train loss: 0.546, val loss: 0.590, test loss: 0.973\n",
      "  Epoch: 825, train loss: 0.519, val loss: 0.571, test loss: 0.963\n",
      "  Epoch: 826, train loss: 0.517, val loss: 0.638, test loss: 1.012\n",
      "  Epoch: 827, train loss: 0.548, val loss: 0.606, test loss: 0.972\n",
      "  Epoch: 828, train loss: 0.484, val loss: 0.685, test loss: 1.060\n",
      "  Epoch: 829, train loss: 0.511, val loss: 0.583, test loss: 0.957\n",
      "  Epoch: 830, train loss: 0.508, val loss: 0.574, test loss: 0.958\n",
      "  Epoch: 831, train loss: 0.501, val loss: 0.598, test loss: 0.961\n",
      "  Epoch: 832, train loss: 0.530, val loss: 0.636, test loss: 1.007\n",
      "  Epoch: 833, train loss: 0.560, val loss: 0.739, test loss: 1.078\n",
      "  Epoch: 834, train loss: 0.512, val loss: 0.639, test loss: 1.015\n",
      "  Epoch: 835, train loss: 0.539, val loss: 0.580, test loss: 0.978\n",
      "Early stopping, best val loss and index:\n",
      "0.5521974 635\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "\n",
    "best_val_index = -1\n",
    "\n",
    "training_start_time = time.time()\n",
    "time_array = []\n",
    "time_array.append(training_start_time)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_model(train_loader, model, optimizer).cpu().detach().numpy()\n",
    "    val_loss = test_model(val_loader, model).cpu().detach().numpy()\n",
    "    test_loss = test_model(test_loader, model).cpu().detach().numpy()\n",
    "    new_min = \" \"\n",
    "    if epoch > 0:\n",
    "        if val_losses[best_val_index] > val_loss:\n",
    "            new_min = \"*\"\n",
    "            best_val_index = epoch\n",
    "            torch.save(model.state_dict(), 'MLP_scan_age_impatient_reshuffle_03.pt') #this thankfully already saves the best val loss model\n",
    "        #temporarily disable early stoppage\n",
    "        # early stopping is called\n",
    "        if len(val_losses) - best_val_index > patience:\n",
    "            print (\"Early stopping, best val loss and index:\")\n",
    "            print(val_losses[best_val_index], best_val_index)\n",
    "            break\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    print(new_min, \"Epoch: %d, train loss: %1.3f, val loss: %1.3f, test loss: %1.3f\" % (epoch, train_loss, val_loss, test_loss))\n",
    "    time_array.append(time.time())\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcb1c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, color='b')\n",
    "plt.plot(val_losses, color='m')\n",
    "plt.plot(test_losses, color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve best val loss\n",
    "best_val_loss = val_losses[best_val_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fa823a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time:  13.45  mins\n",
      "Average training time:  0.96  seconds\n",
      "Longest training time:  2.62 seconds\n",
      "Shortest training time:  0.95 seconds\n",
      "\n",
      "Best Val Loss:  0.5521974  at epoch num  635\n"
     ]
    }
   ],
   "source": [
    "#timing recordings\n",
    "\n",
    "end_time = time.time()\n",
    "total_train_time = end_time - training_start_time\n",
    "time_differences = []\n",
    "for i in range(len(time_array) - 1):\n",
    "    last_time = time_array[i]\n",
    "    this_time = time_array[i + 1]\n",
    "    diff = this_time - last_time\n",
    "    time_differences.append(diff)\n",
    "    \n",
    "\n",
    "average_time = total_train_time / len(time_array)\n",
    "\n",
    "print(\"Total time: \", round(total_train_time / 60, 2), \" mins\")\n",
    "print(\"Average training time: \", round(average_time, 2), \" seconds\")\n",
    "print(\"Longest training time: \", round(max(time_differences), 2), \"seconds\")\n",
    "print(\"Shortest training time: \", round(min(time_differences), 2), \"seconds\")\n",
    "print(\"\\nBest Val Loss: \", best_val_loss, \" at epoch num \", best_val_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59531369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_test import display_test_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc171f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (lin1): Linear(in_features=4, out_features=16, bias=True)\n",
       "  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lin2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lin3): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (bn3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lin4): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (bn4): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lin5): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (lin6): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check against a saved version of the model\n",
    "model_choice = 'MLP_scan_age_impatient_reshuffle_03.pt'\n",
    "model = MLP(4, [16, 16, 16, 16], 1, device=device)\n",
    "model.load_state_dict(torch.load(model_choice))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This ran\n",
      "Will this loss work:  tensor(0.5522, device='cuda:0')\n",
      "test\n",
      "tensor(0.5522, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_loss, predictions, truths, mae = display_test_stats(val_loader, model)\n",
    "\n",
    "print(\"Will this loss work: \", test_loss)\n",
    "#print(\"Predictions: \", predictions, '\\n')\n",
    "print(\"test\")\n",
    "#print(\"Truths: \", truths)\n",
    "print(mae)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
